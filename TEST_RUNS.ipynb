{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDhiLQy105kB"
      },
      "source": [
        "### Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg4p5NliPGAJ",
        "outputId": "7eabc34d-b1ca-4dbd-8647-79188b0e0f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TEAM'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 136 (delta 24), reused 23 (delta 19), pack-reused 103\u001b[K\n",
            "Receiving objects: 100% (136/136), 100.32 MiB | 21.47 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n",
            "Updating files: 100% (88/88), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/declare-lab/TEAM.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNXJCXYfPJvT",
        "outputId": "af219b93-a895-4941-bc9e-00d1cdcf2ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TEAM\n"
          ]
        }
      ],
      "source": [
        "%cd TEAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fab50uCFPYGi",
        "outputId": "9de542fa-c5bf-46dc-851d-1c9a70d54bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==2.5.2\n",
            "  Downloading datasets-2.5.2-py3-none-any.whl (432 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.7/432.7 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.3.5)\n",
            "Collecting scikit_learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (4.64.1)\n",
            "Collecting transformers==4.21.2\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb==0.13.3\n",
            "  Downloading wandb-0.13.3-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (2.25.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (3.8.3)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.5->-r requirements.txt (line 3)) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.5->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.3->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.3->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.3->-r requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.11.0->-r requirements.txt (line 5)) (4.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21.2->-r requirements.txt (line 7)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.21.2->-r requirements.txt (line 7)) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21.2->-r requirements.txt (line 7)) (2022.6.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (3.19.6)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (1.15.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (22.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets==2.5.2->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->-r requirements.txt (line 1)) (4.0.0)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=891a39ffe772af994b3d4f693c63bb2230238822219e85cc3465e936a931ddd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, pathtools, xxhash, urllib3, torch, smmap, shortuuid, setproctitle, numpy, docker-pycreds, dill, sentry-sdk, multiprocess, gitdb, scikit_learn, responses, huggingface-hub, GitPython, wandb, transformers, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "xarray-einstats 0.4.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\n",
            "tifffile 2022.10.10 requires numpy>=1.19.2, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.25 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.30 datasets-2.5.2 dill-0.3.5.1 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.12.0 multiprocess-0.70.13 numpy-1.18.5 pathtools-0.1.2 responses-0.18.0 scikit_learn-1.1.3 sentry-sdk-1.14.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 tokenizers-0.12.1 torch-1.11.0 transformers-4.21.2 urllib3-1.26.14 wandb-0.13.3 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nShsyU2Gh8TQ",
        "outputId": "50bd192f-727d-44b9-9f84-c1bda0134639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.21.0\n",
            "  Downloading numpy-1.21.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.21.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m890.2/890.2 MB\u001b[0m \u001b[31m197.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1112711168 bytes == 0x38d62000 @  0x7fe3bc608680 0x7fe3bc628da2 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.2/890.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wheel\n",
            "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-66.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wheel, typing-extensions, setuptools, nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.38.4\n",
            "    Uninstalling wheel-0.38.4:\n",
            "      Successfully uninstalled wheel-0.38.4\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.13.0 which is incompatible.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 66.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 setuptools-66.1.1 torch-1.13.0 typing-extensions-4.4.0 wheel-0.38.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pio\n",
            "  Downloading pio-0.0.3-py3-none-any.whl (3.4 kB)\n",
            "Collecting prin\n",
            "  Downloading prin-1.1.0.zip (710 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: prin\n",
            "  Building wheel for prin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prin: filename=prin-1.1.0-py3-none-any.whl size=1154 sha256=efaef115f984f3ed1538735208b196daf37ede73cca903afc237196f0fc6854b\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/34/08/b94e5795d376b15ac366057a948257efa0a078b57c640fd2bc\n",
            "Successfully built prin\n",
            "Installing collected packages: sentencepiece, prin, pio\n",
            "Successfully installed pio-0.0.3 prin-1.1.0 sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install --force-reinstall numpy==1.21.0\n",
        "!pip install --force-reinstall torch==1.13.0\n",
        "!pip install sentencepiece pio prin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCgOBjJY05kF"
      },
      "source": [
        "### CSQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt1pnya605kF"
      },
      "source": [
        "##### TEAM - RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbS4s3e2PZJI",
        "outputId": "0e75726b-ff1d-411d-c03e-80228f4708e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=15, epochs=5, eval_bs=15, input_format='1', lr=1e-06, name='roberta-large', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Downloading config.json: 100% 482/482 [00:00<00:00, 444kB/s]\n",
            "Downloading vocab.json: 100% 878k/878k [00:01<00:00, 670kB/s]\n",
            "Downloading merges.txt: 100% 446k/446k [00:01<00:00, 393kB/s]\n",
            "Downloading tokenizer.json: 100% 1.29M/1.29M [00:01<00:00, 1.19MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.33G/1.33G [00:16<00:00, 86.5MB/s]\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230113_095401-t06ov0mv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mkind-sea-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-roberta-large\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-roberta-large/runs/t06ov0mv\u001b[0m\n",
            "Test preds frequency: {'E': 239, 'A': 232, 'C': 232, 'B': 224, 'D': 213}\n",
            "Epoch 1: Loss: Train 0.4836; Val 0.3934\n",
            "Classification Acc: Train 0.7994; Val 0.8028\n",
            "Classification Macro F1: Train 0.4581; Val 0.465\n",
            "Instance Acc: Val 0.6216\n",
            "Test preds frequency: {'A': 241, 'E': 235, 'B': 225, 'D': 222, 'C': 217}\n",
            "Epoch 2: Loss: Train 0.4094; Val 0.3627\n",
            "Classification Acc: Train 0.8108; Val 0.8318\n",
            "Classification Macro F1: Train 0.603; Val 0.7514\n",
            "Instance Acc: Val 0.6773\n",
            "Test preds frequency: {'D': 232, 'C': 231, 'E': 228, 'A': 228, 'B': 221}\n",
            "Epoch 3: Loss: Train 0.372; Val 0.3356\n",
            "Classification Acc: Train 0.8286; Val 0.847\n",
            "Classification Macro F1: Train 0.687; Val 0.7545\n",
            "Instance Acc: Val 0.7191\n",
            "Test preds frequency: {'C': 235, 'B': 229, 'A': 229, 'E': 226, 'D': 221}\n",
            "Epoch 4: Loss: Train 0.3459; Val 0.3434\n",
            "Classification Acc: Train 0.84; Val 0.8424\n",
            "Classification Macro F1: Train 0.7196; Val 0.7707\n",
            "Instance Acc: Val 0.7314\n",
            "Test preds frequency: {'B': 232, 'A': 232, 'C': 228, 'D': 225, 'E': 223}\n",
            "Epoch 5: Loss: Train 0.3193; Val 0.3485\n",
            "Classification Acc: Train 0.8553; Val 0.8314\n",
            "Classification Macro F1: Train 0.7549; Val 0.7657\n",
            "Instance Acc: Val 0.7428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▂▅▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▅▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁▆█▇▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁▄▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▄▁▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.8553\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.3193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.7428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.3485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mkind-sea-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-roberta-large/runs/t06ov0mv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230113_095401-t06ov0mv/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_csqa.py --epochs 5 --lr 1e-6 --shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwBV-9oo05kG"
      },
      "source": [
        "##### SCORE - RoBEERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g3dhLF-8vUx",
        "outputId": "e087426f-16f0-4c2e-9ccb-b9c7d74a0724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved/csqa/mcq/roberta-large/runs/Jan15_15-13-39_50285f0c5623,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=saved/csqa/mcq/roberta-large,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=10,\n",
            "per_device_train_batch_size=5,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=MCQA CSQA ROBERTA,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.005,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-097216e0329ca922\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "100% 3/3 [00:00<00:00, 749.56it/s]\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:13:41,354 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:13:41,356 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:404] 2023-01-15 15:13:42,271 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:13:43,184 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:13:43,185 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,586 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:13:50,503 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:13:50,504 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2041] 2023-01-15 15:13:51,486 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[WARNING|modeling_utils.py:2425] 2023-01-15 15:13:55,418 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2437] 2023-01-15 15:13:55,418 >> Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-416c95ae5e71be9c.arrow\n",
            "  0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a1255d4c19c65799.arrow\n",
            "100% 2/2 [00:00<00:00,  2.95ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2cb265b6aa621904.arrow\n",
            "Epoch count 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:722] 2023-01-15 15:14:01,014 >> The following columns in the training set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-15 15:14:01,029 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-15 15:14:01,029 >>   Num examples = 9741\n",
            "[INFO|trainer.py:1607] 2023-01-15 15:14:01,029 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2023-01-15 15:14:01,029 >>   Instantaneous batch size per device = 5\n",
            "[INFO|trainer.py:1609] 2023-01-15 15:14:01,029 >>   Total train batch size (w. parallel, distributed & accumulation) = 5\n",
            "[INFO|trainer.py:1610] 2023-01-15 15:14:01,029 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-15 15:14:01,029 >>   Total optimization steps = 9745\n",
            "[INFO|integrations.py:607] 2023-01-15 15:14:01,031 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalyvasman\u001b[0m (\u001b[33mnlpteam_gr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/TEAM/wandb/run-20230115_151403-14r1s8x3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMCQA CSQA ROBERTA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/14r1s8x3\u001b[0m\n",
            "{'loss': 1.6186, 'learning_rate': 9.486916367367881e-07, 'epoch': 0.26}\n",
            "{'loss': 1.6222, 'learning_rate': 8.973832734735761e-07, 'epoch': 0.51}\n",
            "{'loss': 1.6186, 'learning_rate': 8.460749102103642e-07, 'epoch': 0.77}\n",
            " 20% 1949/9745 [19:37<1:06:10,  1.96it/s][INFO|trainer.py:722] 2023-01-15 15:33:41,712 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 15:33:41,716 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 15:33:41,716 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 15:33:41,716 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.39it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.34it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:27,  4.25it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:31,  3.77it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:32,  3.61it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:30,  3.85it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:33,  3.46it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.01it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:39,  2.88it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:38,  2.95it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.73it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:39,  2.79it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.81it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:40,  2.67it/s]\u001b[A\n",
            " 13% 16/123 [00:05<00:38,  2.79it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:35,  2.98it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:31,  3.31it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:32,  3.24it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:35,  2.90it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:33,  3.09it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.40it/s]\u001b[A\n",
            " 19% 23/123 [00:07<00:27,  3.66it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:29,  3.37it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.17it/s]\u001b[A\n",
            " 21% 26/123 [00:08<00:31,  3.09it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:33,  2.90it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:29,  3.27it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:29,  3.21it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.48it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.36it/s]\u001b[A\n",
            " 26% 32/123 [00:10<00:32,  2.82it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:30,  2.99it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:30,  2.95it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.29it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:26,  3.33it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.15it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.30it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:29,  2.89it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.08it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.09it/s]\u001b[A\n",
            " 34% 42/123 [00:13<00:23,  3.40it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.38it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.95it/s]\u001b[A\n",
            " 37% 45/123 [00:14<00:25,  3.00it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:25,  3.05it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.00it/s]\u001b[A\n",
            " 39% 48/123 [00:15<00:25,  2.98it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.03it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.13it/s]\u001b[A\n",
            " 41% 51/123 [00:16<00:22,  3.15it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.16it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.87it/s]\u001b[A\n",
            " 44% 54/123 [00:17<00:23,  2.97it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.07it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.14it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:20,  3.28it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.38it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.29it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:19,  3.30it/s]\u001b[A\n",
            " 50% 61/123 [00:19<00:19,  3.26it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.94it/s]\u001b[A\n",
            " 51% 63/123 [00:20<00:24,  2.49it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:24,  2.45it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.46it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:22,  2.58it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:20,  2.79it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.01it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.74it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.79it/s]\u001b[A\n",
            " 58% 71/123 [00:23<00:19,  2.69it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.81it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.89it/s]\u001b[A\n",
            " 60% 74/123 [00:24<00:17,  2.74it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.75it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.87it/s]\u001b[A\n",
            " 63% 77/123 [00:25<00:14,  3.20it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.05it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.07it/s]\u001b[A\n",
            " 65% 80/123 [00:26<00:13,  3.10it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.25it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.55it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.31it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.98it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.00it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.06it/s]\u001b[A\n",
            " 71% 87/123 [00:28<00:10,  3.40it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.48it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.35it/s]\u001b[A\n",
            " 73% 90/123 [00:29<00:09,  3.30it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.86it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.62it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:12,  2.50it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:10,  2.88it/s]\u001b[A\n",
            " 77% 95/123 [00:31<00:10,  2.73it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.77it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.93it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.23it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.38it/s]\u001b[A\n",
            " 81% 100/123 [00:33<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.72it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.81it/s]\u001b[A\n",
            " 84% 103/123 [00:34<00:06,  2.96it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.75it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.12it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.09it/s]\u001b[A\n",
            " 87% 107/123 [00:35<00:04,  3.20it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.91it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.25it/s]\u001b[A\n",
            " 89% 110/123 [00:36<00:03,  3.37it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.46it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.40it/s]\u001b[A\n",
            " 92% 113/123 [00:37<00:02,  3.48it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.34it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.43it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.24it/s]\u001b[A\n",
            " 95% 117/123 [00:38<00:01,  3.19it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.51it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.07it/s]\u001b[A\n",
            " 98% 120/123 [00:39<00:00,  3.08it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.11it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 1.4447888135910034, 'eval_accuracy': 0.4046, 'eval_runtime': 40.2733, 'eval_samples_per_second': 30.318, 'eval_steps_per_second': 3.054, 'epoch': 1.0}\n",
            " 20% 1949/9745 [20:18<1:06:10,  1.96it/s]\n",
            "100% 123/123 [00:39<00:00,  3.04it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 15:34:22,002 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-1949\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 15:34:22,003 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-1949/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 15:34:27,256 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-1949/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 15:34:27,257 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-1949/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 15:34:27,258 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-1949/special_tokens_map.json\n",
            "{'loss': 1.5761, 'learning_rate': 7.947665469471524e-07, 'epoch': 1.03}\n",
            "{'loss': 1.5258, 'learning_rate': 7.434581836839404e-07, 'epoch': 1.28}\n",
            "{'loss': 1.4954, 'learning_rate': 6.921498204207286e-07, 'epoch': 1.54}\n",
            "{'loss': 1.4479, 'learning_rate': 6.408414571575167e-07, 'epoch': 1.8}\n",
            " 40% 3898/9745 [40:11<49:49,  1.96it/s]  [INFO|trainer.py:722] 2023-01-15 15:54:15,010 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 15:54:15,013 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 15:54:15,013 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 15:54:15,013 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.23it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.27it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:28,  4.22it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:31,  3.77it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:32,  3.61it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:30,  3.86it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:33,  3.47it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.02it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:39,  2.88it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:38,  2.95it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.74it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:39,  2.80it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.82it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:40,  2.68it/s]\u001b[A\n",
            " 13% 16/123 [00:05<00:38,  2.79it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:35,  2.99it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:31,  3.32it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:32,  3.24it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:35,  2.90it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:32,  3.09it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.42it/s]\u001b[A\n",
            " 19% 23/123 [00:06<00:27,  3.68it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:29,  3.41it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.20it/s]\u001b[A\n",
            " 21% 26/123 [00:08<00:31,  3.10it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:32,  2.92it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:28,  3.28it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:29,  3.22it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.50it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.38it/s]\u001b[A\n",
            " 26% 32/123 [00:10<00:32,  2.83it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:30,  2.99it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:29,  2.97it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.32it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:25,  3.36it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.15it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.29it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:28,  2.90it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.08it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.09it/s]\u001b[A\n",
            " 34% 42/123 [00:13<00:23,  3.42it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.41it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.96it/s]\u001b[A\n",
            " 37% 45/123 [00:14<00:26,  3.00it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:25,  3.05it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.01it/s]\u001b[A\n",
            " 39% 48/123 [00:15<00:25,  2.99it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.04it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.14it/s]\u001b[A\n",
            " 41% 51/123 [00:16<00:22,  3.16it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.17it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.87it/s]\u001b[A\n",
            " 44% 54/123 [00:17<00:23,  2.96it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.06it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.14it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:20,  3.28it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.39it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.29it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:19,  3.30it/s]\u001b[A\n",
            " 50% 61/123 [00:19<00:19,  3.26it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.94it/s]\u001b[A\n",
            " 51% 63/123 [00:20<00:24,  2.48it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:24,  2.45it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.45it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:22,  2.58it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:20,  2.79it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.01it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.74it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.79it/s]\u001b[A\n",
            " 58% 71/123 [00:23<00:19,  2.69it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.81it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.90it/s]\u001b[A\n",
            " 60% 74/123 [00:24<00:17,  2.74it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.76it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.87it/s]\u001b[A\n",
            " 63% 77/123 [00:24<00:14,  3.20it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.05it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.07it/s]\u001b[A\n",
            " 65% 80/123 [00:25<00:13,  3.10it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.25it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.54it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.28it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.97it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.00it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.04it/s]\u001b[A\n",
            " 71% 87/123 [00:28<00:10,  3.39it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.48it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.37it/s]\u001b[A\n",
            " 73% 90/123 [00:29<00:09,  3.31it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.86it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.63it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:11,  2.50it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:10,  2.89it/s]\u001b[A\n",
            " 77% 95/123 [00:31<00:10,  2.72it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.78it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.94it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.23it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.39it/s]\u001b[A\n",
            " 81% 100/123 [00:33<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.73it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.82it/s]\u001b[A\n",
            " 84% 103/123 [00:33<00:06,  2.96it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.75it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.12it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.10it/s]\u001b[A\n",
            " 87% 107/123 [00:35<00:04,  3.20it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.91it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.27it/s]\u001b[A\n",
            " 89% 110/123 [00:36<00:03,  3.38it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.46it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.39it/s]\u001b[A\n",
            " 92% 113/123 [00:36<00:02,  3.48it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.34it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.42it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.24it/s]\u001b[A\n",
            " 95% 117/123 [00:38<00:01,  3.20it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.53it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.09it/s]\u001b[A\n",
            " 98% 120/123 [00:39<00:00,  3.09it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.09it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1734435558319092, 'eval_accuracy': 0.5364, 'eval_runtime': 40.2211, 'eval_samples_per_second': 30.357, 'eval_steps_per_second': 3.058, 'epoch': 2.0}\n",
            " 40% 3898/9745 [40:51<49:49,  1.96it/s]\n",
            "100% 123/123 [00:39<00:00,  3.02it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 15:54:55,237 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-3898\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 15:54:55,238 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-3898/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 15:55:00,546 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-3898/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 15:55:00,547 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-3898/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 15:55:00,548 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-3898/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 15:55:12,120 >> Deleting older checkpoint [saved/csqa/mcq/roberta-large/checkpoint-1949] due to args.save_total_limit\n",
            "{'loss': 1.4014, 'learning_rate': 5.895330938943047e-07, 'epoch': 2.05}\n",
            "{'loss': 1.3271, 'learning_rate': 5.382247306310928e-07, 'epoch': 2.31}\n",
            "{'loss': 1.3053, 'learning_rate': 4.869163673678809e-07, 'epoch': 2.57}\n",
            "{'loss': 1.245, 'learning_rate': 4.35608004104669e-07, 'epoch': 2.82}\n",
            " 60% 5847/9745 [1:00:44<30:40,  2.12it/s][INFO|trainer.py:722] 2023-01-15 16:14:48,037 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:14:48,039 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:14:48,039 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:14:48,040 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.47it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.38it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:27,  4.28it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:30,  3.83it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:31,  3.68it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:29,  3.91it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:32,  3.52it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.07it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:38,  2.92it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:37,  2.99it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.77it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:38,  2.83it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.85it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:39,  2.71it/s]\u001b[A\n",
            " 13% 16/123 [00:04<00:37,  2.83it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:34,  3.04it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:30,  3.39it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:31,  3.30it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:34,  2.96it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:32,  3.15it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.46it/s]\u001b[A\n",
            " 19% 23/123 [00:06<00:26,  3.71it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:28,  3.45it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.22it/s]\u001b[A\n",
            " 21% 26/123 [00:07<00:30,  3.13it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:32,  2.96it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:28,  3.32it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:28,  3.25it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.54it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.40it/s]\u001b[A\n",
            " 26% 32/123 [00:09<00:32,  2.83it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:29,  3.01it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:29,  2.99it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.32it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:25,  3.36it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.18it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.32it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:28,  2.92it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.10it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.12it/s]\u001b[A\n",
            " 34% 42/123 [00:12<00:23,  3.44it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.41it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.98it/s]\u001b[A\n",
            " 37% 45/123 [00:13<00:25,  3.03it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:25,  3.07it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.04it/s]\u001b[A\n",
            " 39% 48/123 [00:14<00:24,  3.01it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.07it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.17it/s]\u001b[A\n",
            " 41% 51/123 [00:15<00:22,  3.19it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.20it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.90it/s]\u001b[A\n",
            " 44% 54/123 [00:16<00:22,  3.00it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.09it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.16it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:19,  3.30it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.40it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.31it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:18,  3.33it/s]\u001b[A\n",
            " 50% 61/123 [00:18<00:18,  3.29it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.96it/s]\u001b[A\n",
            " 51% 63/123 [00:19<00:23,  2.50it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:23,  2.47it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.46it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:22,  2.59it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:19,  2.80it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.02it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.76it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.81it/s]\u001b[A\n",
            " 58% 71/123 [00:22<00:19,  2.70it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.82it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.91it/s]\u001b[A\n",
            " 60% 74/123 [00:23<00:17,  2.75it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.76it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.88it/s]\u001b[A\n",
            " 63% 77/123 [00:24<00:14,  3.19it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.06it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.09it/s]\u001b[A\n",
            " 65% 80/123 [00:25<00:13,  3.11it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.25it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.56it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.29it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.99it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.01it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.06it/s]\u001b[A\n",
            " 71% 87/123 [00:27<00:10,  3.41it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.48it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.37it/s]\u001b[A\n",
            " 73% 90/123 [00:28<00:09,  3.31it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.86it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.63it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:11,  2.51it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:09,  2.91it/s]\u001b[A\n",
            " 77% 95/123 [00:30<00:10,  2.74it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.79it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.95it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.24it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.39it/s]\u001b[A\n",
            " 81% 100/123 [00:32<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.73it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.82it/s]\u001b[A\n",
            " 84% 103/123 [00:33<00:06,  2.97it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.76it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.13it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.12it/s]\u001b[A\n",
            " 87% 107/123 [00:34<00:04,  3.23it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.93it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.28it/s]\u001b[A\n",
            " 89% 110/123 [00:35<00:03,  3.38it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.47it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.39it/s]\u001b[A\n",
            " 92% 113/123 [00:36<00:02,  3.48it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.36it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.45it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.25it/s]\u001b[A\n",
            " 95% 117/123 [00:37<00:01,  3.20it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.52it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.07it/s]\u001b[A\n",
            " 98% 120/123 [00:38<00:00,  3.09it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.11it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.9874638319015503, 'eval_accuracy': 0.6208, 'eval_runtime': 39.9617, 'eval_samples_per_second': 30.554, 'eval_steps_per_second': 3.078, 'epoch': 3.0}\n",
            " 60% 5847/9745 [1:01:24<30:40,  2.12it/s]\n",
            "100% 123/123 [00:39<00:00,  3.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:15:28,004 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-5847\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:15:28,006 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-5847/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:15:33,302 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-5847/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:15:33,304 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-5847/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:15:33,304 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-5847/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:15:44,635 >> Deleting older checkpoint [saved/csqa/mcq/roberta-large/checkpoint-3898] due to args.save_total_limit\n",
            "{'loss': 1.2381, 'learning_rate': 3.842996408414572e-07, 'epoch': 3.08}\n",
            "{'loss': 1.1718, 'learning_rate': 3.3299127757824525e-07, 'epoch': 3.34}\n",
            "{'loss': 1.1427, 'learning_rate': 2.816829143150333e-07, 'epoch': 3.59}\n",
            "{'loss': 1.1515, 'learning_rate': 2.3037455105182143e-07, 'epoch': 3.85}\n",
            " 80% 7796/9745 [1:21:19<15:51,  2.05it/s][INFO|trainer.py:722] 2023-01-15 16:35:23,204 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:35:23,207 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:35:23,208 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:35:23,208 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.26it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.41it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:27,  4.32it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:30,  3.82it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:31,  3.67it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:29,  3.91it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:32,  3.51it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.05it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:38,  2.92it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:37,  2.98it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.76it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:38,  2.84it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.84it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:40,  2.70it/s]\u001b[A\n",
            " 13% 16/123 [00:04<00:37,  2.82it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:34,  3.04it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:31,  3.38it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:31,  3.30it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:34,  2.95it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:32,  3.13it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.46it/s]\u001b[A\n",
            " 19% 23/123 [00:06<00:26,  3.72it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:28,  3.44it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.21it/s]\u001b[A\n",
            " 21% 26/123 [00:07<00:30,  3.14it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:32,  2.95it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:28,  3.30it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:28,  3.25it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.52it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.39it/s]\u001b[A\n",
            " 26% 32/123 [00:09<00:32,  2.83it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:29,  3.00it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:29,  2.98it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.32it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:25,  3.37it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.17it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.31it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:28,  2.92it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.10it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.12it/s]\u001b[A\n",
            " 34% 42/123 [00:12<00:23,  3.44it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.41it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.99it/s]\u001b[A\n",
            " 37% 45/123 [00:13<00:25,  3.03it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:24,  3.08it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.04it/s]\u001b[A\n",
            " 39% 48/123 [00:14<00:24,  3.01it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.08it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.16it/s]\u001b[A\n",
            " 41% 51/123 [00:15<00:22,  3.18it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.19it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.89it/s]\u001b[A\n",
            " 44% 54/123 [00:16<00:23,  2.98it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.08it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.15it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:20,  3.30it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.39it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.31it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:18,  3.34it/s]\u001b[A\n",
            " 50% 61/123 [00:18<00:18,  3.28it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.97it/s]\u001b[A\n",
            " 51% 63/123 [00:19<00:23,  2.51it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:23,  2.47it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.47it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:21,  2.60it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:19,  2.81it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.03it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.76it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.81it/s]\u001b[A\n",
            " 58% 71/123 [00:22<00:19,  2.70it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.81it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.89it/s]\u001b[A\n",
            " 60% 74/123 [00:23<00:17,  2.75it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.76it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.87it/s]\u001b[A\n",
            " 63% 77/123 [00:24<00:14,  3.20it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.05it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.08it/s]\u001b[A\n",
            " 65% 80/123 [00:25<00:13,  3.11it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.25it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.56it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.30it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.98it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.01it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.06it/s]\u001b[A\n",
            " 71% 87/123 [00:27<00:10,  3.40it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.48it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.35it/s]\u001b[A\n",
            " 73% 90/123 [00:28<00:10,  3.30it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.87it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.62it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:11,  2.51it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:09,  2.90it/s]\u001b[A\n",
            " 77% 95/123 [00:30<00:10,  2.73it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.78it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.94it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.23it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.38it/s]\u001b[A\n",
            " 81% 100/123 [00:32<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.73it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.82it/s]\u001b[A\n",
            " 84% 103/123 [00:33<00:06,  2.98it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.76it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.13it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.10it/s]\u001b[A\n",
            " 87% 107/123 [00:35<00:04,  3.22it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.91it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.25it/s]\u001b[A\n",
            " 89% 110/123 [00:35<00:03,  3.36it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.45it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.37it/s]\u001b[A\n",
            " 92% 113/123 [00:36<00:02,  3.46it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.34it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.41it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.25it/s]\u001b[A\n",
            " 95% 117/123 [00:38<00:01,  3.21it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.53it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.08it/s]\u001b[A\n",
            " 98% 120/123 [00:39<00:00,  3.08it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.09it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.939306378364563, 'eval_accuracy': 0.6495, 'eval_runtime': 40.0248, 'eval_samples_per_second': 30.506, 'eval_steps_per_second': 3.073, 'epoch': 4.0}\n",
            " 80% 7796/9745 [1:21:59<15:51,  2.05it/s]\n",
            "100% 123/123 [00:39<00:00,  3.01it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:36:03,236 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-7796\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:36:03,237 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-7796/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:36:08,582 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-7796/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:36:08,583 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-7796/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:36:08,584 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-7796/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:36:19,898 >> Deleting older checkpoint [saved/csqa/mcq/roberta-large/checkpoint-5847] due to args.save_total_limit\n",
            "{'loss': 1.1419, 'learning_rate': 1.7906618778860955e-07, 'epoch': 4.1}\n",
            "{'loss': 1.119, 'learning_rate': 1.2775782452539762e-07, 'epoch': 4.36}\n",
            "{'loss': 1.1087, 'learning_rate': 7.644946126218574e-08, 'epoch': 4.62}\n",
            "{'loss': 1.0924, 'learning_rate': 2.514109799897383e-08, 'epoch': 4.87}\n",
            "100% 9745/9745 [1:41:52<00:00,  2.06it/s][INFO|trainer.py:722] 2023-01-15 16:55:56,240 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:55:56,243 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:55:56,243 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:55:56,243 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.39it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.38it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:27,  4.28it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:31,  3.80it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:32,  3.65it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:29,  3.90it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:32,  3.49it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.03it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:38,  2.90it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:37,  2.97it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.75it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:39,  2.81it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.84it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:40,  2.70it/s]\u001b[A\n",
            " 13% 16/123 [00:04<00:38,  2.81it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:35,  3.01it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:31,  3.35it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:31,  3.28it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:35,  2.94it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:32,  3.12it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.45it/s]\u001b[A\n",
            " 19% 23/123 [00:06<00:27,  3.70it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:28,  3.43it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.21it/s]\u001b[A\n",
            " 21% 26/123 [00:07<00:31,  3.11it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:32,  2.93it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:28,  3.29it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:29,  3.22it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.50it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.37it/s]\u001b[A\n",
            " 26% 32/123 [00:09<00:32,  2.83it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:29,  3.00it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:29,  2.98it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.32it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:25,  3.36it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.17it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.31it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:28,  2.91it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.09it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.11it/s]\u001b[A\n",
            " 34% 42/123 [00:12<00:23,  3.44it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.41it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.96it/s]\u001b[A\n",
            " 37% 45/123 [00:14<00:25,  3.01it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:25,  3.07it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.02it/s]\u001b[A\n",
            " 39% 48/123 [00:15<00:25,  2.99it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.06it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.14it/s]\u001b[A\n",
            " 41% 51/123 [00:15<00:22,  3.17it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.18it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.89it/s]\u001b[A\n",
            " 44% 54/123 [00:16<00:23,  2.99it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.08it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.15it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:19,  3.30it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.41it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.31it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:18,  3.33it/s]\u001b[A\n",
            " 50% 61/123 [00:19<00:18,  3.29it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.95it/s]\u001b[A\n",
            " 51% 63/123 [00:20<00:24,  2.50it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:23,  2.47it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.46it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:22,  2.59it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:19,  2.81it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.02it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.76it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.81it/s]\u001b[A\n",
            " 58% 71/123 [00:22<00:19,  2.69it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.80it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.89it/s]\u001b[A\n",
            " 60% 74/123 [00:23<00:17,  2.74it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.75it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.86it/s]\u001b[A\n",
            " 63% 77/123 [00:24<00:14,  3.20it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.06it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.09it/s]\u001b[A\n",
            " 65% 80/123 [00:25<00:13,  3.11it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.27it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.56it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.29it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.99it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.02it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.06it/s]\u001b[A\n",
            " 71% 87/123 [00:27<00:10,  3.41it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.49it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.37it/s]\u001b[A\n",
            " 73% 90/123 [00:28<00:09,  3.30it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.86it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.62it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:11,  2.50it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:10,  2.89it/s]\u001b[A\n",
            " 77% 95/123 [00:30<00:10,  2.72it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.78it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.95it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.24it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.39it/s]\u001b[A\n",
            " 81% 100/123 [00:32<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.73it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.82it/s]\u001b[A\n",
            " 84% 103/123 [00:33<00:06,  2.96it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.76it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.13it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.11it/s]\u001b[A\n",
            " 87% 107/123 [00:35<00:04,  3.24it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.93it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.27it/s]\u001b[A\n",
            " 89% 110/123 [00:35<00:03,  3.37it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.46it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.39it/s]\u001b[A\n",
            " 92% 113/123 [00:36<00:02,  3.47it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.35it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.45it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.26it/s]\u001b[A\n",
            " 95% 117/123 [00:38<00:01,  3.19it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.52it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.07it/s]\u001b[A\n",
            " 98% 120/123 [00:39<00:00,  3.08it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.10it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.9197953939437866, 'eval_accuracy': 0.6536, 'eval_runtime': 40.069, 'eval_samples_per_second': 30.472, 'eval_steps_per_second': 3.07, 'epoch': 5.0}\n",
            "100% 9745/9745 [1:42:32<00:00,  2.06it/s]\n",
            "100% 123/123 [00:39<00:00,  3.04it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:56:36,318 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-9745\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:56:36,319 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-9745/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:56:41,624 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-9745/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:56:41,625 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-9745/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:56:41,626 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-9745/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:56:52,992 >> Deleting older checkpoint [saved/csqa/mcq/roberta-large/checkpoint-7796] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2023-01-15 16:56:53,063 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 6172.0336, 'train_samples_per_second': 7.891, 'train_steps_per_second': 1.579, 'train_loss': 1.3281526646777995, 'epoch': 5.0}\n",
            "100% 9745/9745 [1:42:49<00:00,  1.58it/s]\n",
            "[INFO|trainer.py:2640] 2023-01-15 16:56:53,066 >> Saving model checkpoint to saved/csqa/mcq/roberta-large\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:56:53,067 >> Configuration saved in saved/csqa/mcq/roberta-large/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:56:58,584 >> Model weights saved in saved/csqa/mcq/roberta-large/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:56:58,585 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:56:58,586 >> Special tokens file saved in saved/csqa/mcq/roberta-large/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     1.3282\n",
            "  train_runtime            = 1:42:52.03\n",
            "  train_samples            =       9741\n",
            "  train_samples_per_second =      7.891\n",
            "  train_steps_per_second   =      1.579\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-15 16:56:58,721 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:56:58,724 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:56:58,725 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:56:58,726 >>   Batch size = 10\n",
            "100% 123/123 [00:40<00:00,  3.01it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.6536\n",
            "  eval_loss               =     0.9198\n",
            "  eval_runtime            = 0:00:41.28\n",
            "  eval_samples            =       1221\n",
            "  eval_samples_per_second =     29.577\n",
            "  eval_steps_per_second   =       2.98\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:722] 2023-01-15 16:57:40,010 >> The following columns in the test set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:57:40,012 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:57:40,012 >>   Num examples = 1140\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:57:40,013 >>   Batch size = 10\n",
            "100% 114/114 [00:38<00:00,  2.93it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▅▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▄▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▃▂▁▁▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▆▇██▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▆▇██▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ██▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ███▇▇▆▆▅▄▄▃▃▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.6536\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.9198\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 41.2818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 29.577\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 2.98\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 9745\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 1.0924\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.375459608354597e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.32815\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 6172.0336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 7.891\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMCQA CSQA ROBERTA\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/14r1s8x3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230115_151403-14r1s8x3/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run_mcqa_score.py --learning_rate=1e-6 --num_train_epochs 5 --seed 42 \\\n",
        "--train_file=\"data/csqa/mcq_train.json\" --validation_file=\"data/csqa/mcq_valid.json\" --test_file=\"data/csqa/mcq_test.json\" \\\n",
        "--output_dir=\"saved/csqa/mcq/roberta-large\" --model_name_or_path=\"roberta-large\" \\\n",
        "--per_device_train_batch_size=5 --per_device_eval_batch_size=10 --weight_decay=0.005 \\\n",
        "--do_train True --do_eval True --do_predict True --evaluation_strategy=\"epoch\" --save_strategy=\"epoch\" \\\n",
        "--report_to \"wandb\" --run_name \"MCQA CSQA ROBERTA\" --save_total_limit=1 --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "228iV7C_05kG"
      },
      "source": [
        "##### TEAM - DeBEERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIvTULClilxd",
        "outputId": "810865e1-28d8-497c-acca-089f5c23d2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=15, epochs=5, eval_bs=15, input_format='1', lr=1e-06, name='microsoft/deberta-v3-base', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading pytorch_model.bin: 100% 354M/354M [00:12<00:00, 30.9MB/s]\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalyvasman\u001b[0m (\u001b[33mnlpteam_gr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230113_123521-f4hmo5sk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mproud-moon-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-deberta-v3-base\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-deberta-v3-base/runs/f4hmo5sk\u001b[0m\n",
            "Test preds frequency: {'A': 241, 'C': 237, 'D': 233, 'B': 218, 'E': 211}\n",
            "Epoch 1: Loss: Train 0.4582; Val 0.3906\n",
            "Classification Acc: Train 0.7974; Val 0.8\n",
            "Classification Macro F1: Train 0.4505; Val 0.4444\n",
            "Instance Acc: Val 0.6773\n",
            "Test preds frequency: {'A': 247, 'C': 234, 'D': 224, 'B': 222, 'E': 213}\n",
            "Epoch 2: Loss: Train 0.3948; Val 0.3539\n",
            "Classification Acc: Train 0.8143; Val 0.8473\n",
            "Classification Macro F1: Train 0.5727; Val 0.7409\n",
            "Instance Acc: Val 0.7232\n",
            "Test preds frequency: {'A': 250, 'B': 229, 'D': 226, 'C': 222, 'E': 213}\n",
            "Epoch 3: Loss: Train 0.3646; Val 0.3393\n",
            "Classification Acc: Train 0.8325; Val 0.8393\n",
            "Classification Macro F1: Train 0.7011; Val 0.7631\n",
            "Instance Acc: Val 0.7437\n",
            "Test preds frequency: {'A': 243, 'C': 231, 'B': 227, 'D': 226, 'E': 213}\n",
            "Epoch 4: Loss: Train 0.341; Val 0.3312\n",
            "Classification Acc: Train 0.8456; Val 0.8423\n",
            "Classification Macro F1: Train 0.7416; Val 0.7715\n",
            "Instance Acc: Val 0.7576\n",
            "Test preds frequency: {'A': 240, 'C': 229, 'D': 229, 'B': 226, 'E': 216}\n",
            "Epoch 5: Loss: Train 0.324; Val 0.3181\n",
            "Classification Acc: Train 0.856; Val 0.8537\n",
            "Classification Macro F1: Train 0.7614; Val 0.7781\n",
            "Instance Acc: Val 0.7666\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▃▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▅▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁▇▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▄▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.324\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8537\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.7666\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.3181\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mproud-moon-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-deberta-v3-base/runs/f4hmo5sk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230113_123521-f4hmo5sk/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_csqa.py --name \"microsoft/deberta-v3-base\" --epochs 5 --lr 1e-6 --shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SCORE - DeBERTa"
      ],
      "metadata": {
        "id": "qi-nUitSAzjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run_mcqa_score.py --learning_rate=1e-6 --num_train_epochs 5 --seed 42 \\\n",
        "--train_file=\"data/csqa/mcq_train.json\" --validation_file=\"data/csqa/mcq_valid.json\" --test_file=\"data/csqa/mcq_test.json\" \\\n",
        "--output_dir=\"saved/csqa/mcq/deberta-large\" --model_name_or_path=\"deberta-large\" \\\n",
        "--per_device_train_batch_size=10 --per_device_eval_batch_size=10 --weight_decay=0.005 \\\n",
        "--do_train True --do_eval True --do_predict True --evaluation_strategy=\"epoch\" --save_strategy=\"epoch\" \\\n",
        "--report_to \"wandb\" --run_name \"MCQA CSQA DEBERTA\" --save_total_limit=1 --overwrite_output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vf_9YCqA3TC",
        "outputId": "9e1a7a77-3604-4b76-c862-85f25524210d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-27 11:38:18.544426: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved/csqa/mcq/deberta-large/runs/Jan27_11-38-20_f1ddfecb8ec5,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=saved/csqa/mcq/deberta-large,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=10,\n",
            "per_device_train_batch_size=10,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=MCQA CSQA DEBERTA,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.005,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-02f21b54dd4b6d05\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-02f21b54dd4b6d05/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-02f21b54dd4b6d05/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-02f21b54dd4b6d05/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "100% 3/3 [00:00<00:00, 743.93it/s]\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 11:38:21,846 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 11:38:21,847 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 11:38:22,103 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 11:38:22,104 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:38:22,883 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/6386fc34376768db39488179803c16268ff12ee177a43a993690f66b7d7a0b7c.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:38:22,884 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:38:22,884 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:38:22,884 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:38:22,884 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/cae8294cb38511dc11086c090549f0a079bc5537a0f9a482d8358f17acc8cff0.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 11:38:23,011 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 11:38:23,012 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils.py:426] 2023-01-27 11:38:23,541 >> Adding [MASK] to the vocabulary\n",
            "[WARNING|logging.py:279] 2023-01-27 11:38:23,542 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 11:38:23,668 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 11:38:23,669 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:279] 2023-01-27 11:38:23,982 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|modeling_utils.py:2041] 2023-01-27 11:38:24,134 >> loading weights file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/eed737dd80585a756b0286a093059c2b4403b98a17ac2cb50cda7799c653fc11.e38140a56995392eade33ad2835bb905412b65ba305475bd577c00edb10c45d9\n",
            "[WARNING|modeling_utils.py:2425] 2023-01-27 11:38:28,221 >> Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMultipleChoice: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2437] 2023-01-27 11:38:28,221 >> Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:__main__:The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-02f21b54dd4b6d05/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2c2b34a77ff2b663.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-02f21b54dd4b6d05/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2872e211f0acfa2d.arrow\n",
            "  0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-02f21b54dd4b6d05/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6ed81c2461610fad.arrow\n",
            "100% 2/2 [00:00<00:00,  6.34ba/s]\n",
            "Epoch count 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:722] 2023-01-27 11:38:31,433 >> The following columns in the training set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice2, choice4, choice3, choice0, context, choice1. If choice2, choice4, choice3, choice0, context, choice1 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-27 11:38:31,449 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-27 11:38:31,449 >>   Num examples = 9741\n",
            "[INFO|trainer.py:1607] 2023-01-27 11:38:31,449 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2023-01-27 11:38:31,449 >>   Instantaneous batch size per device = 10\n",
            "[INFO|trainer.py:1609] 2023-01-27 11:38:31,449 >>   Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "[INFO|trainer.py:1610] 2023-01-27 11:38:31,449 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-27 11:38:31,449 >>   Total optimization steps = 4875\n",
            "[INFO|integrations.py:607] 2023-01-27 11:38:31,451 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "{'loss': 1.4533, 'learning_rate': 8.974358974358974e-07, 'epoch': 0.51}\n",
            " 20% 975/4875 [06:05<21:38,  3.00it/s][INFO|trainer.py:722] 2023-01-27 11:44:42,241 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice2, choice4, choice3, choice0, context, choice1. If choice2, choice4, choice3, choice0, context, choice1 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:44:42,244 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:44:42,244 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:44:42,244 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:07, 16.56it/s]\u001b[A\n",
            "  4% 5/123 [00:00<00:10, 11.59it/s]\u001b[A\n",
            "  6% 7/123 [00:00<00:10, 11.44it/s]\u001b[A\n",
            "  7% 9/123 [00:00<00:11,  9.99it/s]\u001b[A\n",
            "  9% 11/123 [00:01<00:11,  9.53it/s]\u001b[A\n",
            " 10% 12/123 [00:01<00:11,  9.26it/s]\u001b[A\n",
            " 11% 13/123 [00:01<00:11,  9.27it/s]\u001b[A\n",
            " 11% 14/123 [00:01<00:11,  9.27it/s]\u001b[A\n",
            " 12% 15/123 [00:01<00:12,  9.00it/s]\u001b[A\n",
            " 13% 16/123 [00:01<00:11,  9.12it/s]\u001b[A\n",
            " 14% 17/123 [00:01<00:11,  9.32it/s]\u001b[A\n",
            " 15% 19/123 [00:01<00:10,  9.75it/s]\u001b[A\n",
            " 16% 20/123 [00:02<00:11,  9.28it/s]\u001b[A\n",
            " 18% 22/123 [00:02<00:10,  9.91it/s]\u001b[A\n",
            " 20% 24/123 [00:02<00:09, 10.11it/s]\u001b[A\n",
            " 20% 25/123 [00:02<00:09,  9.93it/s]\u001b[A\n",
            " 21% 26/123 [00:02<00:09,  9.77it/s]\u001b[A\n",
            " 22% 27/123 [00:02<00:10,  9.35it/s]\u001b[A\n",
            " 24% 29/123 [00:02<00:09,  9.70it/s]\u001b[A\n",
            " 25% 31/123 [00:03<00:09,  9.71it/s]\u001b[A\n",
            " 26% 32/123 [00:03<00:10,  9.01it/s]\u001b[A\n",
            " 28% 34/123 [00:03<00:09,  9.38it/s]\u001b[A\n",
            " 29% 36/123 [00:03<00:08,  9.89it/s]\u001b[A\n",
            " 30% 37/123 [00:03<00:08,  9.80it/s]\u001b[A\n",
            " 32% 39/123 [00:04<00:08,  9.38it/s]\u001b[A\n",
            " 33% 41/123 [00:04<00:08,  9.60it/s]\u001b[A\n",
            " 35% 43/123 [00:04<00:08,  9.81it/s]\u001b[A\n",
            " 36% 44/123 [00:04<00:08,  9.21it/s]\u001b[A\n",
            " 37% 45/123 [00:04<00:08,  9.01it/s]\u001b[A\n",
            " 38% 47/123 [00:04<00:08,  9.37it/s]\u001b[A\n",
            " 39% 48/123 [00:04<00:07,  9.41it/s]\u001b[A\n",
            " 40% 49/123 [00:05<00:07,  9.50it/s]\u001b[A\n",
            " 41% 50/123 [00:05<00:07,  9.58it/s]\u001b[A\n",
            " 41% 51/123 [00:05<00:07,  9.35it/s]\u001b[A\n",
            " 43% 53/123 [00:05<00:07,  9.32it/s]\u001b[A\n",
            " 44% 54/123 [00:05<00:07,  9.42it/s]\u001b[A\n",
            " 45% 55/123 [00:05<00:07,  9.51it/s]\u001b[A\n",
            " 46% 56/123 [00:05<00:07,  9.54it/s]\u001b[A\n",
            " 46% 57/123 [00:05<00:06,  9.52it/s]\u001b[A\n",
            " 47% 58/123 [00:06<00:06,  9.59it/s]\u001b[A\n",
            " 48% 59/123 [00:06<00:06,  9.60it/s]\u001b[A\n",
            " 49% 60/123 [00:06<00:06,  9.61it/s]\u001b[A\n",
            " 50% 61/123 [00:06<00:06,  9.21it/s]\u001b[A\n",
            " 50% 62/123 [00:06<00:07,  8.58it/s]\u001b[A\n",
            " 51% 63/123 [00:06<00:07,  7.87it/s]\u001b[A\n",
            " 52% 64/123 [00:06<00:07,  7.93it/s]\u001b[A\n",
            " 53% 65/123 [00:06<00:07,  7.95it/s]\u001b[A\n",
            " 54% 66/123 [00:06<00:06,  8.31it/s]\u001b[A\n",
            " 54% 67/123 [00:07<00:06,  8.72it/s]\u001b[A\n",
            " 56% 69/123 [00:07<00:06,  8.75it/s]\u001b[A\n",
            " 57% 70/123 [00:07<00:05,  8.88it/s]\u001b[A\n",
            " 58% 71/123 [00:07<00:06,  8.64it/s]\u001b[A\n",
            " 59% 72/123 [00:07<00:05,  8.85it/s]\u001b[A\n",
            " 59% 73/123 [00:07<00:05,  8.97it/s]\u001b[A\n",
            " 60% 74/123 [00:07<00:05,  8.76it/s]\u001b[A\n",
            " 61% 75/123 [00:07<00:05,  8.91it/s]\u001b[A\n",
            " 62% 76/123 [00:08<00:05,  8.86it/s]\u001b[A\n",
            " 63% 78/123 [00:08<00:04,  9.38it/s]\u001b[A\n",
            " 64% 79/123 [00:08<00:04,  9.09it/s]\u001b[A\n",
            " 65% 80/123 [00:08<00:04,  8.99it/s]\u001b[A\n",
            " 67% 82/123 [00:08<00:04,  9.74it/s]\u001b[A\n",
            " 67% 83/123 [00:08<00:04,  9.48it/s]\u001b[A\n",
            " 68% 84/123 [00:08<00:04,  9.15it/s]\u001b[A\n",
            " 69% 85/123 [00:09<00:04,  9.27it/s]\u001b[A\n",
            " 70% 86/123 [00:09<00:04,  8.98it/s]\u001b[A\n",
            " 72% 88/123 [00:09<00:03,  9.75it/s]\u001b[A\n",
            " 72% 89/123 [00:09<00:03,  9.63it/s]\u001b[A\n",
            " 73% 90/123 [00:09<00:03,  9.54it/s]\u001b[A\n",
            " 74% 91/123 [00:09<00:03,  8.85it/s]\u001b[A\n",
            " 75% 92/123 [00:09<00:03,  8.39it/s]\u001b[A\n",
            " 76% 93/123 [00:09<00:03,  8.28it/s]\u001b[A\n",
            " 77% 95/123 [00:10<00:03,  8.74it/s]\u001b[A\n",
            " 78% 96/123 [00:10<00:03,  8.87it/s]\u001b[A\n",
            " 79% 97/123 [00:10<00:02,  9.09it/s]\u001b[A\n",
            " 80% 98/123 [00:10<00:03,  7.88it/s]\u001b[A\n",
            " 80% 99/123 [00:10<00:02,  8.22it/s]\u001b[A\n",
            " 81% 100/123 [00:10<00:02,  8.49it/s]\u001b[A\n",
            " 82% 101/123 [00:10<00:02,  8.70it/s]\u001b[A\n",
            " 83% 102/123 [00:10<00:02,  8.86it/s]\u001b[A\n",
            " 84% 103/123 [00:11<00:02,  9.02it/s]\u001b[A\n",
            " 85% 104/123 [00:11<00:02,  8.56it/s]\u001b[A\n",
            " 86% 106/123 [00:11<00:01,  9.27it/s]\u001b[A\n",
            " 88% 108/123 [00:11<00:01,  9.29it/s]\u001b[A\n",
            " 89% 110/123 [00:11<00:01,  9.83it/s]\u001b[A\n",
            " 91% 112/123 [00:12<00:01, 10.00it/s]\u001b[A\n",
            " 92% 113/123 [00:12<00:01,  9.93it/s]\u001b[A\n",
            " 93% 114/123 [00:12<00:00,  9.86it/s]\u001b[A\n",
            " 93% 115/123 [00:12<00:00,  9.84it/s]\u001b[A\n",
            " 94% 116/123 [00:12<00:00,  9.74it/s]\u001b[A\n",
            " 95% 117/123 [00:12<00:00,  9.61it/s]\u001b[A\n",
            " 97% 119/123 [00:12<00:00,  9.54it/s]\u001b[A\n",
            " 98% 120/123 [00:12<00:00,  9.33it/s]\u001b[A\n",
            " 98% 121/123 [00:12<00:00,  9.44it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6945673823356628, 'eval_accuracy': 0.7731, 'eval_runtime': 13.2256, 'eval_samples_per_second': 92.321, 'eval_steps_per_second': 9.3, 'epoch': 1.0}\n",
            " 20% 975/4875 [06:18<21:38,  3.00it/s]\n",
            "100% 123/123 [00:13<00:00,  9.39it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-27 11:44:55,471 >> Saving model checkpoint to saved/csqa/mcq/deberta-large/checkpoint-975\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 11:44:55,472 >> Configuration saved in saved/csqa/mcq/deberta-large/checkpoint-975/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 11:44:58,277 >> Model weights saved in saved/csqa/mcq/deberta-large/checkpoint-975/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 11:44:58,278 >> tokenizer config file saved in saved/csqa/mcq/deberta-large/checkpoint-975/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 11:44:58,278 >> Special tokens file saved in saved/csqa/mcq/deberta-large/checkpoint-975/special_tokens_map.json\n",
            "{'loss': 0.9411, 'learning_rate': 7.948717948717948e-07, 'epoch': 1.03}\n",
            "{'loss': 0.7778, 'learning_rate': 6.923076923076922e-07, 'epoch': 1.54}\n",
            " 40% 1950/4875 [12:34<15:56,  3.06it/s][INFO|trainer.py:722] 2023-01-27 11:51:10,909 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice2, choice4, choice3, choice0, context, choice1. If choice2, choice4, choice3, choice0, context, choice1 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:51:10,911 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:51:10,912 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:51:10,912 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:07, 16.54it/s]\u001b[A\n",
            "  4% 5/123 [00:00<00:10, 11.58it/s]\u001b[A\n",
            "  6% 7/123 [00:00<00:10, 11.44it/s]\u001b[A\n",
            "  7% 9/123 [00:00<00:11,  9.99it/s]\u001b[A\n",
            "  9% 11/123 [00:01<00:11,  9.54it/s]\u001b[A\n",
            " 10% 12/123 [00:01<00:11,  9.25it/s]\u001b[A\n",
            " 11% 13/123 [00:01<00:11,  9.25it/s]\u001b[A\n",
            " 11% 14/123 [00:01<00:11,  9.23it/s]\u001b[A\n",
            " 12% 15/123 [00:01<00:12,  8.96it/s]\u001b[A\n",
            " 13% 16/123 [00:01<00:11,  9.09it/s]\u001b[A\n",
            " 14% 17/123 [00:01<00:11,  9.29it/s]\u001b[A\n",
            " 15% 19/123 [00:01<00:10,  9.73it/s]\u001b[A\n",
            " 16% 20/123 [00:02<00:11,  9.26it/s]\u001b[A\n",
            " 18% 22/123 [00:02<00:10,  9.87it/s]\u001b[A\n",
            " 20% 24/123 [00:02<00:09, 10.08it/s]\u001b[A\n",
            " 20% 25/123 [00:02<00:09,  9.90it/s]\u001b[A\n",
            " 21% 26/123 [00:02<00:09,  9.75it/s]\u001b[A\n",
            " 22% 27/123 [00:02<00:10,  9.33it/s]\u001b[A\n",
            " 24% 29/123 [00:02<00:09,  9.70it/s]\u001b[A\n",
            " 25% 31/123 [00:03<00:09,  9.72it/s]\u001b[A\n",
            " 26% 32/123 [00:03<00:10,  8.93it/s]\u001b[A\n",
            " 28% 34/123 [00:03<00:09,  9.32it/s]\u001b[A\n",
            " 29% 36/123 [00:03<00:08,  9.84it/s]\u001b[A\n",
            " 30% 37/123 [00:03<00:08,  9.76it/s]\u001b[A\n",
            " 32% 39/123 [00:04<00:08,  9.36it/s]\u001b[A\n",
            " 33% 41/123 [00:04<00:08,  9.59it/s]\u001b[A\n",
            " 35% 43/123 [00:04<00:08,  9.78it/s]\u001b[A\n",
            " 36% 44/123 [00:04<00:08,  9.18it/s]\u001b[A\n",
            " 37% 45/123 [00:04<00:08,  8.99it/s]\u001b[A\n",
            " 38% 47/123 [00:04<00:08,  9.36it/s]\u001b[A\n",
            " 39% 48/123 [00:04<00:07,  9.41it/s]\u001b[A\n",
            " 40% 49/123 [00:05<00:07,  9.49it/s]\u001b[A\n",
            " 41% 50/123 [00:05<00:07,  9.56it/s]\u001b[A\n",
            " 41% 51/123 [00:05<00:07,  9.32it/s]\u001b[A\n",
            " 43% 53/123 [00:05<00:07,  9.30it/s]\u001b[A\n",
            " 44% 54/123 [00:05<00:07,  9.41it/s]\u001b[A\n",
            " 45% 55/123 [00:05<00:07,  9.50it/s]\u001b[A\n",
            " 46% 56/123 [00:05<00:07,  9.53it/s]\u001b[A\n",
            " 46% 57/123 [00:05<00:06,  9.62it/s]\u001b[A\n",
            " 47% 58/123 [00:06<00:06,  9.66it/s]\u001b[A\n",
            " 48% 59/123 [00:06<00:06,  9.65it/s]\u001b[A\n",
            " 49% 60/123 [00:06<00:06,  9.64it/s]\u001b[A\n",
            " 50% 61/123 [00:06<00:06,  9.22it/s]\u001b[A\n",
            " 50% 62/123 [00:06<00:07,  8.59it/s]\u001b[A\n",
            " 51% 63/123 [00:06<00:07,  7.87it/s]\u001b[A\n",
            " 52% 64/123 [00:06<00:07,  7.93it/s]\u001b[A\n",
            " 53% 65/123 [00:06<00:07,  7.96it/s]\u001b[A\n",
            " 54% 66/123 [00:06<00:06,  8.31it/s]\u001b[A\n",
            " 54% 67/123 [00:07<00:06,  8.72it/s]\u001b[A\n",
            " 56% 69/123 [00:07<00:06,  8.75it/s]\u001b[A\n",
            " 57% 70/123 [00:07<00:05,  8.87it/s]\u001b[A\n",
            " 58% 71/123 [00:07<00:06,  8.63it/s]\u001b[A\n",
            " 59% 72/123 [00:07<00:05,  8.84it/s]\u001b[A\n",
            " 59% 73/123 [00:07<00:05,  8.96it/s]\u001b[A\n",
            " 60% 74/123 [00:07<00:05,  8.76it/s]\u001b[A\n",
            " 61% 75/123 [00:07<00:05,  8.90it/s]\u001b[A\n",
            " 62% 76/123 [00:08<00:05,  8.86it/s]\u001b[A\n",
            " 63% 78/123 [00:08<00:04,  9.36it/s]\u001b[A\n",
            " 64% 79/123 [00:08<00:04,  9.06it/s]\u001b[A\n",
            " 65% 80/123 [00:08<00:04,  8.98it/s]\u001b[A\n",
            " 67% 82/123 [00:08<00:04,  9.71it/s]\u001b[A\n",
            " 67% 83/123 [00:08<00:04,  9.46it/s]\u001b[A\n",
            " 68% 84/123 [00:08<00:04,  9.13it/s]\u001b[A\n",
            " 69% 85/123 [00:09<00:04,  9.25it/s]\u001b[A\n",
            " 70% 86/123 [00:09<00:04,  8.96it/s]\u001b[A\n",
            " 72% 88/123 [00:09<00:03,  9.72it/s]\u001b[A\n",
            " 72% 89/123 [00:09<00:03,  9.61it/s]\u001b[A\n",
            " 73% 90/123 [00:09<00:03,  9.52it/s]\u001b[A\n",
            " 74% 91/123 [00:09<00:03,  8.83it/s]\u001b[A\n",
            " 75% 92/123 [00:09<00:03,  8.38it/s]\u001b[A\n",
            " 76% 93/123 [00:09<00:03,  8.28it/s]\u001b[A\n",
            " 77% 95/123 [00:10<00:03,  8.77it/s]\u001b[A\n",
            " 78% 96/123 [00:10<00:03,  8.89it/s]\u001b[A\n",
            " 79% 97/123 [00:10<00:02,  9.10it/s]\u001b[A\n",
            " 80% 98/123 [00:10<00:03,  7.88it/s]\u001b[A\n",
            " 80% 99/123 [00:10<00:02,  8.22it/s]\u001b[A\n",
            " 81% 100/123 [00:10<00:02,  8.49it/s]\u001b[A\n",
            " 82% 101/123 [00:10<00:02,  8.69it/s]\u001b[A\n",
            " 83% 102/123 [00:11<00:02,  8.83it/s]\u001b[A\n",
            " 84% 103/123 [00:11<00:02,  9.01it/s]\u001b[A\n",
            " 85% 104/123 [00:11<00:02,  8.55it/s]\u001b[A\n",
            " 86% 106/123 [00:11<00:01,  9.25it/s]\u001b[A\n",
            " 88% 108/123 [00:11<00:01,  9.27it/s]\u001b[A\n",
            " 89% 110/123 [00:11<00:01,  9.82it/s]\u001b[A\n",
            " 91% 112/123 [00:12<00:01, 10.00it/s]\u001b[A\n",
            " 92% 113/123 [00:12<00:01,  9.94it/s]\u001b[A\n",
            " 93% 114/123 [00:12<00:00,  9.86it/s]\u001b[A\n",
            " 93% 115/123 [00:12<00:00,  9.84it/s]\u001b[A\n",
            " 94% 116/123 [00:12<00:00,  9.74it/s]\u001b[A\n",
            " 95% 117/123 [00:12<00:00,  9.61it/s]\u001b[A\n",
            " 97% 119/123 [00:12<00:00,  9.54it/s]\u001b[A\n",
            " 98% 120/123 [00:12<00:00,  9.34it/s]\u001b[A\n",
            " 98% 121/123 [00:12<00:00,  9.45it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.5646516680717468, 'eval_accuracy': 0.8116, 'eval_runtime': 13.2396, 'eval_samples_per_second': 92.223, 'eval_steps_per_second': 9.29, 'epoch': 2.0}\n",
            " 40% 1950/4875 [12:47<15:56,  3.06it/s]\n",
            "100% 123/123 [00:13<00:00,  9.40it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-27 11:51:24,153 >> Saving model checkpoint to saved/csqa/mcq/deberta-large/checkpoint-1950\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 11:51:24,154 >> Configuration saved in saved/csqa/mcq/deberta-large/checkpoint-1950/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 11:51:26,750 >> Model weights saved in saved/csqa/mcq/deberta-large/checkpoint-1950/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 11:51:26,751 >> tokenizer config file saved in saved/csqa/mcq/deberta-large/checkpoint-1950/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 11:51:26,751 >> Special tokens file saved in saved/csqa/mcq/deberta-large/checkpoint-1950/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 11:51:35,174 >> Deleting older checkpoint [saved/csqa/mcq/deberta-large/checkpoint-975] due to args.save_total_limit\n",
            "{'loss': 0.7062, 'learning_rate': 5.897435897435898e-07, 'epoch': 2.05}\n",
            "{'loss': 0.6305, 'learning_rate': 4.871794871794871e-07, 'epoch': 2.56}\n",
            " 60% 2925/4875 [19:04<10:11,  3.19it/s][INFO|trainer.py:722] 2023-01-27 11:57:41,203 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice2, choice4, choice3, choice0, context, choice1. If choice2, choice4, choice3, choice0, context, choice1 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:57:41,206 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:57:41,206 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:57:41,206 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:07, 16.42it/s]\u001b[A\n",
            "  4% 5/123 [00:00<00:10, 11.55it/s]\u001b[A\n",
            "  6% 7/123 [00:00<00:10, 11.41it/s]\u001b[A\n",
            "  7% 9/123 [00:00<00:11,  9.97it/s]\u001b[A\n",
            "  9% 11/123 [00:01<00:11,  9.51it/s]\u001b[A\n",
            " 10% 12/123 [00:01<00:12,  9.24it/s]\u001b[A\n",
            " 11% 13/123 [00:01<00:11,  9.25it/s]\u001b[A\n",
            " 11% 14/123 [00:01<00:11,  9.25it/s]\u001b[A\n",
            " 12% 15/123 [00:01<00:12,  8.98it/s]\u001b[A\n",
            " 13% 16/123 [00:01<00:11,  9.10it/s]\u001b[A\n",
            " 14% 17/123 [00:01<00:11,  9.30it/s]\u001b[A\n",
            " 15% 19/123 [00:01<00:10,  9.73it/s]\u001b[A\n",
            " 16% 20/123 [00:02<00:11,  9.27it/s]\u001b[A\n",
            " 18% 22/123 [00:02<00:10,  9.89it/s]\u001b[A\n",
            " 20% 24/123 [00:02<00:09, 10.10it/s]\u001b[A\n",
            " 20% 25/123 [00:02<00:09,  9.91it/s]\u001b[A\n",
            " 21% 26/123 [00:02<00:09,  9.76it/s]\u001b[A\n",
            " 22% 27/123 [00:02<00:10,  9.34it/s]\u001b[A\n",
            " 24% 29/123 [00:02<00:09,  9.69it/s]\u001b[A\n",
            " 25% 31/123 [00:03<00:09,  9.71it/s]\u001b[A\n",
            " 26% 32/123 [00:03<00:10,  9.00it/s]\u001b[A\n",
            " 28% 34/123 [00:03<00:09,  9.38it/s]\u001b[A\n",
            " 29% 36/123 [00:03<00:08,  9.88it/s]\u001b[A\n",
            " 30% 37/123 [00:03<00:08,  9.79it/s]\u001b[A\n",
            " 32% 39/123 [00:04<00:08,  9.37it/s]\u001b[A\n",
            " 33% 41/123 [00:04<00:08,  9.58it/s]\u001b[A\n",
            " 35% 43/123 [00:04<00:08,  9.79it/s]\u001b[A\n",
            " 36% 44/123 [00:04<00:08,  9.20it/s]\u001b[A\n",
            " 37% 45/123 [00:04<00:08,  9.01it/s]\u001b[A\n",
            " 38% 47/123 [00:04<00:08,  9.38it/s]\u001b[A\n",
            " 39% 48/123 [00:04<00:07,  9.43it/s]\u001b[A\n",
            " 40% 49/123 [00:05<00:07,  9.51it/s]\u001b[A\n",
            " 41% 50/123 [00:05<00:07,  9.58it/s]\u001b[A\n",
            " 41% 51/123 [00:05<00:07,  9.34it/s]\u001b[A\n",
            " 43% 53/123 [00:05<00:07,  9.32it/s]\u001b[A\n",
            " 44% 54/123 [00:05<00:07,  9.42it/s]\u001b[A\n",
            " 45% 55/123 [00:05<00:07,  9.51it/s]\u001b[A\n",
            " 46% 56/123 [00:05<00:07,  9.54it/s]\u001b[A\n",
            " 46% 57/123 [00:05<00:06,  9.62it/s]\u001b[A\n",
            " 47% 58/123 [00:06<00:06,  9.67it/s]\u001b[A\n",
            " 48% 59/123 [00:06<00:06,  9.65it/s]\u001b[A\n",
            " 49% 60/123 [00:06<00:06,  9.59it/s]\u001b[A\n",
            " 50% 61/123 [00:06<00:06,  9.19it/s]\u001b[A\n",
            " 50% 62/123 [00:06<00:07,  8.57it/s]\u001b[A\n",
            " 51% 63/123 [00:06<00:07,  7.86it/s]\u001b[A\n",
            " 52% 64/123 [00:06<00:07,  7.91it/s]\u001b[A\n",
            " 53% 65/123 [00:06<00:07,  7.95it/s]\u001b[A\n",
            " 54% 66/123 [00:06<00:06,  8.30it/s]\u001b[A\n",
            " 54% 67/123 [00:07<00:06,  8.69it/s]\u001b[A\n",
            " 56% 69/123 [00:07<00:06,  8.70it/s]\u001b[A\n",
            " 57% 70/123 [00:07<00:05,  8.84it/s]\u001b[A\n",
            " 58% 71/123 [00:07<00:06,  8.61it/s]\u001b[A\n",
            " 59% 72/123 [00:07<00:05,  8.82it/s]\u001b[A\n",
            " 59% 73/123 [00:07<00:05,  8.94it/s]\u001b[A\n",
            " 60% 74/123 [00:07<00:05,  8.74it/s]\u001b[A\n",
            " 61% 75/123 [00:07<00:05,  8.88it/s]\u001b[A\n",
            " 62% 76/123 [00:08<00:05,  8.84it/s]\u001b[A\n",
            " 63% 78/123 [00:08<00:04,  9.37it/s]\u001b[A\n",
            " 64% 79/123 [00:08<00:04,  9.08it/s]\u001b[A\n",
            " 65% 80/123 [00:08<00:04,  8.98it/s]\u001b[A\n",
            " 67% 82/123 [00:08<00:04,  9.72it/s]\u001b[A\n",
            " 67% 83/123 [00:08<00:04,  9.46it/s]\u001b[A\n",
            " 68% 84/123 [00:08<00:04,  9.12it/s]\u001b[A\n",
            " 69% 85/123 [00:09<00:04,  9.24it/s]\u001b[A\n",
            " 70% 86/123 [00:09<00:04,  8.96it/s]\u001b[A\n",
            " 72% 88/123 [00:09<00:03,  9.74it/s]\u001b[A\n",
            " 72% 89/123 [00:09<00:03,  9.62it/s]\u001b[A\n",
            " 73% 90/123 [00:09<00:03,  9.52it/s]\u001b[A\n",
            " 74% 91/123 [00:09<00:03,  8.84it/s]\u001b[A\n",
            " 75% 92/123 [00:09<00:03,  8.38it/s]\u001b[A\n",
            " 76% 93/123 [00:09<00:03,  8.27it/s]\u001b[A\n",
            " 77% 95/123 [00:10<00:03,  8.77it/s]\u001b[A\n",
            " 78% 96/123 [00:10<00:03,  8.88it/s]\u001b[A\n",
            " 79% 97/123 [00:10<00:02,  9.10it/s]\u001b[A\n",
            " 80% 98/123 [00:10<00:03,  7.88it/s]\u001b[A\n",
            " 80% 99/123 [00:10<00:02,  8.22it/s]\u001b[A\n",
            " 81% 100/123 [00:10<00:02,  8.49it/s]\u001b[A\n",
            " 82% 101/123 [00:10<00:02,  8.70it/s]\u001b[A\n",
            " 83% 102/123 [00:11<00:02,  8.86it/s]\u001b[A\n",
            " 84% 103/123 [00:11<00:02,  9.03it/s]\u001b[A\n",
            " 85% 104/123 [00:11<00:02,  8.56it/s]\u001b[A\n",
            " 86% 106/123 [00:11<00:01,  9.27it/s]\u001b[A\n",
            " 88% 108/123 [00:11<00:01,  9.28it/s]\u001b[A\n",
            " 89% 110/123 [00:11<00:01,  9.83it/s]\u001b[A\n",
            " 91% 112/123 [00:12<00:01, 10.01it/s]\u001b[A\n",
            " 92% 113/123 [00:12<00:01,  9.97it/s]\u001b[A\n",
            " 93% 114/123 [00:12<00:00,  9.89it/s]\u001b[A\n",
            " 93% 115/123 [00:12<00:00,  9.86it/s]\u001b[A\n",
            " 94% 116/123 [00:12<00:00,  9.76it/s]\u001b[A\n",
            " 95% 117/123 [00:12<00:00,  9.62it/s]\u001b[A\n",
            " 97% 119/123 [00:12<00:00,  9.55it/s]\u001b[A\n",
            " 98% 120/123 [00:12<00:00,  9.34it/s]\u001b[A\n",
            " 98% 121/123 [00:12<00:00,  9.45it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.5163545608520508, 'eval_accuracy': 0.8182, 'eval_runtime': 13.2344, 'eval_samples_per_second': 92.26, 'eval_steps_per_second': 9.294, 'epoch': 3.0}\n",
            " 60% 2925/4875 [19:17<10:11,  3.19it/s]\n",
            "100% 123/123 [00:13<00:00,  9.40it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-27 11:57:54,442 >> Saving model checkpoint to saved/csqa/mcq/deberta-large/checkpoint-2925\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 11:57:54,443 >> Configuration saved in saved/csqa/mcq/deberta-large/checkpoint-2925/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 11:57:57,181 >> Model weights saved in saved/csqa/mcq/deberta-large/checkpoint-2925/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 11:57:57,182 >> tokenizer config file saved in saved/csqa/mcq/deberta-large/checkpoint-2925/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 11:57:57,183 >> Special tokens file saved in saved/csqa/mcq/deberta-large/checkpoint-2925/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 11:58:05,386 >> Deleting older checkpoint [saved/csqa/mcq/deberta-large/checkpoint-1950] due to args.save_total_limit\n",
            "{'loss': 0.6025, 'learning_rate': 3.8461538461538463e-07, 'epoch': 3.08}\n",
            "{'loss': 0.5546, 'learning_rate': 2.8205128205128203e-07, 'epoch': 3.59}\n",
            " 80% 3900/4875 [25:34<05:05,  3.20it/s][INFO|trainer.py:722] 2023-01-27 12:04:11,028 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice2, choice4, choice3, choice0, context, choice1. If choice2, choice4, choice3, choice0, context, choice1 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:04:11,030 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:04:11,030 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:04:11,030 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:07, 16.50it/s]\u001b[A\n",
            "  4% 5/123 [00:00<00:10, 11.56it/s]\u001b[A\n",
            "  6% 7/123 [00:00<00:10, 11.42it/s]\u001b[A\n",
            "  7% 9/123 [00:00<00:11,  9.98it/s]\u001b[A\n",
            "  9% 11/123 [00:01<00:11,  9.52it/s]\u001b[A\n",
            " 10% 12/123 [00:01<00:12,  9.25it/s]\u001b[A\n",
            " 11% 13/123 [00:01<00:11,  9.25it/s]\u001b[A\n",
            " 11% 14/123 [00:01<00:11,  9.23it/s]\u001b[A\n",
            " 12% 15/123 [00:01<00:12,  8.91it/s]\u001b[A\n",
            " 13% 16/123 [00:01<00:11,  9.05it/s]\u001b[A\n",
            " 14% 17/123 [00:01<00:11,  9.26it/s]\u001b[A\n",
            " 15% 19/123 [00:01<00:10,  9.71it/s]\u001b[A\n",
            " 16% 20/123 [00:02<00:11,  9.23it/s]\u001b[A\n",
            " 18% 22/123 [00:02<00:10,  9.87it/s]\u001b[A\n",
            " 20% 24/123 [00:02<00:09, 10.09it/s]\u001b[A\n",
            " 20% 25/123 [00:02<00:09,  9.91it/s]\u001b[A\n",
            " 21% 26/123 [00:02<00:09,  9.75it/s]\u001b[A\n",
            " 22% 27/123 [00:02<00:10,  9.34it/s]\u001b[A\n",
            " 24% 29/123 [00:02<00:09,  9.70it/s]\u001b[A\n",
            " 25% 31/123 [00:03<00:09,  9.72it/s]\u001b[A\n",
            " 26% 32/123 [00:03<00:10,  9.02it/s]\u001b[A\n",
            " 28% 34/123 [00:03<00:09,  9.39it/s]\u001b[A\n",
            " 29% 36/123 [00:03<00:08,  9.89it/s]\u001b[A\n",
            " 30% 37/123 [00:03<00:08,  9.81it/s]\u001b[A\n",
            " 32% 39/123 [00:04<00:08,  9.39it/s]\u001b[A\n",
            " 33% 41/123 [00:04<00:08,  9.61it/s]\u001b[A\n",
            " 35% 43/123 [00:04<00:08,  9.82it/s]\u001b[A\n",
            " 36% 44/123 [00:04<00:08,  9.22it/s]\u001b[A\n",
            " 37% 45/123 [00:04<00:08,  9.02it/s]\u001b[A\n",
            " 38% 47/123 [00:04<00:08,  9.39it/s]\u001b[A\n",
            " 39% 48/123 [00:04<00:07,  9.44it/s]\u001b[A\n",
            " 40% 49/123 [00:05<00:07,  9.52it/s]\u001b[A\n",
            " 41% 50/123 [00:05<00:07,  9.59it/s]\u001b[A\n",
            " 41% 51/123 [00:05<00:07,  9.35it/s]\u001b[A\n",
            " 43% 53/123 [00:05<00:07,  9.33it/s]\u001b[A\n",
            " 44% 54/123 [00:05<00:07,  9.40it/s]\u001b[A\n",
            " 45% 55/123 [00:05<00:07,  9.49it/s]\u001b[A\n",
            " 46% 56/123 [00:05<00:07,  9.52it/s]\u001b[A\n",
            " 46% 57/123 [00:05<00:06,  9.61it/s]\u001b[A\n",
            " 47% 58/123 [00:06<00:06,  9.63it/s]\u001b[A\n",
            " 48% 59/123 [00:06<00:06,  9.62it/s]\u001b[A\n",
            " 49% 60/123 [00:06<00:06,  9.63it/s]\u001b[A\n",
            " 50% 61/123 [00:06<00:06,  9.21it/s]\u001b[A\n",
            " 50% 62/123 [00:06<00:07,  8.58it/s]\u001b[A\n",
            " 51% 63/123 [00:06<00:07,  7.87it/s]\u001b[A\n",
            " 52% 64/123 [00:06<00:07,  7.92it/s]\u001b[A\n",
            " 53% 65/123 [00:06<00:07,  7.95it/s]\u001b[A\n",
            " 54% 66/123 [00:06<00:06,  8.31it/s]\u001b[A\n",
            " 54% 67/123 [00:07<00:06,  8.72it/s]\u001b[A\n",
            " 56% 69/123 [00:07<00:06,  8.75it/s]\u001b[A\n",
            " 57% 70/123 [00:07<00:05,  8.87it/s]\u001b[A\n",
            " 58% 71/123 [00:07<00:06,  8.63it/s]\u001b[A\n",
            " 59% 72/123 [00:07<00:05,  8.84it/s]\u001b[A\n",
            " 59% 73/123 [00:07<00:05,  8.96it/s]\u001b[A\n",
            " 60% 74/123 [00:07<00:05,  8.76it/s]\u001b[A\n",
            " 61% 75/123 [00:07<00:05,  8.89it/s]\u001b[A\n",
            " 62% 76/123 [00:08<00:05,  8.81it/s]\u001b[A\n",
            " 63% 78/123 [00:08<00:04,  9.35it/s]\u001b[A\n",
            " 64% 79/123 [00:08<00:04,  9.06it/s]\u001b[A\n",
            " 65% 80/123 [00:08<00:04,  8.97it/s]\u001b[A\n",
            " 67% 82/123 [00:08<00:04,  9.72it/s]\u001b[A\n",
            " 67% 83/123 [00:08<00:04,  9.46it/s]\u001b[A\n",
            " 68% 84/123 [00:08<00:04,  9.14it/s]\u001b[A\n",
            " 69% 85/123 [00:09<00:04,  9.26it/s]\u001b[A\n",
            " 70% 86/123 [00:09<00:04,  8.98it/s]\u001b[A\n",
            " 72% 88/123 [00:09<00:03,  9.75it/s]\u001b[A\n",
            " 72% 89/123 [00:09<00:03,  9.63it/s]\u001b[A\n",
            " 73% 90/123 [00:09<00:03,  9.54it/s]\u001b[A\n",
            " 74% 91/123 [00:09<00:03,  8.84it/s]\u001b[A\n",
            " 75% 92/123 [00:09<00:03,  8.38it/s]\u001b[A\n",
            " 76% 93/123 [00:09<00:03,  8.28it/s]\u001b[A\n",
            " 77% 95/123 [00:10<00:03,  8.77it/s]\u001b[A\n",
            " 78% 96/123 [00:10<00:03,  8.88it/s]\u001b[A\n",
            " 79% 97/123 [00:10<00:02,  9.09it/s]\u001b[A\n",
            " 80% 98/123 [00:10<00:03,  7.87it/s]\u001b[A\n",
            " 80% 99/123 [00:10<00:02,  8.21it/s]\u001b[A\n",
            " 81% 100/123 [00:10<00:02,  8.49it/s]\u001b[A\n",
            " 82% 101/123 [00:10<00:02,  8.71it/s]\u001b[A\n",
            " 83% 102/123 [00:11<00:02,  8.86it/s]\u001b[A\n",
            " 84% 103/123 [00:11<00:02,  9.04it/s]\u001b[A\n",
            " 85% 104/123 [00:11<00:02,  8.56it/s]\u001b[A\n",
            " 86% 106/123 [00:11<00:01,  9.27it/s]\u001b[A\n",
            " 88% 108/123 [00:11<00:01,  9.25it/s]\u001b[A\n",
            " 89% 110/123 [00:11<00:01,  9.81it/s]\u001b[A\n",
            " 91% 112/123 [00:12<00:01, 10.00it/s]\u001b[A\n",
            " 92% 113/123 [00:12<00:01,  9.95it/s]\u001b[A\n",
            " 93% 114/123 [00:12<00:00,  9.88it/s]\u001b[A\n",
            " 93% 115/123 [00:12<00:00,  9.85it/s]\u001b[A\n",
            " 94% 116/123 [00:12<00:00,  9.75it/s]\u001b[A\n",
            " 95% 117/123 [00:12<00:00,  9.62it/s]\u001b[A\n",
            " 97% 119/123 [00:12<00:00,  9.46it/s]\u001b[A\n",
            " 98% 120/123 [00:12<00:00,  9.27it/s]\u001b[A\n",
            " 98% 121/123 [00:12<00:00,  9.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.5056940317153931, 'eval_accuracy': 0.8206, 'eval_runtime': 13.2398, 'eval_samples_per_second': 92.222, 'eval_steps_per_second': 9.29, 'epoch': 4.0}\n",
            " 80% 3900/4875 [25:47<05:05,  3.20it/s]\n",
            "100% 123/123 [00:13<00:00,  9.35it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-27 12:04:24,272 >> Saving model checkpoint to saved/csqa/mcq/deberta-large/checkpoint-3900\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 12:04:24,273 >> Configuration saved in saved/csqa/mcq/deberta-large/checkpoint-3900/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 12:04:27,016 >> Model weights saved in saved/csqa/mcq/deberta-large/checkpoint-3900/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 12:04:27,017 >> tokenizer config file saved in saved/csqa/mcq/deberta-large/checkpoint-3900/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 12:04:27,017 >> Special tokens file saved in saved/csqa/mcq/deberta-large/checkpoint-3900/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 12:04:35,402 >> Deleting older checkpoint [saved/csqa/mcq/deberta-large/checkpoint-2925] due to args.save_total_limit\n",
            "{'loss': 0.5419, 'learning_rate': 1.7948717948717948e-07, 'epoch': 4.1}\n",
            "{'loss': 0.5201, 'learning_rate': 7.692307692307692e-08, 'epoch': 4.62}\n",
            "100% 4875/4875 [32:04<00:00,  3.25it/s][INFO|trainer.py:722] 2023-01-27 12:10:41,671 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice2, choice4, choice3, choice0, context, choice1. If choice2, choice4, choice3, choice0, context, choice1 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:10:41,673 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:10:41,673 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:10:41,673 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:07, 16.55it/s]\u001b[A\n",
            "  4% 5/123 [00:00<00:10, 11.57it/s]\u001b[A\n",
            "  6% 7/123 [00:00<00:10, 11.43it/s]\u001b[A\n",
            "  7% 9/123 [00:00<00:11,  9.99it/s]\u001b[A\n",
            "  9% 11/123 [00:01<00:11,  9.54it/s]\u001b[A\n",
            " 10% 12/123 [00:01<00:11,  9.26it/s]\u001b[A\n",
            " 11% 13/123 [00:01<00:11,  9.26it/s]\u001b[A\n",
            " 11% 14/123 [00:01<00:11,  9.25it/s]\u001b[A\n",
            " 12% 15/123 [00:01<00:12,  8.97it/s]\u001b[A\n",
            " 13% 16/123 [00:01<00:11,  9.09it/s]\u001b[A\n",
            " 14% 17/123 [00:01<00:11,  9.28it/s]\u001b[A\n",
            " 15% 19/123 [00:01<00:10,  9.72it/s]\u001b[A\n",
            " 16% 20/123 [00:02<00:11,  9.26it/s]\u001b[A\n",
            " 18% 22/123 [00:02<00:10,  9.89it/s]\u001b[A\n",
            " 20% 24/123 [00:02<00:09, 10.08it/s]\u001b[A\n",
            " 20% 25/123 [00:02<00:09,  9.89it/s]\u001b[A\n",
            " 21% 26/123 [00:02<00:09,  9.73it/s]\u001b[A\n",
            " 22% 27/123 [00:02<00:10,  9.32it/s]\u001b[A\n",
            " 24% 29/123 [00:02<00:09,  9.68it/s]\u001b[A\n",
            " 25% 31/123 [00:03<00:09,  9.60it/s]\u001b[A\n",
            " 26% 32/123 [00:03<00:10,  8.93it/s]\u001b[A\n",
            " 28% 34/123 [00:03<00:09,  9.31it/s]\u001b[A\n",
            " 29% 36/123 [00:03<00:08,  9.83it/s]\u001b[A\n",
            " 30% 37/123 [00:03<00:08,  9.75it/s]\u001b[A\n",
            " 32% 39/123 [00:04<00:08,  9.35it/s]\u001b[A\n",
            " 33% 41/123 [00:04<00:08,  9.58it/s]\u001b[A\n",
            " 35% 43/123 [00:04<00:08,  9.79it/s]\u001b[A\n",
            " 36% 44/123 [00:04<00:08,  9.20it/s]\u001b[A\n",
            " 37% 45/123 [00:04<00:08,  9.00it/s]\u001b[A\n",
            " 38% 47/123 [00:04<00:08,  9.37it/s]\u001b[A\n",
            " 39% 48/123 [00:04<00:07,  9.41it/s]\u001b[A\n",
            " 40% 49/123 [00:05<00:07,  9.47it/s]\u001b[A\n",
            " 41% 50/123 [00:05<00:07,  9.54it/s]\u001b[A\n",
            " 41% 51/123 [00:05<00:07,  9.31it/s]\u001b[A\n",
            " 43% 53/123 [00:05<00:07,  9.26it/s]\u001b[A\n",
            " 44% 54/123 [00:05<00:07,  9.37it/s]\u001b[A\n",
            " 45% 55/123 [00:05<00:07,  9.47it/s]\u001b[A\n",
            " 46% 56/123 [00:05<00:07,  9.50it/s]\u001b[A\n",
            " 46% 57/123 [00:05<00:06,  9.59it/s]\u001b[A\n",
            " 47% 58/123 [00:06<00:06,  9.63it/s]\u001b[A\n",
            " 48% 59/123 [00:06<00:06,  9.62it/s]\u001b[A\n",
            " 49% 60/123 [00:06<00:06,  9.61it/s]\u001b[A\n",
            " 50% 61/123 [00:06<00:06,  9.20it/s]\u001b[A\n",
            " 50% 62/123 [00:06<00:07,  8.56it/s]\u001b[A\n",
            " 51% 63/123 [00:06<00:07,  7.85it/s]\u001b[A\n",
            " 52% 64/123 [00:06<00:07,  7.90it/s]\u001b[A\n",
            " 53% 65/123 [00:06<00:07,  7.93it/s]\u001b[A\n",
            " 54% 66/123 [00:06<00:06,  8.29it/s]\u001b[A\n",
            " 54% 67/123 [00:07<00:06,  8.69it/s]\u001b[A\n",
            " 56% 69/123 [00:07<00:06,  8.73it/s]\u001b[A\n",
            " 57% 70/123 [00:07<00:05,  8.85it/s]\u001b[A\n",
            " 58% 71/123 [00:07<00:06,  8.61it/s]\u001b[A\n",
            " 59% 72/123 [00:07<00:05,  8.81it/s]\u001b[A\n",
            " 59% 73/123 [00:07<00:05,  8.93it/s]\u001b[A\n",
            " 60% 74/123 [00:07<00:05,  8.73it/s]\u001b[A\n",
            " 61% 75/123 [00:08<00:05,  8.88it/s]\u001b[A\n",
            " 62% 76/123 [00:08<00:05,  8.76it/s]\u001b[A\n",
            " 63% 78/123 [00:08<00:04,  9.32it/s]\u001b[A\n",
            " 64% 79/123 [00:08<00:04,  9.04it/s]\u001b[A\n",
            " 65% 80/123 [00:08<00:04,  8.96it/s]\u001b[A\n",
            " 67% 82/123 [00:08<00:04,  9.71it/s]\u001b[A\n",
            " 67% 83/123 [00:08<00:04,  9.46it/s]\u001b[A\n",
            " 68% 84/123 [00:08<00:04,  9.13it/s]\u001b[A\n",
            " 69% 85/123 [00:09<00:04,  9.25it/s]\u001b[A\n",
            " 70% 86/123 [00:09<00:04,  8.96it/s]\u001b[A\n",
            " 72% 88/123 [00:09<00:03,  9.75it/s]\u001b[A\n",
            " 72% 89/123 [00:09<00:03,  9.60it/s]\u001b[A\n",
            " 73% 90/123 [00:09<00:03,  9.51it/s]\u001b[A\n",
            " 74% 91/123 [00:09<00:03,  8.83it/s]\u001b[A\n",
            " 75% 92/123 [00:09<00:03,  8.37it/s]\u001b[A\n",
            " 76% 93/123 [00:09<00:03,  8.27it/s]\u001b[A\n",
            " 77% 95/123 [00:10<00:03,  8.76it/s]\u001b[A\n",
            " 78% 96/123 [00:10<00:03,  8.88it/s]\u001b[A\n",
            " 79% 97/123 [00:10<00:02,  9.08it/s]\u001b[A\n",
            " 80% 98/123 [00:10<00:03,  7.87it/s]\u001b[A\n",
            " 80% 99/123 [00:10<00:02,  8.20it/s]\u001b[A\n",
            " 81% 100/123 [00:10<00:02,  8.47it/s]\u001b[A\n",
            " 82% 101/123 [00:10<00:02,  8.68it/s]\u001b[A\n",
            " 83% 102/123 [00:11<00:02,  8.85it/s]\u001b[A\n",
            " 84% 103/123 [00:11<00:02,  9.01it/s]\u001b[A\n",
            " 85% 104/123 [00:11<00:02,  8.52it/s]\u001b[A\n",
            " 86% 106/123 [00:11<00:01,  9.22it/s]\u001b[A\n",
            " 88% 108/123 [00:11<00:01,  9.25it/s]\u001b[A\n",
            " 89% 110/123 [00:11<00:01,  9.80it/s]\u001b[A\n",
            " 91% 112/123 [00:12<00:01,  9.99it/s]\u001b[A\n",
            " 92% 113/123 [00:12<00:01,  9.94it/s]\u001b[A\n",
            " 93% 114/123 [00:12<00:00,  9.86it/s]\u001b[A\n",
            " 93% 115/123 [00:12<00:00,  9.82it/s]\u001b[A\n",
            " 94% 116/123 [00:12<00:00,  9.72it/s]\u001b[A\n",
            " 95% 117/123 [00:12<00:00,  9.59it/s]\u001b[A\n",
            " 97% 119/123 [00:12<00:00,  9.52it/s]\u001b[A\n",
            " 98% 120/123 [00:12<00:00,  9.30it/s]\u001b[A\n",
            " 98% 121/123 [00:13<00:00,  9.43it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.5041444897651672, 'eval_accuracy': 0.8206, 'eval_runtime': 13.2598, 'eval_samples_per_second': 92.083, 'eval_steps_per_second': 9.276, 'epoch': 5.0}\n",
            "100% 4875/4875 [32:18<00:00,  3.25it/s]\n",
            "100% 123/123 [00:13<00:00,  9.38it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-27 12:10:54,935 >> Saving model checkpoint to saved/csqa/mcq/deberta-large/checkpoint-4875\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 12:10:54,936 >> Configuration saved in saved/csqa/mcq/deberta-large/checkpoint-4875/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 12:10:57,655 >> Model weights saved in saved/csqa/mcq/deberta-large/checkpoint-4875/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 12:10:57,656 >> tokenizer config file saved in saved/csqa/mcq/deberta-large/checkpoint-4875/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 12:10:57,657 >> Special tokens file saved in saved/csqa/mcq/deberta-large/checkpoint-4875/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 12:11:05,917 >> Deleting older checkpoint [saved/csqa/mcq/deberta-large/checkpoint-3900] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2023-01-27 12:11:06,584 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1955.1358, 'train_samples_per_second': 24.911, 'train_steps_per_second': 2.493, 'train_loss': 0.7315062694060497, 'epoch': 5.0}\n",
            "100% 4875/4875 [32:29<00:00,  2.50it/s]\n",
            "[INFO|trainer.py:2640] 2023-01-27 12:11:06,587 >> Saving model checkpoint to saved/csqa/mcq/deberta-large\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 12:11:06,588 >> Configuration saved in saved/csqa/mcq/deberta-large/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 12:11:10,716 >> Model weights saved in saved/csqa/mcq/deberta-large/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 12:11:10,717 >> tokenizer config file saved in saved/csqa/mcq/deberta-large/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 12:11:10,717 >> Special tokens file saved in saved/csqa/mcq/deberta-large/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.7315\n",
            "  train_runtime            = 0:32:35.13\n",
            "  train_samples            =       9741\n",
            "  train_samples_per_second =     24.911\n",
            "  train_steps_per_second   =      2.493\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-27 12:11:10,898 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice2, choice4, choice3, choice0, context, choice1. If choice2, choice4, choice3, choice0, context, choice1 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:11:10,900 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:11:10,901 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:11:10,901 >>   Batch size = 10\n",
            "100% 123/123 [00:13<00:00,  9.37it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.8206\n",
            "  eval_loss               =     0.5041\n",
            "  eval_runtime            = 0:00:13.26\n",
            "  eval_samples            =       1221\n",
            "  eval_samples_per_second =      92.06\n",
            "  eval_steps_per_second   =      9.274\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:722] 2023-01-27 12:11:24,166 >> The following columns in the test set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice2, choice4, choice3, choice0, context, choice1. If choice2, choice4, choice3, choice0, context, choice1 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:11:24,168 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:11:24,168 >>   Num examples = 1140\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:11:24,168 >>   Batch size = 10\n",
            "100% 114/114 [00:12<00:00,  8.99it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▃▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁▄▃▄▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second █▅▆▅▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second █▅▆▅▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▂▂▃▃▃▄▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▂▂▃▃▃▄▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▆▅▄▄▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▄▃▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.8206\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.50414\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 13.2631\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 92.06\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 9.274\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 4875\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.5201\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.430486703362964e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.73151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1955.1358\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 24.911\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/TEAM/wandb/offline-run-20230127_113836-3l896kuw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230127_113836-3l896kuw/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NPngLf005kH"
      },
      "source": [
        "### CSQA2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lv9jYrP05kH"
      },
      "source": [
        "##### TEAM - RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB2ZsCT9tv1N",
        "outputId": "caf50a87-d888-48f3-b83e-cd164478b5e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-27 10:50:51.142871: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Namespace(adam_epsilon=1e-08, bs=40, epochs=5, eval_bs=20, input_format='0', lr=3e-06, name='roberta-large', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "Test preds frequency: {'yes': 1801, 'no': 672}\n",
            "Epoch 1: Loss: Train 0.7015; Val 0.6932\n",
            "Classification Acc: Train 0.4975; Val 0.4949\n",
            "Instance Acc: Val 0.5183\n",
            "Test preds frequency: {'no': 1257, 'yes': 1216}\n",
            "Epoch 2: Loss: Train 0.6976; Val 0.6931\n",
            "Classification Acc: Train 0.4982; Val 0.5002\n",
            "Instance Acc: Val 0.5187\n",
            "Test preds frequency: {'no': 2090, 'yes': 383}\n",
            "Epoch 3: Loss: Train 0.6962; Val 0.6982\n",
            "Classification Acc: Train 0.5024; Val 0.5\n",
            "Instance Acc: Val 0.5124\n",
            "Test preds frequency: {'no': 2472, 'yes': 1}\n",
            "Epoch 4: Loss: Train 0.6951; Val 0.6931\n",
            "Classification Acc: Train 0.5037; Val 0.5\n",
            "Instance Acc: Val 0.5183\n",
            "Test preds frequency: {'no': 2456, 'yes': 17}\n",
            "Epoch 5: Loss: Train 0.6966; Val 0.693\n",
            "Classification Acc: Train 0.4979; Val 0.5075\n",
            "Instance Acc: Val 0.5171\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▂▇█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▄▂▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁▄▄▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ██▁█▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss ▁▁█▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.4979\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.6966\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.5075\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.5171\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.693\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/TEAM/wandb/offline-run-20230127_105107-sacm471s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230127_105107-sacm471s/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_csqa2.py --name \"roberta-large\" --epochs 5 --lr 3e-6 --bs 40 --eval-bs 20 --shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs0b0f3P05kH"
      },
      "source": [
        "##### SCORE - RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-lwmXvzPZYA",
        "outputId": "549af14f-9de4-4355-e557-546e48718fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-27 10:09:30.464289: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved/csqa2/mcq/roberta-large/runs/Jan27_10-09-32_f1ddfecb8ec5,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=saved/csqa2/mcq/roberta-large,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=80,\n",
            "per_device_train_batch_size=80,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=MCQA CSQA2 ROBERTA,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.005,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-e3640d05b680ab5c\n",
            "INFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 12075.73it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1962.40it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 862.97it/s]\n",
            "[INFO|hub.py:600] 2023-01-27 10:09:34,121 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4l9qza54\n",
            "Downloading config.json: 100% 482/482 [00:00<00:00, 527kB/s]\n",
            "[INFO|hub.py:613] 2023-01-27 10:09:34,258 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|hub.py:621] 2023-01-27 10:09:34,258 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 10:09:34,259 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 10:09:34,260 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:404] 2023-01-27 10:09:34,385 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 10:09:34,516 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 10:09:34,517 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2023-01-27 10:09:34,771 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpq4v6u7k1\n",
            "Downloading vocab.json: 100% 878k/878k [00:00<00:00, 5.38MB/s]\n",
            "[INFO|hub.py:613] 2023-01-27 10:09:35,079 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|hub.py:621] 2023-01-27 10:09:35,079 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|hub.py:600] 2023-01-27 10:09:35,211 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgq34d93a\n",
            "Downloading merges.txt: 100% 446k/446k [00:00<00:00, 3.15MB/s]\n",
            "[INFO|hub.py:613] 2023-01-27 10:09:35,497 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:621] 2023-01-27 10:09:35,497 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:600] 2023-01-27 10:09:35,622 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpo2hunzgw\n",
            "Downloading tokenizer.json: 100% 1.29M/1.29M [00:00<00:00, 7.94MB/s]\n",
            "[INFO|hub.py:613] 2023-01-27 10:09:35,950 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|hub.py:621] 2023-01-27 10:09:35,950 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 10:09:36,339 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 10:09:36,339 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 10:09:36,339 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 10:09:36,339 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 10:09:36,339 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 10:09:36,339 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 10:09:36,472 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 10:09:36,473 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2023-01-27 10:09:36,674 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpuz6ga0as\n",
            "Downloading pytorch_model.bin: 100% 1.33G/1.33G [00:14<00:00, 98.2MB/s]\n",
            "[INFO|hub.py:613] 2023-01-27 10:09:51,329 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[INFO|hub.py:621] 2023-01-27 10:09:51,329 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[INFO|modeling_utils.py:2041] 2023-01-27 10:09:51,330 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[WARNING|modeling_utils.py:2425] 2023-01-27 10:09:55,092 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2437] 2023-01-27 10:09:55,092 >> Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0% 0/10 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-39956aa705fafd5f.arrow\n",
            "100% 10/10 [00:00<00:00, 15.31ba/s]\n",
            "  0% 0/3 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-18f9ce436f980ced.arrow\n",
            "100% 3/3 [00:00<00:00, 10.70ba/s]\n",
            "  0% 0/3 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-301d0cdf4801cb91.arrow\n",
            "100% 3/3 [00:00<00:00, 23.74ba/s]\n",
            "Epoch count 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:722] 2023-01-27 10:10:00,691 >> The following columns in the training set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-27 10:10:00,707 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-27 10:10:00,707 >>   Num examples = 9264\n",
            "[INFO|trainer.py:1607] 2023-01-27 10:10:00,707 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2023-01-27 10:10:00,707 >>   Instantaneous batch size per device = 80\n",
            "[INFO|trainer.py:1609] 2023-01-27 10:10:00,707 >>   Total train batch size (w. parallel, distributed & accumulation) = 80\n",
            "[INFO|trainer.py:1610] 2023-01-27 10:10:00,707 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-27 10:10:00,708 >>   Total optimization steps = 580\n",
            "[INFO|integrations.py:607] 2023-01-27 10:10:00,709 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            " 20% 116/580 [01:43<06:18,  1.23it/s][INFO|trainer.py:722] 2023-01-27 10:19:45,372 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 10:19:45,375 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 10:19:45,375 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 10:19:45,375 >>   Batch size = 80\n",
            "\n",
            "  0% 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/32 [00:00<00:03,  7.61it/s]\u001b[A\n",
            "  9% 3/32 [00:00<00:05,  5.45it/s]\u001b[A\n",
            " 12% 4/32 [00:00<00:06,  4.45it/s]\u001b[A\n",
            " 16% 5/32 [00:01<00:06,  4.19it/s]\u001b[A\n",
            " 19% 6/32 [00:01<00:06,  4.05it/s]\u001b[A\n",
            " 22% 7/32 [00:01<00:06,  3.82it/s]\u001b[A\n",
            " 25% 8/32 [00:01<00:06,  3.81it/s]\u001b[A\n",
            " 28% 9/32 [00:02<00:06,  3.55it/s]\u001b[A\n",
            " 31% 10/32 [00:02<00:05,  3.74it/s]\u001b[A\n",
            " 34% 11/32 [00:02<00:05,  3.62it/s]\u001b[A\n",
            " 38% 12/32 [00:03<00:05,  3.35it/s]\u001b[A\n",
            " 41% 13/32 [00:03<00:05,  3.54it/s]\u001b[A\n",
            " 44% 14/32 [00:03<00:04,  3.64it/s]\u001b[A\n",
            " 47% 15/32 [00:06<00:18,  1.11s/it]\u001b[A\n",
            " 50% 16/32 [00:06<00:13,  1.17it/s]\u001b[A\n",
            " 53% 17/32 [00:07<00:10,  1.44it/s]\u001b[A\n",
            " 56% 18/32 [00:07<00:08,  1.68it/s]\u001b[A\n",
            " 59% 19/32 [00:07<00:06,  2.02it/s]\u001b[A\n",
            " 62% 20/32 [00:08<00:05,  2.35it/s]\u001b[A\n",
            " 66% 21/32 [00:08<00:04,  2.47it/s]\u001b[A\n",
            " 69% 22/32 [00:08<00:03,  2.83it/s]\u001b[A\n",
            " 72% 23/32 [00:09<00:03,  2.99it/s]\u001b[A\n",
            " 75% 24/32 [00:09<00:02,  3.20it/s]\u001b[A\n",
            " 78% 25/32 [00:09<00:02,  3.37it/s]\u001b[A\n",
            " 81% 26/32 [00:09<00:01,  3.38it/s]\u001b[A\n",
            " 84% 27/32 [00:10<00:01,  3.51it/s]\u001b[A\n",
            " 88% 28/32 [00:10<00:01,  3.59it/s]\u001b[A\n",
            " 91% 29/32 [00:10<00:00,  3.54it/s]\u001b[A\n",
            " 94% 30/32 [00:10<00:00,  3.52it/s]\u001b[A\n",
            " 97% 31/32 [00:11<00:00,  3.37it/s]\u001b[A\n",
            "100% 32/32 [00:11<00:00,  3.59it/s]\u001b[A\n",
            "{'eval_loss': 0.6930882334709167, 'eval_accuracy': 0.5116, 'eval_runtime': 11.759, 'eval_samples_per_second': 216.089, 'eval_steps_per_second': 2.721, 'epoch': 1.0}\n",
            "\n",
            " 20% 116/580 [01:55<06:18,  1.23it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 10:19:57,136 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-116\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 10:19:57,137 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-116/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 10:19:59,047 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-116/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 10:19:59,048 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-116/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 10:19:59,048 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-116/special_tokens_map.json\n",
            " 40% 232/580 [03:45<05:09,  1.13it/s][INFO|trainer.py:722] 2023-01-27 10:21:47,531 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 10:21:47,533 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 10:21:47,533 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 10:21:47,534 >>   Batch size = 80\n",
            "\n",
            "  0% 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/32 [00:00<00:03,  7.60it/s]\u001b[A\n",
            "  9% 3/32 [00:00<00:05,  5.45it/s]\u001b[A\n",
            " 12% 4/32 [00:00<00:06,  4.45it/s]\u001b[A\n",
            " 16% 5/32 [00:01<00:06,  4.18it/s]\u001b[A\n",
            " 19% 6/32 [00:01<00:06,  4.05it/s]\u001b[A\n",
            " 22% 7/32 [00:01<00:06,  3.82it/s]\u001b[A\n",
            " 25% 8/32 [00:01<00:06,  3.81it/s]\u001b[A\n",
            " 28% 9/32 [00:02<00:06,  3.55it/s]\u001b[A\n",
            " 31% 10/32 [00:02<00:05,  3.74it/s]\u001b[A\n",
            " 34% 11/32 [00:02<00:05,  3.62it/s]\u001b[A\n",
            " 38% 12/32 [00:03<00:05,  3.35it/s]\u001b[A\n",
            " 41% 13/32 [00:03<00:05,  3.54it/s]\u001b[A\n",
            " 44% 14/32 [00:03<00:04,  3.64it/s]\u001b[A\n",
            " 47% 15/32 [00:06<00:17,  1.04s/it]\u001b[A\n",
            " 50% 16/32 [00:06<00:12,  1.25it/s]\u001b[A\n",
            " 53% 17/32 [00:06<00:09,  1.52it/s]\u001b[A\n",
            " 56% 18/32 [00:07<00:07,  1.76it/s]\u001b[A\n",
            " 59% 19/32 [00:07<00:06,  2.09it/s]\u001b[A\n",
            " 62% 20/32 [00:07<00:04,  2.42it/s]\u001b[A\n",
            " 66% 21/32 [00:08<00:04,  2.53it/s]\u001b[A\n",
            " 69% 22/32 [00:08<00:03,  2.88it/s]\u001b[A\n",
            " 72% 23/32 [00:08<00:02,  3.03it/s]\u001b[A\n",
            " 75% 24/32 [00:09<00:02,  3.23it/s]\u001b[A\n",
            " 78% 25/32 [00:09<00:02,  3.40it/s]\u001b[A\n",
            " 81% 26/32 [00:09<00:01,  3.41it/s]\u001b[A\n",
            " 84% 27/32 [00:09<00:01,  3.53it/s]\u001b[A\n",
            " 88% 28/32 [00:10<00:01,  3.61it/s]\u001b[A\n",
            " 91% 29/32 [00:10<00:00,  3.55it/s]\u001b[A\n",
            " 94% 30/32 [00:10<00:00,  3.52it/s]\u001b[A\n",
            " 97% 31/32 [00:11<00:00,  3.38it/s]\u001b[A\n",
            "100% 32/32 [00:11<00:00,  3.59it/s]\u001b[A\n",
            "{'eval_loss': 0.6929593682289124, 'eval_accuracy': 0.5352, 'eval_runtime': 11.5089, 'eval_samples_per_second': 220.786, 'eval_steps_per_second': 2.78, 'epoch': 2.0}\n",
            "\n",
            " 40% 232/580 [03:57<05:09,  1.13it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 10:21:59,044 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-232\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 10:21:59,045 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-232/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 10:22:00,944 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-232/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 10:22:00,945 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-232/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 10:22:00,945 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-232/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 10:22:07,192 >> Deleting older checkpoint [saved/csqa2/mcq/roberta-large/checkpoint-116] due to args.save_total_limit\n",
            " 60% 348/580 [05:49<03:16,  1.18it/s][INFO|trainer.py:722] 2023-01-27 10:23:50,999 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 10:23:51,001 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 10:23:51,001 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 10:23:51,001 >>   Batch size = 80\n",
            "\n",
            "  0% 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/32 [00:00<00:03,  7.61it/s]\u001b[A\n",
            "  9% 3/32 [00:00<00:05,  5.45it/s]\u001b[A\n",
            " 12% 4/32 [00:00<00:06,  4.45it/s]\u001b[A\n",
            " 16% 5/32 [00:01<00:06,  4.18it/s]\u001b[A\n",
            " 19% 6/32 [00:01<00:06,  4.04it/s]\u001b[A\n",
            " 22% 7/32 [00:01<00:06,  3.82it/s]\u001b[A\n",
            " 25% 8/32 [00:01<00:06,  3.81it/s]\u001b[A\n",
            " 28% 9/32 [00:02<00:06,  3.55it/s]\u001b[A\n",
            " 31% 10/32 [00:02<00:05,  3.74it/s]\u001b[A\n",
            " 34% 11/32 [00:02<00:05,  3.62it/s]\u001b[A\n",
            " 38% 12/32 [00:03<00:05,  3.34it/s]\u001b[A\n",
            " 41% 13/32 [00:03<00:05,  3.54it/s]\u001b[A\n",
            " 44% 14/32 [00:03<00:04,  3.64it/s]\u001b[A\n",
            " 47% 15/32 [00:06<00:17,  1.04s/it]\u001b[A\n",
            " 50% 16/32 [00:06<00:12,  1.25it/s]\u001b[A\n",
            " 53% 17/32 [00:06<00:09,  1.52it/s]\u001b[A\n",
            " 56% 18/32 [00:07<00:07,  1.76it/s]\u001b[A\n",
            " 59% 19/32 [00:07<00:06,  2.09it/s]\u001b[A\n",
            " 62% 20/32 [00:07<00:04,  2.42it/s]\u001b[A\n",
            " 66% 21/32 [00:08<00:04,  2.53it/s]\u001b[A\n",
            " 69% 22/32 [00:08<00:03,  2.88it/s]\u001b[A\n",
            " 72% 23/32 [00:08<00:02,  3.03it/s]\u001b[A\n",
            " 75% 24/32 [00:09<00:02,  3.23it/s]\u001b[A\n",
            " 78% 25/32 [00:09<00:02,  3.40it/s]\u001b[A\n",
            " 81% 26/32 [00:09<00:01,  3.41it/s]\u001b[A\n",
            " 84% 27/32 [00:09<00:01,  3.53it/s]\u001b[A\n",
            " 88% 28/32 [00:10<00:01,  3.61it/s]\u001b[A\n",
            " 91% 29/32 [00:10<00:00,  3.55it/s]\u001b[A\n",
            " 94% 30/32 [00:10<00:00,  3.52it/s]\u001b[A\n",
            " 97% 31/32 [00:11<00:00,  3.38it/s]\u001b[A\n",
            "100% 32/32 [00:11<00:00,  3.59it/s]\u001b[A\n",
            "{'eval_loss': 0.6928155422210693, 'eval_accuracy': 0.5427, 'eval_runtime': 11.5103, 'eval_samples_per_second': 220.76, 'eval_steps_per_second': 2.78, 'epoch': 3.0}\n",
            "\n",
            " 60% 348/580 [06:00<03:16,  1.18it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 10:24:02,513 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-348\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 10:24:02,514 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-348/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 10:24:04,401 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-348/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 10:24:04,402 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-348/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 10:24:04,402 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-348/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 10:24:11,170 >> Deleting older checkpoint [saved/csqa2/mcq/roberta-large/checkpoint-232] due to args.save_total_limit\n",
            " 80% 464/580 [07:52<01:35,  1.21it/s][INFO|trainer.py:722] 2023-01-27 10:25:54,311 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 10:25:54,314 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 10:25:54,314 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 10:25:54,314 >>   Batch size = 80\n",
            "\n",
            "  0% 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/32 [00:00<00:03,  7.63it/s]\u001b[A\n",
            "  9% 3/32 [00:00<00:05,  5.46it/s]\u001b[A\n",
            " 12% 4/32 [00:00<00:06,  4.45it/s]\u001b[A\n",
            " 16% 5/32 [00:01<00:06,  4.19it/s]\u001b[A\n",
            " 19% 6/32 [00:01<00:06,  4.05it/s]\u001b[A\n",
            " 22% 7/32 [00:01<00:06,  3.83it/s]\u001b[A\n",
            " 25% 8/32 [00:01<00:06,  3.77it/s]\u001b[A\n",
            " 28% 9/32 [00:02<00:06,  3.53it/s]\u001b[A\n",
            " 31% 10/32 [00:02<00:05,  3.72it/s]\u001b[A\n",
            " 34% 11/32 [00:02<00:05,  3.61it/s]\u001b[A\n",
            " 38% 12/32 [00:03<00:05,  3.34it/s]\u001b[A\n",
            " 41% 13/32 [00:03<00:05,  3.53it/s]\u001b[A\n",
            " 44% 14/32 [00:03<00:04,  3.64it/s]\u001b[A\n",
            " 47% 15/32 [00:06<00:17,  1.04s/it]\u001b[A\n",
            " 50% 16/32 [00:06<00:12,  1.24it/s]\u001b[A\n",
            " 53% 17/32 [00:07<00:09,  1.52it/s]\u001b[A\n",
            " 56% 18/32 [00:07<00:07,  1.76it/s]\u001b[A\n",
            " 59% 19/32 [00:07<00:06,  2.09it/s]\u001b[A\n",
            " 62% 20/32 [00:07<00:04,  2.42it/s]\u001b[A\n",
            " 66% 21/32 [00:08<00:04,  2.53it/s]\u001b[A\n",
            " 69% 22/32 [00:08<00:03,  2.88it/s]\u001b[A\n",
            " 72% 23/32 [00:08<00:02,  3.03it/s]\u001b[A\n",
            " 75% 24/32 [00:09<00:02,  3.23it/s]\u001b[A\n",
            " 78% 25/32 [00:09<00:02,  3.39it/s]\u001b[A\n",
            " 81% 26/32 [00:09<00:01,  3.40it/s]\u001b[A\n",
            " 84% 27/32 [00:09<00:01,  3.52it/s]\u001b[A\n",
            " 88% 28/32 [00:10<00:01,  3.60it/s]\u001b[A\n",
            " 91% 29/32 [00:10<00:00,  3.55it/s]\u001b[A\n",
            " 94% 30/32 [00:10<00:00,  3.52it/s]\u001b[A\n",
            " 97% 31/32 [00:11<00:00,  3.38it/s]\u001b[A\n",
            "100% 32/32 [00:11<00:00,  3.59it/s]\u001b[A\n",
            "{'eval_loss': 0.6925551891326904, 'eval_accuracy': 0.551, 'eval_runtime': 11.5195, 'eval_samples_per_second': 220.582, 'eval_steps_per_second': 2.778, 'epoch': 4.0}\n",
            "\n",
            " 80% 464/580 [08:04<01:35,  1.21it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 10:26:05,835 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-464\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 10:26:05,836 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-464/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 10:26:07,771 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-464/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 10:26:07,771 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-464/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 10:26:07,772 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-464/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 10:26:14,008 >> Deleting older checkpoint [saved/csqa2/mcq/roberta-large/checkpoint-348] due to args.save_total_limit\n",
            "{'loss': 0.6967, 'learning_rate': 4.1379310344827586e-07, 'epoch': 4.31}\n",
            "100% 580/580 [09:55<00:00,  1.24it/s][INFO|trainer.py:722] 2023-01-27 10:27:57,272 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 10:27:57,275 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 10:27:57,275 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 10:27:57,275 >>   Batch size = 80\n",
            "\n",
            "  0% 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/32 [00:00<00:03,  7.63it/s]\u001b[A\n",
            "  9% 3/32 [00:00<00:05,  5.45it/s]\u001b[A\n",
            " 12% 4/32 [00:00<00:06,  4.46it/s]\u001b[A\n",
            " 16% 5/32 [00:01<00:06,  4.19it/s]\u001b[A\n",
            " 19% 6/32 [00:01<00:06,  4.05it/s]\u001b[A\n",
            " 22% 7/32 [00:01<00:06,  3.83it/s]\u001b[A\n",
            " 25% 8/32 [00:01<00:06,  3.82it/s]\u001b[A\n",
            " 28% 9/32 [00:02<00:06,  3.56it/s]\u001b[A\n",
            " 31% 10/32 [00:02<00:05,  3.75it/s]\u001b[A\n",
            " 34% 11/32 [00:02<00:05,  3.62it/s]\u001b[A\n",
            " 38% 12/32 [00:03<00:05,  3.35it/s]\u001b[A\n",
            " 41% 13/32 [00:03<00:05,  3.54it/s]\u001b[A\n",
            " 44% 14/32 [00:03<00:04,  3.65it/s]\u001b[A\n",
            " 47% 15/32 [00:06<00:17,  1.04s/it]\u001b[A\n",
            " 50% 16/32 [00:06<00:12,  1.24it/s]\u001b[A\n",
            " 53% 17/32 [00:06<00:09,  1.52it/s]\u001b[A\n",
            " 56% 18/32 [00:07<00:07,  1.76it/s]\u001b[A\n",
            " 59% 19/32 [00:07<00:06,  2.09it/s]\u001b[A\n",
            " 62% 20/32 [00:07<00:04,  2.42it/s]\u001b[A\n",
            " 66% 21/32 [00:08<00:04,  2.53it/s]\u001b[A\n",
            " 69% 22/32 [00:08<00:03,  2.88it/s]\u001b[A\n",
            " 72% 23/32 [00:08<00:02,  3.03it/s]\u001b[A\n",
            " 75% 24/32 [00:09<00:02,  3.23it/s]\u001b[A\n",
            " 78% 25/32 [00:09<00:02,  3.39it/s]\u001b[A\n",
            " 81% 26/32 [00:09<00:01,  3.40it/s]\u001b[A\n",
            " 84% 27/32 [00:09<00:01,  3.52it/s]\u001b[A\n",
            " 88% 28/32 [00:10<00:01,  3.60it/s]\u001b[A\n",
            " 91% 29/32 [00:10<00:00,  3.55it/s]\u001b[A\n",
            " 94% 30/32 [00:10<00:00,  3.52it/s]\u001b[A\n",
            " 97% 31/32 [00:11<00:00,  3.38it/s]\u001b[A\n",
            "100% 32/32 [00:11<00:00,  3.59it/s]\u001b[A\n",
            "{'eval_loss': 0.692484974861145, 'eval_accuracy': 0.5462, 'eval_runtime': 11.5099, 'eval_samples_per_second': 220.766, 'eval_steps_per_second': 2.78, 'epoch': 5.0}\n",
            "\n",
            "100% 580/580 [10:07<00:00,  1.24it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 10:28:08,787 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-580\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 10:28:08,787 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-580/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 10:28:10,658 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-580/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 10:28:10,659 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-580/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 10:28:10,659 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-580/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 10:28:16,941 >> Deleting older checkpoint [saved/csqa2/mcq/roberta-large/checkpoint-464] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2023-01-27 10:28:17,483 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1096.7759, 'train_samples_per_second': 42.233, 'train_steps_per_second': 0.529, 'train_loss': 0.6964864993917531, 'epoch': 5.0}\n",
            "100% 580/580 [10:15<00:00,  1.06s/it]\n",
            "[INFO|trainer.py:2640] 2023-01-27 10:28:17,485 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 10:28:17,486 >> Configuration saved in saved/csqa2/mcq/roberta-large/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 10:28:20,563 >> Model weights saved in saved/csqa2/mcq/roberta-large/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 10:28:20,564 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 10:28:20,565 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.6965\n",
            "  train_runtime            = 0:18:16.77\n",
            "  train_samples            =       9264\n",
            "  train_samples_per_second =     42.233\n",
            "  train_steps_per_second   =      0.529\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-27 10:28:20,663 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 10:28:20,665 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 10:28:20,665 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 10:28:20,665 >>   Batch size = 80\n",
            "100% 32/32 [00:11<00:00,  2.84it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.5462\n",
            "  eval_loss               =     0.6925\n",
            "  eval_runtime            = 0:00:11.55\n",
            "  eval_samples            =       2541\n",
            "  eval_samples_per_second =     219.92\n",
            "  eval_steps_per_second   =       2.77\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:722] 2023-01-27 10:28:32,222 >> The following columns in the test set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 10:28:32,224 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 10:28:32,224 >>   Num examples = 2473\n",
            "[INFO|trainer.py:2896] 2023-01-27 10:28:32,224 >>   Batch size = 80\n",
            "100% 31/31 [00:08<00:00,  3.57it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▅▇█▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▇▅▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁████▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁████▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▃▅▆▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▃▅▆▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.5462\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.69248\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 11.5542\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 219.92\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 2.77\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 580\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.6967\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 7579896378376896.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.69649\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1096.7759\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 42.233\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.529\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/TEAM/wandb/offline-run-20230127_101801-1ayxippx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230127_101801-1ayxippx/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run_mcqa_score.py --learning_rate=3e-6 --num_train_epochs 5 --seed 42 \\\n",
        "--train_file=\"data/csqa2/mcq_train.json\" --validation_file=\"data/csqa2/mcq_dev.json\" --test_file=\"data/csqa2/mcq_test.json\" \\\n",
        "--output_dir=\"saved/csqa2/mcq/roberta-large\" --model_name_or_path=\"roberta-large\" \\\n",
        "--per_device_train_batch_size=80 --per_device_eval_batch_size=80 --weight_decay=0.005 \\\n",
        "--do_train True --do_eval True --do_predict True --evaluation_strategy=\"epoch\" --save_strategy=\"epoch\" \\\n",
        "--report_to \"wandb\" --run_name \"MCQA CSQA2 ROBERTA\" --save_total_limit=1 --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D0SKbWo05kI"
      },
      "source": [
        "##### TEAM - DeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgY54f7I43aQ",
        "outputId": "abae835f-17b7-4f60-9781-09e73f344563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=4, epochs=5, eval_bs=4, input_format='0', lr=3e-06, name='microsoft/deberta-v3-large', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "\rDownloading tokenizer_config.json:   0% 0.00/52.0 [00:00<?, ?B/s]\rDownloading tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 80.0kB/s]\n",
            "Downloading config.json: 100% 580/580 [00:00<00:00, 1.05MB/s]\n",
            "Downloading spm.model: 100% 2.35M/2.35M [00:00<00:00, 75.5MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading pytorch_model.bin: 100% 833M/833M [00:09<00:00, 88.0MB/s]\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230114_105326-17f1xp5b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrevived-brook-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA2-deberta-v3-large\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA2-deberta-v3-large/runs/17f1xp5b\u001b[0m\n",
            "Test preds frequency: {'yes': 1701, 'no': 772}\n",
            "Epoch 1: Loss: Train 0.6641; Val 0.6953\n",
            "Classification Acc: Train 0.5604; Val 0.6257\n",
            "Instance Acc: Val 0.6305\n",
            "Test preds frequency: {'yes': 1329, 'no': 1144}\n",
            "Epoch 2: Loss: Train 0.3874; Val 0.9017\n",
            "Classification Acc: Train 0.8353; Val 0.6702\n",
            "Instance Acc: Val 0.6749\n",
            "Test preds frequency: {'yes': 1302, 'no': 1171}\n",
            "Epoch 3: Loss: Train 0.1562; Val 1.1955\n",
            "Classification Acc: Train 0.947; Val 0.6688\n",
            "Instance Acc: Val 0.6671\n",
            "Test preds frequency: {'no': 1281, 'yes': 1192}\n",
            "Epoch 4: Loss: Train 0.0748; Val 1.4857\n",
            "Classification Acc: Train 0.9763; Val 0.669\n",
            "Instance Acc: Val 0.6686\n",
            "Test preds frequency: {'yes': 1327, 'no': 1146}\n",
            "Epoch 5: Loss: Train 0.0466; Val 1.459\n",
            "Classification Acc: Train 0.9865; Val 0.671\n",
            "Instance Acc: Val 0.669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▆▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▅▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁█▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss ▁▃▅██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.9865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.0466\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.671\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 1.459\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrevived-brook-3\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA2-deberta-v3-large/runs/17f1xp5b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230114_105326-17f1xp5b/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_csqa2.py --name \"microsoft/deberta-v3-large\" --epochs 5 --lr 3e-6 --bs 4 --eval-bs 4 --shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SCORE - DeBERTa"
      ],
      "metadata": {
        "id": "cwa_ZKvx5roy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run_mcqa_score.py --learning_rate=3e-6 --num_train_epochs 5 --seed 42 \\\n",
        "--train_file=\"data/csqa2/mcq_train.json\" --validation_file=\"data/csqa2/mcq_dev.json\" --test_file=\"data/csqa2/mcq_test.json\" \\\n",
        "--output_dir=\"saved/csqa2/mcq/deberta-large\" --model_name_or_path=\"deberta-large\" \\\n",
        "--per_device_train_batch_size=40 --per_device_eval_batch_size=40 --weight_decay=0.005 \\\n",
        "--do_train True --do_eval True --do_predict True --evaluation_strategy=\"epoch\" --save_strategy=\"epoch\" \\\n",
        "--report_to \"wandb\" --run_name \"MCQA CSQA2 DEBERTA\" --save_total_limit=1 --overwrite_output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC6EblSf5u2F",
        "outputId": "5efa8a87-1d89-466b-f794-66a03fb5b336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-27 11:06:53.020628: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved/csqa2/mcq/deberta-large/runs/Jan27_11-06-55_f1ddfecb8ec5,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=saved/csqa2/mcq/deberta-large,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=40,\n",
            "per_device_train_batch_size=40,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=MCQA CSQA2 DEBERTA,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.005,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-e3640d05b680ab5c\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "100% 3/3 [00:00<00:00, 723.86it/s]\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 11:06:56,230 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 11:06:56,231 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 11:06:56,501 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 11:06:56,502 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:06:57,310 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/6386fc34376768db39488179803c16268ff12ee177a43a993690f66b7d7a0b7c.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:06:57,310 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:06:57,310 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:06:57,310 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 11:06:57,310 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/cae8294cb38511dc11086c090549f0a079bc5537a0f9a482d8358f17acc8cff0.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 11:06:57,449 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 11:06:57,450 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils.py:426] 2023-01-27 11:06:57,993 >> Adding [MASK] to the vocabulary\n",
            "[WARNING|logging.py:279] 2023-01-27 11:06:57,994 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 11:06:58,127 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 11:06:58,128 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:279] 2023-01-27 11:06:58,422 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|modeling_utils.py:2041] 2023-01-27 11:06:58,587 >> loading weights file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/eed737dd80585a756b0286a093059c2b4403b98a17ac2cb50cda7799c653fc11.e38140a56995392eade33ad2835bb905412b65ba305475bd577c00edb10c45d9\n",
            "[WARNING|modeling_utils.py:2425] 2023-01-27 11:07:02,700 >> Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMultipleChoice: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2437] 2023-01-27 11:07:02,700 >> Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:__main__:The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-75f9a1603e2d257b.arrow\n",
            "  0% 0/3 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5aef5627dbc93ed1.arrow\n",
            "100% 3/3 [00:00<00:00,  9.39ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e3640d05b680ab5c/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-46e7c7314baece7b.arrow\n",
            "Epoch count 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:722] 2023-01-27 11:07:05,895 >> The following columns in the training set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-27 11:07:05,910 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-27 11:07:05,911 >>   Num examples = 9264\n",
            "[INFO|trainer.py:1607] 2023-01-27 11:07:05,911 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2023-01-27 11:07:05,911 >>   Instantaneous batch size per device = 40\n",
            "[INFO|trainer.py:1609] 2023-01-27 11:07:05,911 >>   Total train batch size (w. parallel, distributed & accumulation) = 40\n",
            "[INFO|trainer.py:1610] 2023-01-27 11:07:05,911 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-27 11:07:05,911 >>   Total optimization steps = 1160\n",
            "[INFO|integrations.py:607] 2023-01-27 11:07:05,912 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            " 20% 232/1160 [02:19<08:19,  1.86it/s][INFO|trainer.py:722] 2023-01-27 11:09:29,722 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:09:29,725 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:09:29,725 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:09:29,726 >>   Batch size = 40\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/64 [00:00<00:05, 10.60it/s]\u001b[A\n",
            "  6% 4/64 [00:00<00:08,  6.78it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:08,  6.64it/s]\u001b[A\n",
            "  9% 6/64 [00:01<00:12,  4.64it/s]\u001b[A\n",
            " 11% 7/64 [00:01<00:11,  5.07it/s]\u001b[A\n",
            " 12% 8/64 [00:01<00:10,  5.12it/s]\u001b[A\n",
            " 14% 9/64 [00:01<00:10,  5.32it/s]\u001b[A\n",
            " 16% 10/64 [00:01<00:10,  5.31it/s]\u001b[A\n",
            " 17% 11/64 [00:01<00:09,  5.35it/s]\u001b[A\n",
            " 19% 12/64 [00:02<00:09,  5.36it/s]\u001b[A\n",
            " 20% 13/64 [00:02<00:09,  5.41it/s]\u001b[A\n",
            " 22% 14/64 [00:02<00:09,  5.25it/s]\u001b[A\n",
            " 23% 15/64 [00:02<00:09,  5.29it/s]\u001b[A\n",
            " 25% 16/64 [00:02<00:09,  5.33it/s]\u001b[A\n",
            " 27% 17/64 [00:03<00:09,  5.07it/s]\u001b[A\n",
            " 28% 18/64 [00:03<00:08,  5.16it/s]\u001b[A\n",
            " 30% 19/64 [00:03<00:08,  5.32it/s]\u001b[A\n",
            " 31% 20/64 [00:03<00:08,  5.43it/s]\u001b[A\n",
            " 33% 21/64 [00:03<00:08,  5.23it/s]\u001b[A\n",
            " 34% 22/64 [00:04<00:08,  5.23it/s]\u001b[A\n",
            " 36% 23/64 [00:04<00:08,  4.86it/s]\u001b[A\n",
            " 38% 24/64 [00:04<00:08,  4.87it/s]\u001b[A\n",
            " 39% 25/64 [00:04<00:07,  5.14it/s]\u001b[A\n",
            " 41% 26/64 [00:04<00:07,  5.34it/s]\u001b[A\n",
            " 42% 27/64 [00:05<00:06,  5.44it/s]\u001b[A\n",
            " 44% 28/64 [00:05<00:06,  5.51it/s]\u001b[A\n",
            " 45% 29/64 [00:05<00:06,  5.48it/s]\u001b[A\n",
            " 47% 30/64 [00:05<00:06,  5.25it/s]\u001b[A\n",
            " 48% 31/64 [00:05<00:05,  5.53it/s]\u001b[A\n",
            " 50% 32/64 [00:05<00:05,  5.49it/s]\u001b[A\n",
            " 52% 33/64 [00:06<00:05,  5.72it/s]\u001b[A\n",
            " 53% 34/64 [00:06<00:05,  5.29it/s]\u001b[A\n",
            " 55% 35/64 [00:06<00:05,  4.87it/s]\u001b[A\n",
            " 56% 36/64 [00:06<00:05,  4.86it/s]\u001b[A\n",
            " 58% 37/64 [00:06<00:05,  5.22it/s]\u001b[A\n",
            " 59% 38/64 [00:07<00:04,  5.24it/s]\u001b[A\n",
            " 61% 39/64 [00:07<00:04,  5.28it/s]\u001b[A\n",
            " 62% 40/64 [00:07<00:04,  5.33it/s]\u001b[A\n",
            " 64% 41/64 [00:07<00:04,  5.20it/s]\u001b[A\n",
            " 66% 42/64 [00:07<00:04,  4.78it/s]\u001b[A\n",
            " 67% 43/64 [00:08<00:04,  5.02it/s]\u001b[A\n",
            " 69% 44/64 [00:08<00:03,  5.21it/s]\u001b[A\n",
            " 70% 45/64 [00:08<00:03,  5.34it/s]\u001b[A\n",
            " 72% 46/64 [00:08<00:03,  5.21it/s]\u001b[A\n",
            " 73% 47/64 [00:08<00:03,  5.29it/s]\u001b[A\n",
            " 75% 48/64 [00:09<00:02,  5.40it/s]\u001b[A\n",
            " 77% 49/64 [00:09<00:02,  5.41it/s]\u001b[A\n",
            " 78% 50/64 [00:09<00:02,  5.41it/s]\u001b[A\n",
            " 80% 51/64 [00:09<00:02,  5.55it/s]\u001b[A\n",
            " 81% 52/64 [00:09<00:02,  5.47it/s]\u001b[A\n",
            " 83% 53/64 [00:09<00:02,  5.49it/s]\u001b[A\n",
            " 84% 54/64 [00:10<00:01,  5.59it/s]\u001b[A\n",
            " 86% 55/64 [00:10<00:01,  5.66it/s]\u001b[A\n",
            " 88% 56/64 [00:10<00:01,  5.58it/s]\u001b[A\n",
            " 89% 57/64 [00:10<00:01,  5.54it/s]\u001b[A\n",
            " 91% 58/64 [00:10<00:01,  5.45it/s]\u001b[A\n",
            " 92% 59/64 [00:11<00:00,  5.45it/s]\u001b[A\n",
            " 94% 60/64 [00:11<00:00,  5.40it/s]\u001b[A\n",
            " 95% 61/64 [00:11<00:00,  5.10it/s]\u001b[A\n",
            " 97% 62/64 [00:11<00:00,  5.22it/s]\u001b[A\n",
            " 98% 63/64 [00:11<00:00,  5.51it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6930082440376282, 'eval_accuracy': 0.5455, 'eval_runtime': 12.0858, 'eval_samples_per_second': 210.247, 'eval_steps_per_second': 5.295, 'epoch': 1.0}\n",
            " 20% 232/1160 [02:31<08:19,  1.86it/s]\n",
            "100% 64/64 [00:11<00:00,  6.16it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 11:09:41,813 >> Saving model checkpoint to saved/csqa2/mcq/deberta-large/checkpoint-232\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 11:09:41,814 >> Configuration saved in saved/csqa2/mcq/deberta-large/checkpoint-232/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 11:09:44,257 >> Model weights saved in saved/csqa2/mcq/deberta-large/checkpoint-232/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 11:09:44,258 >> tokenizer config file saved in saved/csqa2/mcq/deberta-large/checkpoint-232/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 11:09:44,258 >> Special tokens file saved in saved/csqa2/mcq/deberta-large/checkpoint-232/special_tokens_map.json\n",
            " 40% 464/1160 [05:00<06:21,  1.83it/s][INFO|trainer.py:722] 2023-01-27 11:12:11,277 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:12:11,279 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:12:11,280 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:12:11,280 >>   Batch size = 40\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/64 [00:00<00:05, 10.61it/s]\u001b[A\n",
            "  6% 4/64 [00:00<00:08,  6.79it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:08,  6.63it/s]\u001b[A\n",
            "  9% 6/64 [00:00<00:09,  6.25it/s]\u001b[A\n",
            " 11% 7/64 [00:01<00:09,  6.31it/s]\u001b[A\n",
            " 12% 8/64 [00:01<00:09,  5.96it/s]\u001b[A\n",
            " 14% 9/64 [00:01<00:09,  5.93it/s]\u001b[A\n",
            " 16% 10/64 [00:01<00:09,  5.73it/s]\u001b[A\n",
            " 17% 11/64 [00:01<00:09,  5.64it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:09,  5.56it/s]\u001b[A\n",
            " 20% 13/64 [00:02<00:09,  5.55it/s]\u001b[A\n",
            " 22% 14/64 [00:02<00:09,  5.34it/s]\u001b[A\n",
            " 23% 15/64 [00:02<00:09,  5.36it/s]\u001b[A\n",
            " 25% 16/64 [00:02<00:08,  5.38it/s]\u001b[A\n",
            " 27% 17/64 [00:02<00:09,  5.10it/s]\u001b[A\n",
            " 28% 18/64 [00:03<00:08,  5.18it/s]\u001b[A\n",
            " 30% 19/64 [00:03<00:08,  5.33it/s]\u001b[A\n",
            " 31% 20/64 [00:03<00:08,  5.44it/s]\u001b[A\n",
            " 33% 21/64 [00:03<00:08,  5.23it/s]\u001b[A\n",
            " 34% 22/64 [00:03<00:08,  5.23it/s]\u001b[A\n",
            " 36% 23/64 [00:04<00:08,  4.87it/s]\u001b[A\n",
            " 38% 24/64 [00:04<00:08,  4.87it/s]\u001b[A\n",
            " 39% 25/64 [00:04<00:07,  5.13it/s]\u001b[A\n",
            " 41% 26/64 [00:04<00:07,  5.34it/s]\u001b[A\n",
            " 42% 27/64 [00:04<00:06,  5.44it/s]\u001b[A\n",
            " 44% 28/64 [00:05<00:06,  5.51it/s]\u001b[A\n",
            " 45% 29/64 [00:05<00:06,  5.48it/s]\u001b[A\n",
            " 47% 30/64 [00:05<00:06,  5.26it/s]\u001b[A\n",
            " 48% 31/64 [00:05<00:05,  5.51it/s]\u001b[A\n",
            " 50% 32/64 [00:05<00:05,  5.47it/s]\u001b[A\n",
            " 52% 33/64 [00:05<00:05,  5.71it/s]\u001b[A\n",
            " 53% 34/64 [00:06<00:05,  5.29it/s]\u001b[A\n",
            " 55% 35/64 [00:06<00:05,  4.87it/s]\u001b[A\n",
            " 56% 36/64 [00:06<00:05,  4.86it/s]\u001b[A\n",
            " 58% 37/64 [00:06<00:05,  5.21it/s]\u001b[A\n",
            " 59% 38/64 [00:06<00:04,  5.23it/s]\u001b[A\n",
            " 61% 39/64 [00:07<00:04,  5.28it/s]\u001b[A\n",
            " 62% 40/64 [00:07<00:04,  5.32it/s]\u001b[A\n",
            " 64% 41/64 [00:07<00:04,  5.20it/s]\u001b[A\n",
            " 66% 42/64 [00:07<00:04,  4.85it/s]\u001b[A\n",
            " 67% 43/64 [00:07<00:04,  5.07it/s]\u001b[A\n",
            " 69% 44/64 [00:08<00:03,  5.25it/s]\u001b[A\n",
            " 70% 45/64 [00:08<00:03,  5.38it/s]\u001b[A\n",
            " 72% 46/64 [00:08<00:03,  5.23it/s]\u001b[A\n",
            " 73% 47/64 [00:08<00:03,  5.36it/s]\u001b[A\n",
            " 75% 48/64 [00:08<00:02,  5.45it/s]\u001b[A\n",
            " 77% 49/64 [00:09<00:02,  5.45it/s]\u001b[A\n",
            " 78% 50/64 [00:09<00:02,  5.44it/s]\u001b[A\n",
            " 80% 51/64 [00:09<00:02,  5.57it/s]\u001b[A\n",
            " 81% 52/64 [00:09<00:02,  5.49it/s]\u001b[A\n",
            " 83% 53/64 [00:09<00:01,  5.50it/s]\u001b[A\n",
            " 84% 54/64 [00:09<00:01,  5.61it/s]\u001b[A\n",
            " 86% 55/64 [00:10<00:01,  5.69it/s]\u001b[A\n",
            " 88% 56/64 [00:10<00:01,  5.60it/s]\u001b[A\n",
            " 89% 57/64 [00:10<00:01,  5.56it/s]\u001b[A\n",
            " 91% 58/64 [00:10<00:01,  5.46it/s]\u001b[A\n",
            " 92% 59/64 [00:10<00:00,  5.46it/s]\u001b[A\n",
            " 94% 60/64 [00:11<00:00,  5.40it/s]\u001b[A\n",
            " 95% 61/64 [00:11<00:00,  5.10it/s]\u001b[A\n",
            " 97% 62/64 [00:11<00:00,  5.23it/s]\u001b[A\n",
            " 98% 63/64 [00:11<00:00,  5.51it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6708648800849915, 'eval_accuracy': 0.5903, 'eval_runtime': 11.8778, 'eval_samples_per_second': 213.929, 'eval_steps_per_second': 5.388, 'epoch': 2.0}\n",
            " 40% 464/1160 [05:12<06:21,  1.83it/s]\n",
            "100% 64/64 [00:11<00:00,  6.16it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 11:12:23,159 >> Saving model checkpoint to saved/csqa2/mcq/deberta-large/checkpoint-464\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 11:12:23,160 >> Configuration saved in saved/csqa2/mcq/deberta-large/checkpoint-464/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 11:12:25,746 >> Model weights saved in saved/csqa2/mcq/deberta-large/checkpoint-464/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 11:12:25,747 >> tokenizer config file saved in saved/csqa2/mcq/deberta-large/checkpoint-464/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 11:12:25,747 >> Special tokens file saved in saved/csqa2/mcq/deberta-large/checkpoint-464/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 11:12:34,000 >> Deleting older checkpoint [saved/csqa2/mcq/deberta-large/checkpoint-232] due to args.save_total_limit\n",
            "{'loss': 0.6838, 'learning_rate': 1.706896551724138e-06, 'epoch': 2.16}\n",
            " 60% 696/1160 [07:42<04:01,  1.92it/s][INFO|trainer.py:722] 2023-01-27 11:14:53,189 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:14:53,191 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:14:53,191 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:14:53,191 >>   Batch size = 40\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/64 [00:00<00:05, 10.60it/s]\u001b[A\n",
            "  6% 4/64 [00:00<00:08,  6.78it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:08,  6.64it/s]\u001b[A\n",
            "  9% 6/64 [00:00<00:09,  6.26it/s]\u001b[A\n",
            " 11% 7/64 [00:01<00:09,  6.32it/s]\u001b[A\n",
            " 12% 8/64 [00:01<00:09,  5.97it/s]\u001b[A\n",
            " 14% 9/64 [00:01<00:09,  5.94it/s]\u001b[A\n",
            " 16% 10/64 [00:01<00:09,  5.73it/s]\u001b[A\n",
            " 17% 11/64 [00:01<00:09,  5.64it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:09,  5.56it/s]\u001b[A\n",
            " 20% 13/64 [00:02<00:09,  5.55it/s]\u001b[A\n",
            " 22% 14/64 [00:02<00:09,  5.35it/s]\u001b[A\n",
            " 23% 15/64 [00:02<00:09,  5.37it/s]\u001b[A\n",
            " 25% 16/64 [00:02<00:08,  5.38it/s]\u001b[A\n",
            " 27% 17/64 [00:02<00:09,  5.10it/s]\u001b[A\n",
            " 28% 18/64 [00:03<00:08,  5.18it/s]\u001b[A\n",
            " 30% 19/64 [00:03<00:08,  5.33it/s]\u001b[A\n",
            " 31% 20/64 [00:03<00:08,  5.44it/s]\u001b[A\n",
            " 33% 21/64 [00:03<00:08,  5.23it/s]\u001b[A\n",
            " 34% 22/64 [00:03<00:08,  5.23it/s]\u001b[A\n",
            " 36% 23/64 [00:04<00:08,  4.86it/s]\u001b[A\n",
            " 38% 24/64 [00:04<00:08,  4.87it/s]\u001b[A\n",
            " 39% 25/64 [00:04<00:07,  5.13it/s]\u001b[A\n",
            " 41% 26/64 [00:04<00:07,  5.34it/s]\u001b[A\n",
            " 42% 27/64 [00:04<00:06,  5.44it/s]\u001b[A\n",
            " 44% 28/64 [00:05<00:06,  5.51it/s]\u001b[A\n",
            " 45% 29/64 [00:05<00:06,  5.48it/s]\u001b[A\n",
            " 47% 30/64 [00:05<00:06,  5.26it/s]\u001b[A\n",
            " 48% 31/64 [00:05<00:05,  5.53it/s]\u001b[A\n",
            " 50% 32/64 [00:05<00:05,  5.49it/s]\u001b[A\n",
            " 52% 33/64 [00:05<00:05,  5.72it/s]\u001b[A\n",
            " 53% 34/64 [00:06<00:05,  5.29it/s]\u001b[A\n",
            " 55% 35/64 [00:06<00:05,  4.87it/s]\u001b[A\n",
            " 56% 36/64 [00:06<00:05,  4.86it/s]\u001b[A\n",
            " 58% 37/64 [00:06<00:05,  5.22it/s]\u001b[A\n",
            " 59% 38/64 [00:06<00:04,  5.24it/s]\u001b[A\n",
            " 61% 39/64 [00:07<00:04,  5.29it/s]\u001b[A\n",
            " 62% 40/64 [00:07<00:04,  5.33it/s]\u001b[A\n",
            " 64% 41/64 [00:07<00:04,  5.20it/s]\u001b[A\n",
            " 66% 42/64 [00:07<00:04,  4.86it/s]\u001b[A\n",
            " 67% 43/64 [00:07<00:04,  5.08it/s]\u001b[A\n",
            " 69% 44/64 [00:08<00:03,  5.25it/s]\u001b[A\n",
            " 70% 45/64 [00:08<00:03,  5.38it/s]\u001b[A\n",
            " 72% 46/64 [00:08<00:03,  5.23it/s]\u001b[A\n",
            " 73% 47/64 [00:08<00:03,  5.36it/s]\u001b[A\n",
            " 75% 48/64 [00:08<00:02,  5.45it/s]\u001b[A\n",
            " 77% 49/64 [00:09<00:02,  5.44it/s]\u001b[A\n",
            " 78% 50/64 [00:09<00:02,  5.44it/s]\u001b[A\n",
            " 80% 51/64 [00:09<00:02,  5.57it/s]\u001b[A\n",
            " 81% 52/64 [00:09<00:02,  5.48it/s]\u001b[A\n",
            " 83% 53/64 [00:09<00:02,  5.50it/s]\u001b[A\n",
            " 84% 54/64 [00:09<00:01,  5.61it/s]\u001b[A\n",
            " 86% 55/64 [00:10<00:01,  5.68it/s]\u001b[A\n",
            " 88% 56/64 [00:10<00:01,  5.60it/s]\u001b[A\n",
            " 89% 57/64 [00:10<00:01,  5.55it/s]\u001b[A\n",
            " 91% 58/64 [00:10<00:01,  5.46it/s]\u001b[A\n",
            " 92% 59/64 [00:10<00:00,  5.45it/s]\u001b[A\n",
            " 94% 60/64 [00:11<00:00,  5.40it/s]\u001b[A\n",
            " 95% 61/64 [00:11<00:00,  5.11it/s]\u001b[A\n",
            " 97% 62/64 [00:11<00:00,  5.23it/s]\u001b[A\n",
            " 98% 63/64 [00:11<00:00,  5.51it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6768587231636047, 'eval_accuracy': 0.6179, 'eval_runtime': 11.8763, 'eval_samples_per_second': 213.956, 'eval_steps_per_second': 5.389, 'epoch': 3.0}\n",
            " 60% 696/1160 [07:54<04:01,  1.92it/s]\n",
            "100% 64/64 [00:11<00:00,  6.16it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 11:15:05,069 >> Saving model checkpoint to saved/csqa2/mcq/deberta-large/checkpoint-696\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 11:15:05,070 >> Configuration saved in saved/csqa2/mcq/deberta-large/checkpoint-696/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 11:15:07,566 >> Model weights saved in saved/csqa2/mcq/deberta-large/checkpoint-696/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 11:15:07,567 >> tokenizer config file saved in saved/csqa2/mcq/deberta-large/checkpoint-696/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 11:15:07,568 >> Special tokens file saved in saved/csqa2/mcq/deberta-large/checkpoint-696/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 11:15:16,379 >> Deleting older checkpoint [saved/csqa2/mcq/deberta-large/checkpoint-464] due to args.save_total_limit\n",
            " 80% 928/1160 [10:24<02:08,  1.81it/s][INFO|trainer.py:722] 2023-01-27 11:17:35,206 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:17:35,208 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:17:35,208 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:17:35,209 >>   Batch size = 40\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/64 [00:00<00:05, 10.49it/s]\u001b[A\n",
            "  6% 4/64 [00:00<00:08,  6.73it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:08,  6.59it/s]\u001b[A\n",
            "  9% 6/64 [00:00<00:09,  6.23it/s]\u001b[A\n",
            " 11% 7/64 [00:01<00:09,  6.30it/s]\u001b[A\n",
            " 12% 8/64 [00:01<00:09,  5.95it/s]\u001b[A\n",
            " 14% 9/64 [00:01<00:09,  5.92it/s]\u001b[A\n",
            " 16% 10/64 [00:01<00:09,  5.68it/s]\u001b[A\n",
            " 17% 11/64 [00:01<00:09,  5.59it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:09,  5.53it/s]\u001b[A\n",
            " 20% 13/64 [00:02<00:09,  5.53it/s]\u001b[A\n",
            " 22% 14/64 [00:02<00:09,  5.33it/s]\u001b[A\n",
            " 23% 15/64 [00:02<00:09,  5.35it/s]\u001b[A\n",
            " 25% 16/64 [00:02<00:08,  5.37it/s]\u001b[A\n",
            " 27% 17/64 [00:02<00:09,  5.07it/s]\u001b[A\n",
            " 28% 18/64 [00:03<00:08,  5.15it/s]\u001b[A\n",
            " 30% 19/64 [00:03<00:08,  5.29it/s]\u001b[A\n",
            " 31% 20/64 [00:03<00:08,  5.39it/s]\u001b[A\n",
            " 33% 21/64 [00:03<00:08,  5.18it/s]\u001b[A\n",
            " 34% 22/64 [00:03<00:08,  5.18it/s]\u001b[A\n",
            " 36% 23/64 [00:04<00:08,  4.81it/s]\u001b[A\n",
            " 38% 24/64 [00:04<00:08,  4.82it/s]\u001b[A\n",
            " 39% 25/64 [00:04<00:07,  5.08it/s]\u001b[A\n",
            " 41% 26/64 [00:04<00:07,  5.28it/s]\u001b[A\n",
            " 42% 27/64 [00:04<00:06,  5.38it/s]\u001b[A\n",
            " 44% 28/64 [00:05<00:06,  5.45it/s]\u001b[A\n",
            " 45% 29/64 [00:05<00:06,  5.42it/s]\u001b[A\n",
            " 47% 30/64 [00:05<00:06,  5.21it/s]\u001b[A\n",
            " 48% 31/64 [00:05<00:06,  5.48it/s]\u001b[A\n",
            " 50% 32/64 [00:05<00:05,  5.45it/s]\u001b[A\n",
            " 52% 33/64 [00:05<00:05,  5.69it/s]\u001b[A\n",
            " 53% 34/64 [00:06<00:05,  5.27it/s]\u001b[A\n",
            " 55% 35/64 [00:06<00:05,  4.86it/s]\u001b[A\n",
            " 56% 36/64 [00:06<00:05,  4.85it/s]\u001b[A\n",
            " 58% 37/64 [00:06<00:05,  5.22it/s]\u001b[A\n",
            " 59% 38/64 [00:06<00:04,  5.24it/s]\u001b[A\n",
            " 61% 39/64 [00:07<00:04,  5.28it/s]\u001b[A\n",
            " 62% 40/64 [00:07<00:04,  5.33it/s]\u001b[A\n",
            " 64% 41/64 [00:07<00:04,  5.20it/s]\u001b[A\n",
            " 66% 42/64 [00:07<00:04,  4.84it/s]\u001b[A\n",
            " 67% 43/64 [00:07<00:04,  5.05it/s]\u001b[A\n",
            " 69% 44/64 [00:08<00:03,  5.23it/s]\u001b[A\n",
            " 70% 45/64 [00:08<00:03,  5.36it/s]\u001b[A\n",
            " 72% 46/64 [00:08<00:03,  5.21it/s]\u001b[A\n",
            " 73% 47/64 [00:08<00:03,  5.35it/s]\u001b[A\n",
            " 75% 48/64 [00:08<00:02,  5.44it/s]\u001b[A\n",
            " 77% 49/64 [00:09<00:02,  5.44it/s]\u001b[A\n",
            " 78% 50/64 [00:09<00:02,  5.43it/s]\u001b[A\n",
            " 80% 51/64 [00:09<00:02,  5.56it/s]\u001b[A\n",
            " 81% 52/64 [00:09<00:02,  5.48it/s]\u001b[A\n",
            " 83% 53/64 [00:09<00:02,  5.49it/s]\u001b[A\n",
            " 84% 54/64 [00:09<00:01,  5.59it/s]\u001b[A\n",
            " 86% 55/64 [00:10<00:01,  5.67it/s]\u001b[A\n",
            " 88% 56/64 [00:10<00:01,  5.58it/s]\u001b[A\n",
            " 89% 57/64 [00:10<00:01,  5.54it/s]\u001b[A\n",
            " 91% 58/64 [00:10<00:01,  5.45it/s]\u001b[A\n",
            " 92% 59/64 [00:10<00:00,  5.44it/s]\u001b[A\n",
            " 94% 60/64 [00:11<00:00,  5.39it/s]\u001b[A\n",
            " 95% 61/64 [00:11<00:00,  5.10it/s]\u001b[A\n",
            " 97% 62/64 [00:11<00:00,  5.22it/s]\u001b[A\n",
            " 98% 63/64 [00:11<00:00,  5.51it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7365572452545166, 'eval_accuracy': 0.6364, 'eval_runtime': 11.9288, 'eval_samples_per_second': 213.015, 'eval_steps_per_second': 5.365, 'epoch': 4.0}\n",
            " 80% 928/1160 [10:36<02:08,  1.81it/s]\n",
            "100% 64/64 [00:11<00:00,  6.16it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 11:17:47,139 >> Saving model checkpoint to saved/csqa2/mcq/deberta-large/checkpoint-928\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 11:17:47,140 >> Configuration saved in saved/csqa2/mcq/deberta-large/checkpoint-928/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 11:17:49,659 >> Model weights saved in saved/csqa2/mcq/deberta-large/checkpoint-928/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 11:17:49,660 >> tokenizer config file saved in saved/csqa2/mcq/deberta-large/checkpoint-928/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 11:17:49,661 >> Special tokens file saved in saved/csqa2/mcq/deberta-large/checkpoint-928/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 11:17:58,077 >> Deleting older checkpoint [saved/csqa2/mcq/deberta-large/checkpoint-696] due to args.save_total_limit\n",
            "{'loss': 0.5029, 'learning_rate': 4.1379310344827586e-07, 'epoch': 4.31}\n",
            "100% 1160/1160 [13:06<00:00,  1.87it/s][INFO|trainer.py:722] 2023-01-27 11:20:16,777 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:20:16,779 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:20:16,780 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:20:16,780 >>   Batch size = 40\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/64 [00:00<00:05, 10.55it/s]\u001b[A\n",
            "  6% 4/64 [00:00<00:08,  6.76it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:08,  6.61it/s]\u001b[A\n",
            "  9% 6/64 [00:00<00:09,  6.24it/s]\u001b[A\n",
            " 11% 7/64 [00:01<00:09,  6.31it/s]\u001b[A\n",
            " 12% 8/64 [00:01<00:09,  5.94it/s]\u001b[A\n",
            " 14% 9/64 [00:01<00:09,  5.91it/s]\u001b[A\n",
            " 16% 10/64 [00:01<00:09,  5.71it/s]\u001b[A\n",
            " 17% 11/64 [00:01<00:09,  5.63it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:09,  5.56it/s]\u001b[A\n",
            " 20% 13/64 [00:02<00:09,  5.55it/s]\u001b[A\n",
            " 22% 14/64 [00:02<00:09,  5.34it/s]\u001b[A\n",
            " 23% 15/64 [00:02<00:09,  5.36it/s]\u001b[A\n",
            " 25% 16/64 [00:02<00:08,  5.37it/s]\u001b[A\n",
            " 27% 17/64 [00:02<00:09,  5.10it/s]\u001b[A\n",
            " 28% 18/64 [00:03<00:08,  5.18it/s]\u001b[A\n",
            " 30% 19/64 [00:03<00:08,  5.33it/s]\u001b[A\n",
            " 31% 20/64 [00:03<00:08,  5.42it/s]\u001b[A\n",
            " 33% 21/64 [00:03<00:08,  5.21it/s]\u001b[A\n",
            " 34% 22/64 [00:03<00:08,  5.22it/s]\u001b[A\n",
            " 36% 23/64 [00:04<00:08,  4.86it/s]\u001b[A\n",
            " 38% 24/64 [00:04<00:08,  4.86it/s]\u001b[A\n",
            " 39% 25/64 [00:04<00:07,  5.13it/s]\u001b[A\n",
            " 41% 26/64 [00:04<00:07,  5.33it/s]\u001b[A\n",
            " 42% 27/64 [00:04<00:06,  5.43it/s]\u001b[A\n",
            " 44% 28/64 [00:05<00:06,  5.51it/s]\u001b[A\n",
            " 45% 29/64 [00:05<00:06,  5.47it/s]\u001b[A\n",
            " 47% 30/64 [00:05<00:06,  5.25it/s]\u001b[A\n",
            " 48% 31/64 [00:05<00:05,  5.52it/s]\u001b[A\n",
            " 50% 32/64 [00:05<00:05,  5.48it/s]\u001b[A\n",
            " 52% 33/64 [00:05<00:05,  5.70it/s]\u001b[A\n",
            " 53% 34/64 [00:06<00:05,  5.28it/s]\u001b[A\n",
            " 55% 35/64 [00:06<00:05,  4.86it/s]\u001b[A\n",
            " 56% 36/64 [00:06<00:05,  4.85it/s]\u001b[A\n",
            " 58% 37/64 [00:06<00:05,  5.21it/s]\u001b[A\n",
            " 59% 38/64 [00:06<00:04,  5.22it/s]\u001b[A\n",
            " 61% 39/64 [00:07<00:04,  5.27it/s]\u001b[A\n",
            " 62% 40/64 [00:07<00:04,  5.31it/s]\u001b[A\n",
            " 64% 41/64 [00:07<00:04,  5.19it/s]\u001b[A\n",
            " 66% 42/64 [00:07<00:04,  4.84it/s]\u001b[A\n",
            " 67% 43/64 [00:07<00:04,  5.06it/s]\u001b[A\n",
            " 69% 44/64 [00:08<00:03,  5.23it/s]\u001b[A\n",
            " 70% 45/64 [00:08<00:03,  5.36it/s]\u001b[A\n",
            " 72% 46/64 [00:08<00:03,  5.22it/s]\u001b[A\n",
            " 73% 47/64 [00:08<00:03,  5.35it/s]\u001b[A\n",
            " 75% 48/64 [00:08<00:02,  5.44it/s]\u001b[A\n",
            " 77% 49/64 [00:09<00:02,  5.43it/s]\u001b[A\n",
            " 78% 50/64 [00:09<00:02,  5.42it/s]\u001b[A\n",
            " 80% 51/64 [00:09<00:02,  5.55it/s]\u001b[A\n",
            " 81% 52/64 [00:09<00:02,  5.46it/s]\u001b[A\n",
            " 83% 53/64 [00:09<00:02,  5.48it/s]\u001b[A\n",
            " 84% 54/64 [00:09<00:01,  5.57it/s]\u001b[A\n",
            " 86% 55/64 [00:10<00:01,  5.64it/s]\u001b[A\n",
            " 88% 56/64 [00:10<00:01,  5.56it/s]\u001b[A\n",
            " 89% 57/64 [00:10<00:01,  5.52it/s]\u001b[A\n",
            " 91% 58/64 [00:10<00:01,  5.44it/s]\u001b[A\n",
            " 92% 59/64 [00:10<00:00,  5.44it/s]\u001b[A\n",
            " 94% 60/64 [00:11<00:00,  5.38it/s]\u001b[A\n",
            " 95% 61/64 [00:11<00:00,  5.09it/s]\u001b[A\n",
            " 97% 62/64 [00:11<00:00,  5.21it/s]\u001b[A\n",
            " 98% 63/64 [00:11<00:00,  5.50it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7702575325965881, 'eval_accuracy': 0.6336, 'eval_runtime': 11.907, 'eval_samples_per_second': 213.403, 'eval_steps_per_second': 5.375, 'epoch': 5.0}\n",
            "100% 1160/1160 [13:18<00:00,  1.87it/s]\n",
            "100% 64/64 [00:11<00:00,  6.15it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 11:20:28,688 >> Saving model checkpoint to saved/csqa2/mcq/deberta-large/checkpoint-1160\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 11:20:28,689 >> Configuration saved in saved/csqa2/mcq/deberta-large/checkpoint-1160/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 11:20:31,465 >> Model weights saved in saved/csqa2/mcq/deberta-large/checkpoint-1160/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 11:20:31,466 >> tokenizer config file saved in saved/csqa2/mcq/deberta-large/checkpoint-1160/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 11:20:31,467 >> Special tokens file saved in saved/csqa2/mcq/deberta-large/checkpoint-1160/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 11:20:39,812 >> Deleting older checkpoint [saved/csqa2/mcq/deberta-large/checkpoint-928] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2023-01-27 11:20:40,493 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 814.5832, 'train_samples_per_second': 56.863, 'train_steps_per_second': 1.424, 'train_loss': 0.5713662114636652, 'epoch': 5.0}\n",
            "100% 1160/1160 [13:30<00:00,  1.43it/s]\n",
            "[INFO|trainer.py:2640] 2023-01-27 11:20:40,497 >> Saving model checkpoint to saved/csqa2/mcq/deberta-large\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 11:20:40,498 >> Configuration saved in saved/csqa2/mcq/deberta-large/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 11:20:44,285 >> Model weights saved in saved/csqa2/mcq/deberta-large/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 11:20:44,286 >> tokenizer config file saved in saved/csqa2/mcq/deberta-large/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 11:20:44,287 >> Special tokens file saved in saved/csqa2/mcq/deberta-large/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.5714\n",
            "  train_runtime            = 0:13:34.58\n",
            "  train_samples            =       9264\n",
            "  train_samples_per_second =     56.863\n",
            "  train_steps_per_second   =      1.424\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-27 11:20:44,484 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:20:44,487 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:20:44,487 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:20:44,487 >>   Batch size = 40\n",
            "100% 64/64 [00:11<00:00,  5.47it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.6336\n",
            "  eval_loss               =     0.7703\n",
            "  eval_runtime            = 0:00:11.93\n",
            "  eval_samples            =       2541\n",
            "  eval_samples_per_second =    212.929\n",
            "  eval_steps_per_second   =      5.363\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:722] 2023-01-27 11:20:56,423 >> The following columns in the test set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice0, choice1, context. If choice0, choice1, context are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 11:20:56,425 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 11:20:56,425 >>   Num examples = 2473\n",
            "[INFO|trainer.py:2896] 2023-01-27 11:20:56,425 >>   Batch size = 40\n",
            "100% 62/62 [00:11<00:00,  5.43it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▄▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ▃▁▁▆██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▁▁▃▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁██▆▇▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁██▆▇▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▃▃▅▆▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▃▃▅▆▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.6336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.77026\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 11.9336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 212.929\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 5.363\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1160\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.5029\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 6551033609814528.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.57137\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 814.5832\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 56.863\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.424\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/TEAM/wandb/offline-run-20230127_110710-3j15kqrr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230127_110710-3j15kqrr/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-XvqCkU05kI"
      },
      "source": [
        "### QASC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2An0cowL05kI"
      },
      "source": [
        "##### TEAM - RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ca8604-16cb-4814-ba3a-32e9b169b8a0",
        "id": "KK9zilfy05kJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=8, epochs=5, eval_bs=8, input_format='1', lr=3e-06, name='roberta-large', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Downloading config.json: 100% 482/482 [00:00<00:00, 622kB/s]\n",
            "Downloading vocab.json: 100% 878k/878k [00:00<00:00, 7.63MB/s]\n",
            "Downloading merges.txt: 100% 446k/446k [00:00<00:00, 4.68MB/s]\n",
            "Downloading tokenizer.json: 100% 1.29M/1.29M [00:00<00:00, 9.78MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.33G/1.33G [00:16<00:00, 86.0MB/s]\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "Test preds frequency: {'A': 126, 'F': 124, 'E': 118, 'H': 112, 'D': 112, 'B': 111, 'G': 110, 'C': 107}\n",
            "Epoch 1: Loss: Train 0.3828; Val 0.3756\n",
            "Classification Acc: Train 0.875; Val 0.875\n",
            "Classification Macro F1: Train 0.4668; Val 0.4667\n",
            "Instance Acc: Val 0.1587\n",
            "Test preds frequency: {'F': 134, 'G': 126, 'A': 123, 'C': 116, 'D': 114, 'H': 112, 'E': 109, 'B': 86}\n",
            "Epoch 2: Loss: Train 0.3619; Val 0.3325\n",
            "Classification Acc: Train 0.875; Val 0.8737\n",
            "Classification Macro F1: Train 0.4765; Val 0.5381\n",
            "Instance Acc: Val 0.4546\n",
            "Test preds frequency: {'F': 126, 'C': 123, 'A': 117, 'H': 117, 'D': 116, 'E': 111, 'G': 106, 'B': 104}\n",
            "Epoch 3: Loss: Train 0.3098; Val 0.3437\n",
            "Classification Acc: Train 0.8823; Val 0.8464\n",
            "Classification Macro F1: Train 0.603; Val 0.6342\n",
            "Instance Acc: Val 0.4752\n",
            "Test preds frequency: {'H': 127, 'A': 123, 'F': 123, 'G': 113, 'C': 112, 'D': 110, 'B': 106, 'E': 106}\n",
            "Epoch 4: Loss: Train 0.2574; Val 0.3966\n",
            "Classification Acc: Train 0.9022; Val 0.8359\n",
            "Classification Macro F1: Train 0.7192; Val 0.6518\n",
            "Instance Acc: Val 0.4654\n",
            "Test preds frequency: {'A': 132, 'H': 123, 'F': 120, 'C': 119, 'G': 113, 'D': 112, 'B': 101, 'E': 100}\n",
            "Epoch 5: Loss: Train 0.2138; Val 0.385\n",
            "Classification Acc: Train 0.9187; Val 0.8371\n",
            "Classification Macro F1: Train 0.7828; Val 0.6705\n",
            "Instance Acc: Val 0.4752\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▁▂▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▇▅▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ██▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss ▆▁▂█▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.9187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.2138\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8371\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.4752\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/TEAM/wandb/offline-run-20230112_203518-2hoqa6ax\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230112_203518-2hoqa6ax/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_qasc.py --name \"roberta-large\" --epochs 5 --lr 3e-6 --shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlkWf8Qc05kJ"
      },
      "source": [
        "##### TEAM - DeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmfzMn8qJHy9",
        "outputId": "86777def-2d2b-4cf7-a9a8-596771096fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=8, epochs=5, eval_bs=8, input_format='1', lr=3e-06, name='microsoft/deberta-v3-base', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Downloading tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 47.1kB/s]\n",
            "Downloading config.json: 100% 579/579 [00:00<00:00, 534kB/s]\n",
            "Downloading spm.model: 100% 2.35M/2.35M [00:01<00:00, 1.88MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading pytorch_model.bin: 100% 354M/354M [00:04<00:00, 81.9MB/s]\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias']\n",
            "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalyvasman\u001b[0m (\u001b[33mnlpteam_gr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230115_160433-1rahq0f8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeach-glitter-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/QASC-deberta-v3-base\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/QASC-deberta-v3-base/runs/1rahq0f8\u001b[0m\n",
            "Test preds frequency: {'D': 128, 'C': 126, 'A': 119, 'F': 117, 'H': 111, 'G': 111, 'E': 105, 'B': 103}\n",
            "Epoch 1: Loss: Train 0.347; Val 0.2889\n",
            "Classification Acc: Train 0.8741; Val 0.8867\n",
            "Classification Macro F1: Train 0.4792; Val 0.5792\n",
            "Instance Acc: Val 0.6069\n",
            "Test preds frequency: {'F': 132, 'C': 123, 'A': 118, 'G': 117, 'B': 110, 'E': 108, 'H': 107, 'D': 105}\n",
            "Epoch 2: Loss: Train 0.288; Val 0.273\n",
            "Classification Acc: Train 0.8903; Val 0.8921\n",
            "Classification Macro F1: Train 0.6592; Val 0.6955\n",
            "Instance Acc: Val 0.5907\n",
            "Test preds frequency: {'F': 128, 'C': 122, 'H': 121, 'A': 120, 'D': 110, 'E': 109, 'B': 107, 'G': 103}\n",
            "Epoch 3: Loss: Train 0.2391; Val 0.3019\n",
            "Classification Acc: Train 0.9083; Val 0.8699\n",
            "Classification Macro F1: Train 0.7485; Val 0.7009\n",
            "Instance Acc: Val 0.5508\n",
            "Test preds frequency: {'F': 132, 'C': 123, 'A': 123, 'G': 122, 'B': 107, 'H': 106, 'E': 106, 'D': 101}\n",
            "Epoch 4: Loss: Train 0.2008; Val 0.2964\n",
            "Classification Acc: Train 0.9235; Val 0.8838\n",
            "Classification Macro F1: Train 0.8015; Val 0.7126\n",
            "Instance Acc: Val 0.5713\n",
            "Test preds frequency: {'F': 133, 'C': 122, 'H': 119, 'A': 119, 'G': 114, 'E': 107, 'B': 105, 'D': 101}\n",
            "Epoch 5: Loss: Train 0.166; Val 0.3688\n",
            "Classification Acc: Train 0.9374; Val 0.8506\n",
            "Classification Macro F1: Train 0.8437; Val 0.7038\n",
            "Instance Acc: Val 0.5594\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▃▅▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▆▄▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▇█▄▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy █▆▁▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss ▂▁▃▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.9374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.166\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.5594\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.3688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mpeach-glitter-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/QASC-deberta-v3-base/runs/1rahq0f8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230115_160433-1rahq0f8/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_qasc.py --name \"microsoft/deberta-v3-base\" --epochs 5 --lr 3e-6 --shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQCZhyKM05kJ"
      },
      "source": [
        "##### SCORE - RoBEERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSmwWRsi__VH",
        "outputId": "0f39521b-1354-4a14-827c-bf5583253b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved/qasc/mcq/roberta-large/runs/Jan15_15-17-39_f5ec42a1e8f2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=saved/qasc/mcq/roberta-large,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=10,\n",
            "per_device_train_batch_size=5,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=MCQA qasc ROBERTA,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.005,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-ee3bb49e6900120e\n",
            "INFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 12839.71it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1187.74it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 750.90it/s]\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:41,859 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8gzjrlhi\n",
            "Downloading config.json: 100% 482/482 [00:00<00:00, 399kB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 15:17:42,778 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|hub.py:621] 2023-01-15 15:17:42,778 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:17:42,778 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:17:42,780 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:404] 2023-01-15 15:17:43,693 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:17:44,607 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:17:44,607 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:46,458 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4mj7ttw2\n",
            "Downloading vocab.json: 100% 878k/878k [00:01<00:00, 670kB/s] \n",
            "[INFO|hub.py:613] 2023-01-15 15:17:48,731 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|hub.py:621] 2023-01-15 15:17:48,732 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:49,647 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgp7oxj_g\n",
            "Downloading merges.txt: 100% 446k/446k [00:01<00:00, 406kB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 15:17:51,692 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:621] 2023-01-15 15:17:51,692 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:52,607 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptgioprmk\n",
            "Downloading tokenizer.json: 100% 1.29M/1.29M [00:01<00:00, 1.00MB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 15:17:54,906 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|hub.py:621] 2023-01-15 15:17:54,906 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:17:58,560 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:17:58,561 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:59,559 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp81x2hinf\n",
            "Downloading pytorch_model.bin: 100% 1.33G/1.33G [00:15<00:00, 90.3MB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 15:18:15,375 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[INFO|hub.py:621] 2023-01-15 15:18:15,375 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[INFO|modeling_utils.py:2041] 2023-01-15 15:18:15,375 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[WARNING|modeling_utils.py:2425] 2023-01-15 15:18:19,329 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2437] 2023-01-15 15:18:19,329 >> Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-ff0c7627b8efec73.arrow\n",
            "100% 9/9 [00:03<00:00,  2.69ba/s]\n",
            "  0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-914372885e09b527.arrow\n",
            "100% 1/1 [00:00<00:00,  2.10ba/s]\n",
            "  0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-14209fe1256c3792.arrow\n",
            "100% 1/1 [00:00<00:00,  3.46ba/s]\n",
            "Epoch count 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:722] 2023-01-15 15:18:27,964 >> The following columns in the training set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-15 15:18:27,985 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-15 15:18:27,985 >>   Num examples = 8134\n",
            "[INFO|trainer.py:1607] 2023-01-15 15:18:27,985 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2023-01-15 15:18:27,985 >>   Instantaneous batch size per device = 5\n",
            "[INFO|trainer.py:1609] 2023-01-15 15:18:27,985 >>   Total train batch size (w. parallel, distributed & accumulation) = 5\n",
            "[INFO|trainer.py:1610] 2023-01-15 15:18:27,985 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-15 15:18:27,985 >>   Total optimization steps = 8135\n",
            "[INFO|integrations.py:607] 2023-01-15 15:18:27,987 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalyvasman\u001b[0m (\u001b[33mnlpteam_gr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230115_151829-1lvrhmk8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMCQA qasc ROBERTA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/1lvrhmk8\u001b[0m\n",
            "{'loss': 2.0986, 'learning_rate': 2.8156115550092194e-06, 'epoch': 0.31}\n",
            "{'loss': 2.0588, 'learning_rate': 2.631223110018439e-06, 'epoch': 0.61}\n",
            "{'loss': 1.9575, 'learning_rate': 2.4468346650276585e-06, 'epoch': 0.92}\n",
            " 20% 1627/8135 [17:22<1:03:23,  1.71it/s][INFO|trainer.py:722] 2023-01-15 15:35:53,509 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 15:35:53,512 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 15:35:53,513 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 15:35:53,513 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.37it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.06it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.09it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:24,  3.63it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.29it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.31it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.36it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.26it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.09it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.89it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.07it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.97it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.85it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:29,  2.69it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.80it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.90it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.98it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.02it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:23,  3.05it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.08it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.10it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.12it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.09it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.11it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.00it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.86it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.01it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.04it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.08it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.11it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.18it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:18,  3.21it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:18,  3.15it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.22it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.17it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.13it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.13it/s]\u001b[A\n",
            " 44% 41/93 [00:13<00:16,  3.15it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.15it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.16it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.15it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.15it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.27it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.45it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.36it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.10it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.13it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.02it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.86it/s]\u001b[A\n",
            " 57% 53/93 [00:17<00:14,  2.69it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.82it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.90it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.02it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:13,  2.76it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.84it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.73it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.61it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.59it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.77it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.85it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.91it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.98it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.01it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.10it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.15it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.01it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.85it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.91it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.79it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.06it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.09it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  2.99it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.02it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.86it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.05it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.13it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.13it/s]\u001b[A\n",
            " 89% 83/93 [00:27<00:03,  3.14it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.89it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.94it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  2.98it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.78it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.72it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.74it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.83it/s]\u001b[A\n",
            " 98% 91/93 [00:30<00:00,  2.74it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.83it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.22it/s]\u001b[A\n",
            "{'eval_loss': 1.6697083711624146, 'eval_accuracy': 0.3963, 'eval_runtime': 30.9179, 'eval_samples_per_second': 29.95, 'eval_steps_per_second': 3.008, 'epoch': 1.0}\n",
            "\n",
            " 20% 1627/8135 [17:53<1:03:23,  1.71it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 15:36:24,433 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-1627\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 15:36:24,435 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-1627/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 15:36:29,696 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-1627/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 15:36:29,697 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-1627/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 15:36:29,698 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-1627/special_tokens_map.json\n",
            "{'loss': 1.8461, 'learning_rate': 2.262446220036878e-06, 'epoch': 1.23}\n",
            "{'loss': 1.761, 'learning_rate': 2.078057775046097e-06, 'epoch': 1.54}\n",
            "{'loss': 1.6944, 'learning_rate': 1.8936693300553166e-06, 'epoch': 1.84}\n",
            " 40% 3254/8135 [35:41<48:36,  1.67it/s][INFO|trainer.py:722] 2023-01-15 15:54:11,982 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 15:54:11,985 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 15:54:11,985 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 15:54:11,985 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.29it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.07it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.11it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:24,  3.65it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.30it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.33it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.38it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.27it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.10it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.90it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.08it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.98it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.86it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:28,  2.70it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.80it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.89it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.98it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.02it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:24,  3.04it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.07it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.09it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.12it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.02it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.10it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.12it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.01it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.86it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.04it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.06it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.10it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.12it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.21it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:17,  3.17it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.18it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.14it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.14it/s]\u001b[A\n",
            " 44% 41/93 [00:13<00:16,  3.15it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.15it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.17it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.15it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.15it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.28it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.45it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.37it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.11it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.14it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.02it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.86it/s]\u001b[A\n",
            " 57% 53/93 [00:16<00:14,  2.70it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.83it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.89it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.01it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:12,  2.78it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.85it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.75it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.63it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.60it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.78it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.87it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.92it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.99it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.02it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.12it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.17it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.15it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.02it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.85it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.92it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.81it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.07it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.10it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  3.01it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.04it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.87it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.06it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.14it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.15it/s]\u001b[A\n",
            " 89% 83/93 [00:27<00:03,  3.17it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.91it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.95it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  3.00it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.80it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.74it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.75it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.85it/s]\u001b[A\n",
            " 98% 91/93 [00:29<00:00,  2.75it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.84it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.23it/s]\u001b[A\n",
            "{'eval_loss': 1.4767152070999146, 'eval_accuracy': 0.4838, 'eval_runtime': 30.8273, 'eval_samples_per_second': 30.038, 'eval_steps_per_second': 3.017, 'epoch': 2.0}\n",
            "\n",
            " 40% 3254/8135 [36:12<48:36,  1.67it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 15:54:42,815 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-3254\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 15:54:42,816 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-3254/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 15:54:48,109 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-3254/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 15:54:48,110 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-3254/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 15:54:48,111 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-3254/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 15:54:59,472 >> Deleting older checkpoint [saved/qasc/mcq/roberta-large/checkpoint-1627] due to args.save_total_limit\n",
            "{'loss': 1.5568, 'learning_rate': 1.7092808850645358e-06, 'epoch': 2.15}\n",
            "{'loss': 1.4548, 'learning_rate': 1.5248924400737553e-06, 'epoch': 2.46}\n",
            "{'loss': 1.459, 'learning_rate': 1.3405039950829748e-06, 'epoch': 2.77}\n",
            " 60% 4881/8135 [53:59<32:41,  1.66it/s][INFO|trainer.py:722] 2023-01-15 16:12:30,437 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:12:30,440 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:12:30,440 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:12:30,440 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.35it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.06it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.10it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:23,  3.69it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.32it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.33it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.39it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.29it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.11it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.92it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.09it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.99it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.87it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:28,  2.71it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.82it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.91it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.98it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.03it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:23,  3.05it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.07it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.10it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.13it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.02it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.03it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.11it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.14it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.02it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.86it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.03it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.05it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.09it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.11it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.19it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:17,  3.17it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.19it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.16it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.16it/s]\u001b[A\n",
            " 44% 41/93 [00:12<00:16,  3.18it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.18it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.18it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.16it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.15it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.28it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.45it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.38it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.11it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.13it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.02it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.86it/s]\u001b[A\n",
            " 57% 53/93 [00:16<00:14,  2.70it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.83it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.91it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.02it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:12,  2.77it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.86it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.75it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.62it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.60it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.78it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.86it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.93it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.99it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.03it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.12it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.16it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.02it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.85it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.92it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.80it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.06it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.10it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  3.00it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.01it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.85it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.04it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.11it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.12it/s]\u001b[A\n",
            " 89% 83/93 [00:27<00:03,  3.14it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.89it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.94it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  2.98it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.77it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.72it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.74it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.83it/s]\u001b[A\n",
            " 98% 91/93 [00:29<00:00,  2.74it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.83it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.21it/s]\u001b[A\n",
            "{'eval_loss': 1.4598098993301392, 'eval_accuracy': 0.4914, 'eval_runtime': 30.8214, 'eval_samples_per_second': 30.044, 'eval_steps_per_second': 3.017, 'epoch': 3.0}\n",
            "\n",
            " 60% 4881/8135 [54:30<32:41,  1.66it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:13:01,267 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-4881\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:13:01,269 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-4881/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:13:06,573 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-4881/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:13:06,574 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-4881/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:13:06,574 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-4881/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:13:17,905 >> Deleting older checkpoint [saved/qasc/mcq/roberta-large/checkpoint-3254] due to args.save_total_limit\n",
            "{'loss': 1.38, 'learning_rate': 1.1561155500921944e-06, 'epoch': 3.07}\n",
            "{'loss': 1.3165, 'learning_rate': 9.717271051014137e-07, 'epoch': 3.38}\n",
            "{'loss': 1.3006, 'learning_rate': 7.87338660110633e-07, 'epoch': 3.69}\n",
            "{'loss': 1.288, 'learning_rate': 6.029502151198525e-07, 'epoch': 4.0}\n",
            " 80% 6508/8135 [1:12:19<16:55,  1.60it/s][INFO|trainer.py:722] 2023-01-15 16:30:49,972 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:30:49,975 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:30:49,975 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:30:49,975 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.39it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.02it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.09it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:24,  3.66it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.29it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.32it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.38it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.27it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.10it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.91it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.08it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.98it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.86it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:28,  2.70it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.80it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.90it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.99it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.02it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:24,  3.04it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.07it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.10it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.12it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.02it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.10it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.11it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.01it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.86it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.02it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.05it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.09it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.12it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.20it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:18,  3.22it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:18,  3.16it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.19it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.16it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.16it/s]\u001b[A\n",
            " 44% 41/93 [00:13<00:16,  3.17it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.17it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.18it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.16it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.15it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.28it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.45it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.36it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.11it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.12it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.02it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.87it/s]\u001b[A\n",
            " 57% 53/93 [00:16<00:14,  2.70it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.83it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.90it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.02it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:13,  2.76it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.86it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.76it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.63it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.61it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.78it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.86it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.93it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.99it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.02it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.13it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.17it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.15it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.02it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.85it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.93it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.80it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.07it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.11it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  3.01it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.04it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.88it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.06it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.13it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.14it/s]\u001b[A\n",
            " 89% 83/93 [00:27<00:03,  3.16it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.91it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.95it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  2.99it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.79it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.73it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.75it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.85it/s]\u001b[A\n",
            " 98% 91/93 [00:29<00:00,  2.75it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.84it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.23it/s]\u001b[A\n",
            "{'eval_loss': 1.5148590803146362, 'eval_accuracy': 0.4946, 'eval_runtime': 30.8167, 'eval_samples_per_second': 30.049, 'eval_steps_per_second': 3.018, 'epoch': 4.0}\n",
            "\n",
            " 80% 6508/8135 [1:12:50<16:55,  1.60it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:31:20,798 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-6508\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:31:20,800 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-6508/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:31:26,093 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-6508/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:31:26,094 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-6508/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:31:26,094 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-6508/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:31:37,495 >> Deleting older checkpoint [saved/qasc/mcq/roberta-large/checkpoint-4881] due to args.save_total_limit\n",
            "{'loss': 1.1974, 'learning_rate': 4.185617701290719e-07, 'epoch': 4.3}\n",
            "{'loss': 1.1527, 'learning_rate': 2.3417332513829134e-07, 'epoch': 4.61}\n",
            "{'loss': 1.1758, 'learning_rate': 4.978488014751076e-08, 'epoch': 4.92}\n",
            "100% 8135/8135 [1:30:36<00:00,  1.66it/s][INFO|trainer.py:722] 2023-01-15 16:49:07,333 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:49:07,337 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:49:07,337 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:49:07,337 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.69it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.10it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.12it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:23,  3.69it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.32it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.33it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.39it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.28it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.09it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.91it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.08it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.97it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.86it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:28,  2.70it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.81it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.91it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.99it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.03it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:23,  3.06it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.08it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.11it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.13it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.03it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.12it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.14it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.03it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.87it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.04it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.05it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.09it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.13it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.20it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:17,  3.18it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.24it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.19it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.15it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.15it/s]\u001b[A\n",
            " 44% 41/93 [00:12<00:16,  3.16it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.16it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.17it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.17it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.16it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.28it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.46it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.37it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.12it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.14it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.03it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.87it/s]\u001b[A\n",
            " 57% 53/93 [00:16<00:14,  2.70it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.83it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.91it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.01it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:13,  2.76it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.85it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.75it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.62it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.60it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.77it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.86it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.91it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.98it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.03it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.12it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.17it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.15it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.01it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.84it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.92it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.80it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.05it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.10it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  3.00it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.03it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.87it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.05it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.14it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.15it/s]\u001b[A\n",
            " 89% 83/93 [00:26<00:03,  3.16it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.90it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.95it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  2.99it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.78it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.73it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.75it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.83it/s]\u001b[A\n",
            " 98% 91/93 [00:29<00:00,  2.74it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.83it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.23it/s]\u001b[A\n",
            "{'eval_loss': 1.5644265413284302, 'eval_accuracy': 0.4914, 'eval_runtime': 30.8012, 'eval_samples_per_second': 30.064, 'eval_steps_per_second': 3.019, 'epoch': 5.0}\n",
            "\n",
            "100% 8135/8135 [1:31:07<00:00,  1.66it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:49:38,141 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-8135\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:49:38,142 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-8135/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:49:43,439 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-8135/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:49:43,440 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-8135/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:49:43,441 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-8135/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:49:54,800 >> Deleting older checkpoint [saved/qasc/mcq/roberta-large/checkpoint-6508] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2023-01-15 16:49:54,864 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5486.879, 'train_samples_per_second': 7.412, 'train_steps_per_second': 1.483, 'train_loss': 1.5372323212626526, 'epoch': 5.0}\n",
            "100% 8135/8135 [1:31:24<00:00,  1.48it/s]\n",
            "[INFO|trainer.py:2640] 2023-01-15 16:49:54,870 >> Saving model checkpoint to saved/qasc/mcq/roberta-large\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:49:54,871 >> Configuration saved in saved/qasc/mcq/roberta-large/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:50:00,442 >> Model weights saved in saved/qasc/mcq/roberta-large/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:50:00,443 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:50:00,444 >> Special tokens file saved in saved/qasc/mcq/roberta-large/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     1.5372\n",
            "  train_runtime            = 1:31:26.87\n",
            "  train_samples            =       8134\n",
            "  train_samples_per_second =      7.412\n",
            "  train_steps_per_second   =      1.483\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-15 16:50:00,574 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:50:00,578 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:50:00,578 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:50:00,579 >>   Batch size = 10\n",
            "100% 93/93 [00:29<00:00,  3.11it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.4914\n",
            "  eval_loss               =     1.5644\n",
            "  eval_runtime            = 0:00:30.32\n",
            "  eval_samples            =        926\n",
            "  eval_samples_per_second =     30.532\n",
            "  eval_steps_per_second   =      3.066\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:722] 2023-01-15 16:50:30,910 >> The following columns in the test set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:50:30,913 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:50:30,914 >>   Num examples = 920\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:50:30,914 >>   Batch size = 10\n",
            "100% 92/92 [00:30<00:00,  3.04it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▂▁▃▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▇▇▇▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▂▂▂▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▂▂▂▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ██▇▆▆▅▄▃▃▃▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.4914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.56443\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 30.3287\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 30.532\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 3.066\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 8135\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 1.1758\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.3161141648293328e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.53723\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 5486.879\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 7.412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMCQA qasc ROBERTA\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/1lvrhmk8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230115_151829-1lvrhmk8/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run_mcqa_score.py --learning_rate=3e-6 --num_train_epochs 5 --seed 42 \\\n",
        "--train_file=\"data/qasc/mcq_train.json\" --validation_file=\"data/qasc/mcq_dev.json\" --test_file=\"data/qasc/mcq_test.json\" \\\n",
        "--output_dir=\"saved/qasc/mcq/roberta-large\" --model_name_or_path=\"roberta-large\" \\\n",
        "--per_device_train_batch_size=5 --per_device_eval_batch_size=10 --weight_decay=0.005 \\\n",
        "--do_train True --do_eval True --do_predict True --evaluation_strategy=\"epoch\" --save_strategy=\"epoch\" \\\n",
        "--report_to \"wandb\" --run_name \"MCQA qasc ROBERTA\" --save_total_limit=1 --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SCORE - DeBERTa"
      ],
      "metadata": {
        "id": "j94oJz5UJa-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run_mcqa_score.py --learning_rate=3e-6 --num_train_epochs 5 --seed 42 \\\n",
        "--train_file=\"data/qasc/mcq_train.json\" --validation_file=\"data/qasc/mcq_dev.json\" --test_file=\"data/qasc/mcq_test.json\" \\\n",
        "--output_dir=\"saved/qasc/mcq/deberta-large\" --model_name_or_path=\"deberta-large\" \\\n",
        "--per_device_train_batch_size=15 --per_device_eval_batch_size=10 --weight_decay=0.005 \\\n",
        "--do_train True --do_eval True --do_predict True --evaluation_strategy=\"epoch\" --save_strategy=\"epoch\" \\\n",
        "--report_to \"wandb\" --run_name \"MCQA qasc DEBERTA\" --save_total_limit=1 --overwrite_output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTurQsDJd2p",
        "outputId": "729d1a41-2023-4c57-9ede-a1a96b5ab017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-27 12:14:51.030182: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved/qasc/mcq/deberta-large/runs/Jan27_12-14-53_f1ddfecb8ec5,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=saved/qasc/mcq/deberta-large,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=10,\n",
            "per_device_train_batch_size=15,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=MCQA qasc DEBERTA,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.005,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-d96b148594059fd5\n",
            "INFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-d96b148594059fd5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d96b148594059fd5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 13981.01it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1874.97it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d96b148594059fd5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 747.65it/s]\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 12:14:54,361 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 12:14:54,362 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 12:14:54,637 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 12:14:54,638 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 12:14:55,444 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/6386fc34376768db39488179803c16268ff12ee177a43a993690f66b7d7a0b7c.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 12:14:55,444 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 12:14:55,444 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 12:14:55,444 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 12:14:55,444 >> loading file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/cae8294cb38511dc11086c090549f0a079bc5537a0f9a482d8358f17acc8cff0.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 12:14:55,573 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 12:14:55,574 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils.py:426] 2023-01-27 12:14:56,093 >> Adding [MASK] to the vocabulary\n",
            "[WARNING|logging.py:279] 2023-01-27 12:14:56,093 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 12:14:56,229 >> loading configuration file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5d66efa509542e643c08a1579633e747d1697b1bec7de32c51c6969a16e81b9.3554ddad32be74b53d95a4b5760f07a2cd799268a921ae9437b1ee7a47adebc9\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 12:14:56,229 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:279] 2023-01-27 12:14:56,527 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|modeling_utils.py:2041] 2023-01-27 12:14:56,675 >> loading weights file https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/eed737dd80585a756b0286a093059c2b4403b98a17ac2cb50cda7799c653fc11.e38140a56995392eade33ad2835bb905412b65ba305475bd577c00edb10c45d9\n",
            "[WARNING|modeling_utils.py:2425] 2023-01-27 12:15:00,786 >> Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMultipleChoice: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2437] 2023-01-27 12:15:00,786 >> Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:__main__:The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\n",
            "  0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d96b148594059fd5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-378a8c0b67541bbc.arrow\n",
            "100% 9/9 [00:02<00:00,  4.04ba/s]\n",
            "  0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d96b148594059fd5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-4f9257485737a02e.arrow\n",
            "100% 1/1 [00:00<00:00,  3.17ba/s]\n",
            "  0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d96b148594059fd5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5b263158d7812862.arrow\n",
            "100% 1/1 [00:00<00:00,  6.35ba/s]\n",
            "Epoch count 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:722] 2023-01-27 12:15:06,564 >> The following columns in the training set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3. If choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-27 12:15:06,581 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-27 12:15:06,581 >>   Num examples = 8134\n",
            "[INFO|trainer.py:1607] 2023-01-27 12:15:06,581 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2023-01-27 12:15:06,581 >>   Instantaneous batch size per device = 15\n",
            "[INFO|trainer.py:1609] 2023-01-27 12:15:06,581 >>   Total train batch size (w. parallel, distributed & accumulation) = 15\n",
            "[INFO|trainer.py:1610] 2023-01-27 12:15:06,581 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-27 12:15:06,581 >>   Total optimization steps = 2715\n",
            "[INFO|integrations.py:607] 2023-01-27 12:15:06,583 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "  0% 1/2715 [00:01<1:09:07,  1.53s/it]3\n",
            "{'loss': 1.6844, 'learning_rate': 2.447513812154696e-06, 'epoch': 0.92}\n",
            " 20% 543/2715 [05:27<17:22,  2.08it/s][INFO|trainer.py:722] 2023-01-27 12:30:57,022 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3. If choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:30:57,025 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:30:57,025 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:30:57,025 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:06, 13.74it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:08, 10.15it/s]\u001b[A\n",
            "  8% 7/93 [00:00<00:09,  8.95it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:09,  9.03it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:09,  8.85it/s]\u001b[A\n",
            " 11% 10/93 [00:01<00:09,  8.40it/s]\u001b[A\n",
            " 12% 11/93 [00:01<00:10,  8.08it/s]\u001b[A\n",
            " 13% 12/93 [00:01<00:09,  8.16it/s]\u001b[A\n",
            " 14% 13/93 [00:01<00:10,  7.91it/s]\u001b[A\n",
            " 15% 14/93 [00:01<00:10,  7.87it/s]\u001b[A\n",
            " 16% 15/93 [00:01<00:10,  7.65it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:09,  7.80it/s]\u001b[A\n",
            " 18% 17/93 [00:02<00:09,  7.78it/s]\u001b[A\n",
            " 19% 18/93 [00:02<00:09,  7.88it/s]\u001b[A\n",
            " 20% 19/93 [00:02<00:09,  7.83it/s]\u001b[A\n",
            " 22% 20/93 [00:02<00:09,  7.71it/s]\u001b[A\n",
            " 23% 21/93 [00:02<00:09,  7.59it/s]\u001b[A\n",
            " 24% 22/93 [00:02<00:09,  7.64it/s]\u001b[A\n",
            " 25% 23/93 [00:02<00:08,  7.79it/s]\u001b[A\n",
            " 26% 24/93 [00:02<00:09,  7.66it/s]\u001b[A\n",
            " 27% 25/93 [00:03<00:08,  7.79it/s]\u001b[A\n",
            " 28% 26/93 [00:03<00:08,  7.98it/s]\u001b[A\n",
            " 29% 27/93 [00:03<00:08,  8.02it/s]\u001b[A\n",
            " 30% 28/93 [00:03<00:08,  7.94it/s]\u001b[A\n",
            " 31% 29/93 [00:03<00:08,  7.76it/s]\u001b[A\n",
            " 32% 30/93 [00:03<00:07,  8.16it/s]\u001b[A\n",
            " 33% 31/93 [00:03<00:07,  8.15it/s]\u001b[A\n",
            " 34% 32/93 [00:03<00:07,  8.03it/s]\u001b[A\n",
            " 35% 33/93 [00:04<00:07,  8.05it/s]\u001b[A\n",
            " 37% 34/93 [00:04<00:07,  8.16it/s]\u001b[A\n",
            " 38% 35/93 [00:04<00:07,  8.16it/s]\u001b[A\n",
            " 39% 36/93 [00:04<00:07,  8.02it/s]\u001b[A\n",
            " 40% 37/93 [00:04<00:06,  8.16it/s]\u001b[A\n",
            " 41% 38/93 [00:04<00:06,  8.14it/s]\u001b[A\n",
            " 42% 39/93 [00:04<00:06,  8.13it/s]\u001b[A\n",
            " 43% 40/93 [00:04<00:06,  8.12it/s]\u001b[A\n",
            " 44% 41/93 [00:05<00:06,  8.18it/s]\u001b[A\n",
            " 45% 42/93 [00:05<00:06,  8.04it/s]\u001b[A\n",
            " 46% 43/93 [00:05<00:06,  7.84it/s]\u001b[A\n",
            " 47% 44/93 [00:05<00:06,  7.81it/s]\u001b[A\n",
            " 48% 45/93 [00:05<00:06,  7.79it/s]\u001b[A\n",
            " 49% 46/93 [00:05<00:05,  8.19it/s]\u001b[A\n",
            " 51% 47/93 [00:05<00:05,  8.49it/s]\u001b[A\n",
            " 52% 48/93 [00:05<00:05,  8.14it/s]\u001b[A\n",
            " 53% 49/93 [00:06<00:05,  7.92it/s]\u001b[A\n",
            " 54% 50/93 [00:06<00:05,  7.86it/s]\u001b[A\n",
            " 55% 51/93 [00:06<00:05,  7.72it/s]\u001b[A\n",
            " 56% 52/93 [00:06<00:05,  7.62it/s]\u001b[A\n",
            " 57% 53/93 [00:06<00:05,  7.29it/s]\u001b[A\n",
            " 58% 54/93 [00:06<00:05,  7.33it/s]\u001b[A\n",
            " 59% 55/93 [00:06<00:05,  7.56it/s]\u001b[A\n",
            " 60% 56/93 [00:06<00:04,  7.82it/s]\u001b[A\n",
            " 61% 57/93 [00:07<00:04,  7.40it/s]\u001b[A\n",
            " 62% 58/93 [00:07<00:04,  7.61it/s]\u001b[A\n",
            " 63% 59/93 [00:07<00:04,  7.55it/s]\u001b[A\n",
            " 65% 60/93 [00:07<00:04,  7.49it/s]\u001b[A\n",
            " 66% 61/93 [00:07<00:04,  7.38it/s]\u001b[A\n",
            " 67% 62/93 [00:07<00:04,  7.65it/s]\u001b[A\n",
            " 68% 63/93 [00:07<00:03,  7.78it/s]\u001b[A\n",
            " 69% 64/93 [00:07<00:03,  7.90it/s]\u001b[A\n",
            " 70% 65/93 [00:08<00:03,  7.73it/s]\u001b[A\n",
            " 71% 66/93 [00:08<00:03,  7.86it/s]\u001b[A\n",
            " 72% 67/93 [00:08<00:03,  7.99it/s]\u001b[A\n",
            " 73% 68/93 [00:08<00:03,  8.03it/s]\u001b[A\n",
            " 74% 69/93 [00:08<00:02,  8.07it/s]\u001b[A\n",
            " 75% 70/93 [00:08<00:02,  8.08it/s]\u001b[A\n",
            " 76% 71/93 [00:08<00:02,  7.86it/s]\u001b[A\n",
            " 77% 72/93 [00:09<00:02,  7.71it/s]\u001b[A\n",
            " 78% 73/93 [00:09<00:02,  7.83it/s]\u001b[A\n",
            " 80% 74/93 [00:09<00:02,  7.70it/s]\u001b[A\n",
            " 81% 75/93 [00:09<00:02,  8.12it/s]\u001b[A\n",
            " 82% 76/93 [00:09<00:02,  8.01it/s]\u001b[A\n",
            " 83% 77/93 [00:09<00:02,  7.83it/s]\u001b[A\n",
            " 84% 78/93 [00:09<00:01,  7.92it/s]\u001b[A\n",
            " 85% 79/93 [00:09<00:01,  7.68it/s]\u001b[A\n",
            " 86% 80/93 [00:10<00:01,  7.91it/s]\u001b[A\n",
            " 87% 81/93 [00:10<00:01,  7.99it/s]\u001b[A\n",
            " 88% 82/93 [00:10<00:01,  7.92it/s]\u001b[A\n",
            " 89% 83/93 [00:10<00:01,  8.00it/s]\u001b[A\n",
            " 90% 84/93 [00:10<00:01,  7.61it/s]\u001b[A\n",
            " 91% 85/93 [00:10<00:01,  7.65it/s]\u001b[A\n",
            " 92% 86/93 [00:10<00:00,  7.59it/s]\u001b[A\n",
            " 94% 87/93 [00:10<00:00,  7.52it/s]\u001b[A\n",
            " 95% 88/93 [00:11<00:00,  7.40it/s]\u001b[A\n",
            " 96% 89/93 [00:11<00:00,  7.50it/s]\u001b[A\n",
            " 97% 90/93 [00:11<00:00,  7.48it/s]\u001b[A\n",
            " 98% 91/93 [00:11<00:00,  7.46it/s]\u001b[A\n",
            " 99% 92/93 [00:11<00:00,  7.66it/s]\u001b[A\n",
            "{'eval_loss': 0.8729598522186279, 'eval_accuracy': 0.7419, 'eval_runtime': 11.8117, 'eval_samples_per_second': 78.397, 'eval_steps_per_second': 7.874, 'epoch': 1.0}\n",
            "\n",
            " 20% 543/2715 [05:39<17:22,  2.08it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 12:31:08,839 >> Saving model checkpoint to saved/qasc/mcq/deberta-large/checkpoint-543\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 12:31:08,840 >> Configuration saved in saved/qasc/mcq/deberta-large/checkpoint-543/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 12:31:11,481 >> Model weights saved in saved/qasc/mcq/deberta-large/checkpoint-543/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 12:31:11,482 >> tokenizer config file saved in saved/qasc/mcq/deberta-large/checkpoint-543/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 12:31:11,482 >> Special tokens file saved in saved/qasc/mcq/deberta-large/checkpoint-543/special_tokens_map.json\n",
            "{'loss': 1.1555, 'learning_rate': 1.8950276243093924e-06, 'epoch': 1.84}\n",
            " 40% 1086/2715 [11:18<13:30,  2.01it/s][INFO|trainer.py:722] 2023-01-27 12:36:47,935 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3. If choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:36:47,937 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:36:47,937 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:36:47,938 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:06, 13.64it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:08, 10.14it/s]\u001b[A\n",
            "  8% 7/93 [00:00<00:09,  8.94it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:09,  9.02it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:09,  8.85it/s]\u001b[A\n",
            " 11% 10/93 [00:01<00:09,  8.41it/s]\u001b[A\n",
            " 12% 11/93 [00:01<00:10,  8.10it/s]\u001b[A\n",
            " 13% 12/93 [00:01<00:09,  8.17it/s]\u001b[A\n",
            " 14% 13/93 [00:01<00:10,  7.92it/s]\u001b[A\n",
            " 15% 14/93 [00:01<00:10,  7.87it/s]\u001b[A\n",
            " 16% 15/93 [00:01<00:10,  7.65it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:09,  7.79it/s]\u001b[A\n",
            " 18% 17/93 [00:02<00:09,  7.78it/s]\u001b[A\n",
            " 19% 18/93 [00:02<00:09,  7.89it/s]\u001b[A\n",
            " 20% 19/93 [00:02<00:09,  7.85it/s]\u001b[A\n",
            " 22% 20/93 [00:02<00:09,  7.72it/s]\u001b[A\n",
            " 23% 21/93 [00:02<00:09,  7.61it/s]\u001b[A\n",
            " 24% 22/93 [00:02<00:09,  7.65it/s]\u001b[A\n",
            " 25% 23/93 [00:02<00:08,  7.80it/s]\u001b[A\n",
            " 26% 24/93 [00:02<00:09,  7.65it/s]\u001b[A\n",
            " 27% 25/93 [00:03<00:08,  7.79it/s]\u001b[A\n",
            " 28% 26/93 [00:03<00:08,  7.98it/s]\u001b[A\n",
            " 29% 27/93 [00:03<00:08,  8.02it/s]\u001b[A\n",
            " 30% 28/93 [00:03<00:08,  7.94it/s]\u001b[A\n",
            " 31% 29/93 [00:03<00:08,  7.69it/s]\u001b[A\n",
            " 32% 30/93 [00:03<00:07,  8.10it/s]\u001b[A\n",
            " 33% 31/93 [00:03<00:07,  8.11it/s]\u001b[A\n",
            " 34% 32/93 [00:03<00:07,  8.00it/s]\u001b[A\n",
            " 35% 33/93 [00:04<00:07,  8.04it/s]\u001b[A\n",
            " 37% 34/93 [00:04<00:07,  8.17it/s]\u001b[A\n",
            " 38% 35/93 [00:04<00:07,  8.17it/s]\u001b[A\n",
            " 39% 36/93 [00:04<00:07,  8.04it/s]\u001b[A\n",
            " 40% 37/93 [00:04<00:06,  8.17it/s]\u001b[A\n",
            " 41% 38/93 [00:04<00:06,  8.16it/s]\u001b[A\n",
            " 42% 39/93 [00:04<00:06,  8.15it/s]\u001b[A\n",
            " 43% 40/93 [00:04<00:06,  8.14it/s]\u001b[A\n",
            " 44% 41/93 [00:05<00:06,  8.21it/s]\u001b[A\n",
            " 45% 42/93 [00:05<00:06,  8.06it/s]\u001b[A\n",
            " 46% 43/93 [00:05<00:06,  7.86it/s]\u001b[A\n",
            " 47% 44/93 [00:05<00:06,  7.82it/s]\u001b[A\n",
            " 48% 45/93 [00:05<00:06,  7.80it/s]\u001b[A\n",
            " 49% 46/93 [00:05<00:05,  8.19it/s]\u001b[A\n",
            " 51% 47/93 [00:05<00:05,  8.50it/s]\u001b[A\n",
            " 52% 48/93 [00:05<00:05,  8.15it/s]\u001b[A\n",
            " 53% 49/93 [00:06<00:05,  7.92it/s]\u001b[A\n",
            " 54% 50/93 [00:06<00:05,  7.87it/s]\u001b[A\n",
            " 55% 51/93 [00:06<00:05,  7.71it/s]\u001b[A\n",
            " 56% 52/93 [00:06<00:05,  7.61it/s]\u001b[A\n",
            " 57% 53/93 [00:06<00:05,  7.28it/s]\u001b[A\n",
            " 58% 54/93 [00:06<00:05,  7.33it/s]\u001b[A\n",
            " 59% 55/93 [00:06<00:05,  7.55it/s]\u001b[A\n",
            " 60% 56/93 [00:06<00:04,  7.82it/s]\u001b[A\n",
            " 61% 57/93 [00:07<00:04,  7.41it/s]\u001b[A\n",
            " 62% 58/93 [00:07<00:04,  7.62it/s]\u001b[A\n",
            " 63% 59/93 [00:07<00:04,  7.56it/s]\u001b[A\n",
            " 65% 60/93 [00:07<00:04,  7.50it/s]\u001b[A\n",
            " 66% 61/93 [00:07<00:04,  7.39it/s]\u001b[A\n",
            " 67% 62/93 [00:07<00:04,  7.66it/s]\u001b[A\n",
            " 68% 63/93 [00:07<00:03,  7.79it/s]\u001b[A\n",
            " 69% 64/93 [00:07<00:03,  7.90it/s]\u001b[A\n",
            " 70% 65/93 [00:08<00:03,  7.74it/s]\u001b[A\n",
            " 71% 66/93 [00:08<00:03,  7.84it/s]\u001b[A\n",
            " 72% 67/93 [00:08<00:03,  7.99it/s]\u001b[A\n",
            " 73% 68/93 [00:08<00:03,  8.04it/s]\u001b[A\n",
            " 74% 69/93 [00:08<00:02,  8.07it/s]\u001b[A\n",
            " 75% 70/93 [00:08<00:02,  8.09it/s]\u001b[A\n",
            " 76% 71/93 [00:08<00:02,  7.86it/s]\u001b[A\n",
            " 77% 72/93 [00:09<00:02,  7.71it/s]\u001b[A\n",
            " 78% 73/93 [00:09<00:02,  7.81it/s]\u001b[A\n",
            " 80% 74/93 [00:09<00:02,  7.68it/s]\u001b[A\n",
            " 81% 75/93 [00:09<00:02,  8.10it/s]\u001b[A\n",
            " 82% 76/93 [00:09<00:02,  8.00it/s]\u001b[A\n",
            " 83% 77/93 [00:09<00:02,  7.81it/s]\u001b[A\n",
            " 84% 78/93 [00:09<00:01,  7.90it/s]\u001b[A\n",
            " 85% 79/93 [00:09<00:01,  7.66it/s]\u001b[A\n",
            " 86% 80/93 [00:10<00:01,  7.89it/s]\u001b[A\n",
            " 87% 81/93 [00:10<00:01,  7.97it/s]\u001b[A\n",
            " 88% 82/93 [00:10<00:01,  7.90it/s]\u001b[A\n",
            " 89% 83/93 [00:10<00:01,  7.98it/s]\u001b[A\n",
            " 90% 84/93 [00:10<00:01,  7.59it/s]\u001b[A\n",
            " 91% 85/93 [00:10<00:01,  7.64it/s]\u001b[A\n",
            " 92% 86/93 [00:10<00:00,  7.58it/s]\u001b[A\n",
            " 94% 87/93 [00:10<00:00,  7.51it/s]\u001b[A\n",
            " 95% 88/93 [00:11<00:00,  7.40it/s]\u001b[A\n",
            " 96% 89/93 [00:11<00:00,  7.51it/s]\u001b[A\n",
            " 97% 90/93 [00:11<00:00,  7.48it/s]\u001b[A\n",
            " 98% 91/93 [00:11<00:00,  7.46it/s]\u001b[A\n",
            " 99% 92/93 [00:11<00:00,  7.66it/s]\u001b[A\n",
            "{'eval_loss': 0.7777447700500488, 'eval_accuracy': 0.7462, 'eval_runtime': 11.8122, 'eval_samples_per_second': 78.394, 'eval_steps_per_second': 7.873, 'epoch': 2.0}\n",
            "\n",
            " 40% 1086/2715 [11:30<13:30,  2.01it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 12:36:59,751 >> Saving model checkpoint to saved/qasc/mcq/deberta-large/checkpoint-1086\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 12:36:59,752 >> Configuration saved in saved/qasc/mcq/deberta-large/checkpoint-1086/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 12:37:02,114 >> Model weights saved in saved/qasc/mcq/deberta-large/checkpoint-1086/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 12:37:02,115 >> tokenizer config file saved in saved/qasc/mcq/deberta-large/checkpoint-1086/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 12:37:02,116 >> Special tokens file saved in saved/qasc/mcq/deberta-large/checkpoint-1086/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 12:37:11,066 >> Deleting older checkpoint [saved/qasc/mcq/deberta-large/checkpoint-543] due to args.save_total_limit\n",
            "{'loss': 0.9311, 'learning_rate': 1.3425414364640885e-06, 'epoch': 2.76}\n",
            " 60% 1629/2715 [17:09<08:41,  2.08it/s][INFO|trainer.py:722] 2023-01-27 12:42:39,056 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3. If choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:42:39,059 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:42:39,059 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:42:39,059 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:06, 13.74it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:08, 10.16it/s]\u001b[A\n",
            "  8% 7/93 [00:00<00:09,  8.95it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:09,  9.02it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:09,  8.84it/s]\u001b[A\n",
            " 11% 10/93 [00:01<00:09,  8.39it/s]\u001b[A\n",
            " 12% 11/93 [00:01<00:10,  8.08it/s]\u001b[A\n",
            " 13% 12/93 [00:01<00:09,  8.11it/s]\u001b[A\n",
            " 14% 13/93 [00:01<00:10,  7.88it/s]\u001b[A\n",
            " 15% 14/93 [00:01<00:10,  7.71it/s]\u001b[A\n",
            " 16% 15/93 [00:01<00:10,  7.53it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:09,  7.70it/s]\u001b[A\n",
            " 18% 17/93 [00:02<00:09,  7.71it/s]\u001b[A\n",
            " 19% 18/93 [00:02<00:09,  7.83it/s]\u001b[A\n",
            " 20% 19/93 [00:02<00:09,  7.81it/s]\u001b[A\n",
            " 22% 20/93 [00:02<00:09,  7.68it/s]\u001b[A\n",
            " 23% 21/93 [00:02<00:09,  7.48it/s]\u001b[A\n",
            " 24% 22/93 [00:02<00:09,  7.55it/s]\u001b[A\n",
            " 25% 23/93 [00:02<00:09,  7.73it/s]\u001b[A\n",
            " 26% 24/93 [00:02<00:09,  7.61it/s]\u001b[A\n",
            " 27% 25/93 [00:03<00:08,  7.74it/s]\u001b[A\n",
            " 28% 26/93 [00:03<00:08,  7.94it/s]\u001b[A\n",
            " 29% 27/93 [00:03<00:08,  7.99it/s]\u001b[A\n",
            " 30% 28/93 [00:03<00:08,  7.92it/s]\u001b[A\n",
            " 31% 29/93 [00:03<00:08,  7.71it/s]\u001b[A\n",
            " 32% 30/93 [00:03<00:07,  8.12it/s]\u001b[A\n",
            " 33% 31/93 [00:03<00:07,  8.12it/s]\u001b[A\n",
            " 34% 32/93 [00:03<00:07,  7.99it/s]\u001b[A\n",
            " 35% 33/93 [00:04<00:07,  8.02it/s]\u001b[A\n",
            " 37% 34/93 [00:04<00:07,  8.15it/s]\u001b[A\n",
            " 38% 35/93 [00:04<00:07,  8.15it/s]\u001b[A\n",
            " 39% 36/93 [00:04<00:07,  8.02it/s]\u001b[A\n",
            " 40% 37/93 [00:04<00:06,  8.15it/s]\u001b[A\n",
            " 41% 38/93 [00:04<00:06,  8.14it/s]\u001b[A\n",
            " 42% 39/93 [00:04<00:06,  8.13it/s]\u001b[A\n",
            " 43% 40/93 [00:04<00:06,  8.12it/s]\u001b[A\n",
            " 44% 41/93 [00:05<00:06,  8.19it/s]\u001b[A\n",
            " 45% 42/93 [00:05<00:06,  8.03it/s]\u001b[A\n",
            " 46% 43/93 [00:05<00:06,  7.84it/s]\u001b[A\n",
            " 47% 44/93 [00:05<00:06,  7.82it/s]\u001b[A\n",
            " 48% 45/93 [00:05<00:06,  7.80it/s]\u001b[A\n",
            " 49% 46/93 [00:05<00:05,  8.20it/s]\u001b[A\n",
            " 51% 47/93 [00:05<00:05,  8.50it/s]\u001b[A\n",
            " 52% 48/93 [00:05<00:05,  8.15it/s]\u001b[A\n",
            " 53% 49/93 [00:06<00:05,  7.92it/s]\u001b[A\n",
            " 54% 50/93 [00:06<00:05,  7.86it/s]\u001b[A\n",
            " 55% 51/93 [00:06<00:05,  7.71it/s]\u001b[A\n",
            " 56% 52/93 [00:06<00:05,  7.61it/s]\u001b[A\n",
            " 57% 53/93 [00:06<00:05,  7.28it/s]\u001b[A\n",
            " 58% 54/93 [00:06<00:05,  7.32it/s]\u001b[A\n",
            " 59% 55/93 [00:06<00:05,  7.55it/s]\u001b[A\n",
            " 60% 56/93 [00:06<00:04,  7.81it/s]\u001b[A\n",
            " 61% 57/93 [00:07<00:04,  7.40it/s]\u001b[A\n",
            " 62% 58/93 [00:07<00:04,  7.62it/s]\u001b[A\n",
            " 63% 59/93 [00:07<00:04,  7.55it/s]\u001b[A\n",
            " 65% 60/93 [00:07<00:04,  7.49it/s]\u001b[A\n",
            " 66% 61/93 [00:07<00:04,  7.38it/s]\u001b[A\n",
            " 67% 62/93 [00:07<00:04,  7.65it/s]\u001b[A\n",
            " 68% 63/93 [00:07<00:03,  7.78it/s]\u001b[A\n",
            " 69% 64/93 [00:08<00:03,  7.90it/s]\u001b[A\n",
            " 70% 65/93 [00:08<00:03,  7.73it/s]\u001b[A\n",
            " 71% 66/93 [00:08<00:03,  7.86it/s]\u001b[A\n",
            " 72% 67/93 [00:08<00:03,  8.00it/s]\u001b[A\n",
            " 73% 68/93 [00:08<00:03,  8.04it/s]\u001b[A\n",
            " 74% 69/93 [00:08<00:02,  8.07it/s]\u001b[A\n",
            " 75% 70/93 [00:08<00:02,  8.09it/s]\u001b[A\n",
            " 76% 71/93 [00:08<00:02,  7.86it/s]\u001b[A\n",
            " 77% 72/93 [00:09<00:02,  7.71it/s]\u001b[A\n",
            " 78% 73/93 [00:09<00:02,  7.83it/s]\u001b[A\n",
            " 80% 74/93 [00:09<00:02,  7.69it/s]\u001b[A\n",
            " 81% 75/93 [00:09<00:02,  8.12it/s]\u001b[A\n",
            " 82% 76/93 [00:09<00:02,  8.00it/s]\u001b[A\n",
            " 83% 77/93 [00:09<00:02,  7.82it/s]\u001b[A\n",
            " 84% 78/93 [00:09<00:01,  7.91it/s]\u001b[A\n",
            " 85% 79/93 [00:09<00:01,  7.67it/s]\u001b[A\n",
            " 86% 80/93 [00:10<00:01,  7.90it/s]\u001b[A\n",
            " 87% 81/93 [00:10<00:01,  7.98it/s]\u001b[A\n",
            " 88% 82/93 [00:10<00:01,  7.91it/s]\u001b[A\n",
            " 89% 83/93 [00:10<00:01,  7.99it/s]\u001b[A\n",
            " 90% 84/93 [00:10<00:01,  7.60it/s]\u001b[A\n",
            " 91% 85/93 [00:10<00:01,  7.64it/s]\u001b[A\n",
            " 92% 86/93 [00:10<00:00,  7.57it/s]\u001b[A\n",
            " 94% 87/93 [00:10<00:00,  7.50it/s]\u001b[A\n",
            " 95% 88/93 [00:11<00:00,  7.39it/s]\u001b[A\n",
            " 96% 89/93 [00:11<00:00,  7.50it/s]\u001b[A\n",
            " 97% 90/93 [00:11<00:00,  7.48it/s]\u001b[A\n",
            " 98% 91/93 [00:11<00:00,  7.45it/s]\u001b[A\n",
            " 99% 92/93 [00:11<00:00,  7.66it/s]\u001b[A\n",
            "{'eval_loss': 0.7886231541633606, 'eval_accuracy': 0.7354, 'eval_runtime': 11.8329, 'eval_samples_per_second': 78.256, 'eval_steps_per_second': 7.859, 'epoch': 3.0}\n",
            "\n",
            " 60% 1629/2715 [17:21<08:41,  2.08it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 12:42:50,893 >> Saving model checkpoint to saved/qasc/mcq/deberta-large/checkpoint-1629\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 12:42:50,894 >> Configuration saved in saved/qasc/mcq/deberta-large/checkpoint-1629/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 12:42:53,394 >> Model weights saved in saved/qasc/mcq/deberta-large/checkpoint-1629/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 12:42:53,395 >> tokenizer config file saved in saved/qasc/mcq/deberta-large/checkpoint-1629/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 12:42:53,395 >> Special tokens file saved in saved/qasc/mcq/deberta-large/checkpoint-1629/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 12:43:01,830 >> Deleting older checkpoint [saved/qasc/mcq/deberta-large/checkpoint-1086] due to args.save_total_limit\n",
            "{'loss': 0.7857, 'learning_rate': 7.900552486187845e-07, 'epoch': 3.68}\n",
            " 80% 2172/2715 [23:00<04:23,  2.06it/s][INFO|trainer.py:722] 2023-01-27 12:48:29,901 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3. If choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:48:29,903 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:48:29,903 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:48:29,903 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:06, 13.77it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:08, 10.16it/s]\u001b[A\n",
            "  8% 7/93 [00:00<00:09,  8.95it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:09,  9.02it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:09,  8.85it/s]\u001b[A\n",
            " 11% 10/93 [00:01<00:09,  8.40it/s]\u001b[A\n",
            " 12% 11/93 [00:01<00:10,  8.08it/s]\u001b[A\n",
            " 13% 12/93 [00:01<00:09,  8.13it/s]\u001b[A\n",
            " 14% 13/93 [00:01<00:10,  7.89it/s]\u001b[A\n",
            " 15% 14/93 [00:01<00:10,  7.85it/s]\u001b[A\n",
            " 16% 15/93 [00:01<00:10,  7.63it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:09,  7.78it/s]\u001b[A\n",
            " 18% 17/93 [00:02<00:09,  7.77it/s]\u001b[A\n",
            " 19% 18/93 [00:02<00:09,  7.89it/s]\u001b[A\n",
            " 20% 19/93 [00:02<00:09,  7.84it/s]\u001b[A\n",
            " 22% 20/93 [00:02<00:09,  7.72it/s]\u001b[A\n",
            " 23% 21/93 [00:02<00:09,  7.61it/s]\u001b[A\n",
            " 24% 22/93 [00:02<00:09,  7.65it/s]\u001b[A\n",
            " 25% 23/93 [00:02<00:08,  7.80it/s]\u001b[A\n",
            " 26% 24/93 [00:02<00:08,  7.67it/s]\u001b[A\n",
            " 27% 25/93 [00:03<00:08,  7.80it/s]\u001b[A\n",
            " 28% 26/93 [00:03<00:08,  8.00it/s]\u001b[A\n",
            " 29% 27/93 [00:03<00:08,  8.03it/s]\u001b[A\n",
            " 30% 28/93 [00:03<00:08,  7.95it/s]\u001b[A\n",
            " 31% 29/93 [00:03<00:08,  7.76it/s]\u001b[A\n",
            " 32% 30/93 [00:03<00:07,  8.17it/s]\u001b[A\n",
            " 33% 31/93 [00:03<00:07,  8.16it/s]\u001b[A\n",
            " 34% 32/93 [00:03<00:07,  8.04it/s]\u001b[A\n",
            " 35% 33/93 [00:04<00:07,  8.06it/s]\u001b[A\n",
            " 37% 34/93 [00:04<00:07,  8.19it/s]\u001b[A\n",
            " 38% 35/93 [00:04<00:07,  8.19it/s]\u001b[A\n",
            " 39% 36/93 [00:04<00:07,  8.04it/s]\u001b[A\n",
            " 40% 37/93 [00:04<00:06,  8.18it/s]\u001b[A\n",
            " 41% 38/93 [00:04<00:06,  8.16it/s]\u001b[A\n",
            " 42% 39/93 [00:04<00:06,  8.16it/s]\u001b[A\n",
            " 43% 40/93 [00:04<00:06,  8.15it/s]\u001b[A\n",
            " 44% 41/93 [00:05<00:06,  8.21it/s]\u001b[A\n",
            " 45% 42/93 [00:05<00:06,  8.07it/s]\u001b[A\n",
            " 46% 43/93 [00:05<00:06,  7.86it/s]\u001b[A\n",
            " 47% 44/93 [00:05<00:06,  7.83it/s]\u001b[A\n",
            " 48% 45/93 [00:05<00:06,  7.80it/s]\u001b[A\n",
            " 49% 46/93 [00:05<00:05,  8.19it/s]\u001b[A\n",
            " 51% 47/93 [00:05<00:05,  8.49it/s]\u001b[A\n",
            " 52% 48/93 [00:05<00:05,  8.14it/s]\u001b[A\n",
            " 53% 49/93 [00:06<00:05,  7.92it/s]\u001b[A\n",
            " 54% 50/93 [00:06<00:05,  7.86it/s]\u001b[A\n",
            " 55% 51/93 [00:06<00:05,  7.72it/s]\u001b[A\n",
            " 56% 52/93 [00:06<00:05,  7.61it/s]\u001b[A\n",
            " 57% 53/93 [00:06<00:05,  7.28it/s]\u001b[A\n",
            " 58% 54/93 [00:06<00:05,  7.33it/s]\u001b[A\n",
            " 59% 55/93 [00:06<00:05,  7.56it/s]\u001b[A\n",
            " 60% 56/93 [00:06<00:04,  7.82it/s]\u001b[A\n",
            " 61% 57/93 [00:07<00:04,  7.40it/s]\u001b[A\n",
            " 62% 58/93 [00:07<00:04,  7.62it/s]\u001b[A\n",
            " 63% 59/93 [00:07<00:04,  7.56it/s]\u001b[A\n",
            " 65% 60/93 [00:07<00:04,  7.49it/s]\u001b[A\n",
            " 66% 61/93 [00:07<00:04,  7.38it/s]\u001b[A\n",
            " 67% 62/93 [00:07<00:04,  7.65it/s]\u001b[A\n",
            " 68% 63/93 [00:07<00:03,  7.79it/s]\u001b[A\n",
            " 69% 64/93 [00:07<00:03,  7.90it/s]\u001b[A\n",
            " 70% 65/93 [00:08<00:03,  7.73it/s]\u001b[A\n",
            " 71% 66/93 [00:08<00:03,  7.84it/s]\u001b[A\n",
            " 72% 67/93 [00:08<00:03,  7.97it/s]\u001b[A\n",
            " 73% 68/93 [00:08<00:03,  8.02it/s]\u001b[A\n",
            " 74% 69/93 [00:08<00:02,  8.05it/s]\u001b[A\n",
            " 75% 70/93 [00:08<00:02,  8.07it/s]\u001b[A\n",
            " 76% 71/93 [00:08<00:02,  7.86it/s]\u001b[A\n",
            " 77% 72/93 [00:09<00:02,  7.71it/s]\u001b[A\n",
            " 78% 73/93 [00:09<00:02,  7.81it/s]\u001b[A\n",
            " 80% 74/93 [00:09<00:02,  7.69it/s]\u001b[A\n",
            " 81% 75/93 [00:09<00:02,  8.10it/s]\u001b[A\n",
            " 82% 76/93 [00:09<00:02,  7.99it/s]\u001b[A\n",
            " 83% 77/93 [00:09<00:02,  7.82it/s]\u001b[A\n",
            " 84% 78/93 [00:09<00:01,  7.91it/s]\u001b[A\n",
            " 85% 79/93 [00:09<00:01,  7.67it/s]\u001b[A\n",
            " 86% 80/93 [00:10<00:01,  7.90it/s]\u001b[A\n",
            " 87% 81/93 [00:10<00:01,  7.99it/s]\u001b[A\n",
            " 88% 82/93 [00:10<00:01,  7.92it/s]\u001b[A\n",
            " 89% 83/93 [00:10<00:01,  7.99it/s]\u001b[A\n",
            " 90% 84/93 [00:10<00:01,  7.60it/s]\u001b[A\n",
            " 91% 85/93 [00:10<00:01,  7.65it/s]\u001b[A\n",
            " 92% 86/93 [00:10<00:00,  7.58it/s]\u001b[A\n",
            " 94% 87/93 [00:10<00:00,  7.51it/s]\u001b[A\n",
            " 95% 88/93 [00:11<00:00,  7.40it/s]\u001b[A\n",
            " 96% 89/93 [00:11<00:00,  7.47it/s]\u001b[A\n",
            " 97% 90/93 [00:11<00:00,  7.46it/s]\u001b[A\n",
            " 98% 91/93 [00:11<00:00,  7.44it/s]\u001b[A\n",
            " 99% 92/93 [00:11<00:00,  7.64it/s]\u001b[A\n",
            "{'eval_loss': 0.788858950138092, 'eval_accuracy': 0.7343, 'eval_runtime': 11.8096, 'eval_samples_per_second': 78.411, 'eval_steps_per_second': 7.875, 'epoch': 4.0}\n",
            "\n",
            " 80% 2172/2715 [23:12<04:23,  2.06it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 12:48:41,715 >> Saving model checkpoint to saved/qasc/mcq/deberta-large/checkpoint-2172\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 12:48:41,716 >> Configuration saved in saved/qasc/mcq/deberta-large/checkpoint-2172/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 12:48:44,013 >> Model weights saved in saved/qasc/mcq/deberta-large/checkpoint-2172/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 12:48:44,014 >> tokenizer config file saved in saved/qasc/mcq/deberta-large/checkpoint-2172/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 12:48:44,014 >> Special tokens file saved in saved/qasc/mcq/deberta-large/checkpoint-2172/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 12:48:52,623 >> Deleting older checkpoint [saved/qasc/mcq/deberta-large/checkpoint-1629] due to args.save_total_limit\n",
            "{'loss': 0.7231, 'learning_rate': 2.375690607734807e-07, 'epoch': 4.6}\n",
            "100% 2715/2715 [28:51<00:00,  2.02it/s][INFO|trainer.py:722] 2023-01-27 12:54:21,235 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3. If choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:54:21,237 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:54:21,238 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:54:21,238 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:06, 13.77it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:08, 10.17it/s]\u001b[A\n",
            "  8% 7/93 [00:00<00:09,  8.95it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:09,  9.03it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:09,  8.85it/s]\u001b[A\n",
            " 11% 10/93 [00:01<00:09,  8.40it/s]\u001b[A\n",
            " 12% 11/93 [00:01<00:10,  8.09it/s]\u001b[A\n",
            " 13% 12/93 [00:01<00:09,  8.17it/s]\u001b[A\n",
            " 14% 13/93 [00:01<00:10,  7.92it/s]\u001b[A\n",
            " 15% 14/93 [00:01<00:10,  7.87it/s]\u001b[A\n",
            " 16% 15/93 [00:01<00:10,  7.66it/s]\u001b[A\n",
            " 17% 16/93 [00:01<00:10,  7.69it/s]\u001b[A\n",
            " 18% 17/93 [00:02<00:09,  7.70it/s]\u001b[A\n",
            " 19% 18/93 [00:02<00:09,  7.82it/s]\u001b[A\n",
            " 20% 19/93 [00:02<00:09,  7.80it/s]\u001b[A\n",
            " 22% 20/93 [00:02<00:09,  7.68it/s]\u001b[A\n",
            " 23% 21/93 [00:02<00:09,  7.58it/s]\u001b[A\n",
            " 24% 22/93 [00:02<00:09,  7.63it/s]\u001b[A\n",
            " 25% 23/93 [00:02<00:08,  7.78it/s]\u001b[A\n",
            " 26% 24/93 [00:02<00:09,  7.61it/s]\u001b[A\n",
            " 27% 25/93 [00:03<00:08,  7.75it/s]\u001b[A\n",
            " 28% 26/93 [00:03<00:08,  7.95it/s]\u001b[A\n",
            " 29% 27/93 [00:03<00:08,  8.00it/s]\u001b[A\n",
            " 30% 28/93 [00:03<00:08,  7.93it/s]\u001b[A\n",
            " 31% 29/93 [00:03<00:08,  7.75it/s]\u001b[A\n",
            " 32% 30/93 [00:03<00:07,  8.16it/s]\u001b[A\n",
            " 33% 31/93 [00:03<00:07,  8.15it/s]\u001b[A\n",
            " 34% 32/93 [00:03<00:07,  8.02it/s]\u001b[A\n",
            " 35% 33/93 [00:04<00:07,  8.04it/s]\u001b[A\n",
            " 37% 34/93 [00:04<00:07,  8.17it/s]\u001b[A\n",
            " 38% 35/93 [00:04<00:07,  8.17it/s]\u001b[A\n",
            " 39% 36/93 [00:04<00:07,  8.04it/s]\u001b[A\n",
            " 40% 37/93 [00:04<00:06,  8.17it/s]\u001b[A\n",
            " 41% 38/93 [00:04<00:06,  8.16it/s]\u001b[A\n",
            " 42% 39/93 [00:04<00:06,  8.15it/s]\u001b[A\n",
            " 43% 40/93 [00:04<00:06,  8.14it/s]\u001b[A\n",
            " 44% 41/93 [00:05<00:06,  8.19it/s]\u001b[A\n",
            " 45% 42/93 [00:05<00:06,  8.03it/s]\u001b[A\n",
            " 46% 43/93 [00:05<00:06,  7.82it/s]\u001b[A\n",
            " 47% 44/93 [00:05<00:06,  7.79it/s]\u001b[A\n",
            " 48% 45/93 [00:05<00:06,  7.78it/s]\u001b[A\n",
            " 49% 46/93 [00:05<00:05,  8.18it/s]\u001b[A\n",
            " 51% 47/93 [00:05<00:05,  8.45it/s]\u001b[A\n",
            " 52% 48/93 [00:05<00:05,  8.11it/s]\u001b[A\n",
            " 53% 49/93 [00:06<00:05,  7.90it/s]\u001b[A\n",
            " 54% 50/93 [00:06<00:05,  7.85it/s]\u001b[A\n",
            " 55% 51/93 [00:06<00:05,  7.72it/s]\u001b[A\n",
            " 56% 52/93 [00:06<00:05,  7.62it/s]\u001b[A\n",
            " 57% 53/93 [00:06<00:05,  7.25it/s]\u001b[A\n",
            " 58% 54/93 [00:06<00:05,  7.31it/s]\u001b[A\n",
            " 59% 55/93 [00:06<00:05,  7.55it/s]\u001b[A\n",
            " 60% 56/93 [00:06<00:04,  7.81it/s]\u001b[A\n",
            " 61% 57/93 [00:07<00:04,  7.38it/s]\u001b[A\n",
            " 62% 58/93 [00:07<00:04,  7.60it/s]\u001b[A\n",
            " 63% 59/93 [00:07<00:04,  7.55it/s]\u001b[A\n",
            " 65% 60/93 [00:07<00:04,  7.48it/s]\u001b[A\n",
            " 66% 61/93 [00:07<00:04,  7.37it/s]\u001b[A\n",
            " 67% 62/93 [00:07<00:04,  7.64it/s]\u001b[A\n",
            " 68% 63/93 [00:07<00:03,  7.78it/s]\u001b[A\n",
            " 69% 64/93 [00:08<00:03,  7.89it/s]\u001b[A\n",
            " 70% 65/93 [00:08<00:03,  7.73it/s]\u001b[A\n",
            " 71% 66/93 [00:08<00:03,  7.86it/s]\u001b[A\n",
            " 72% 67/93 [00:08<00:03,  8.01it/s]\u001b[A\n",
            " 73% 68/93 [00:08<00:03,  8.04it/s]\u001b[A\n",
            " 74% 69/93 [00:08<00:02,  8.07it/s]\u001b[A\n",
            " 75% 70/93 [00:08<00:02,  8.08it/s]\u001b[A\n",
            " 76% 71/93 [00:08<00:02,  7.86it/s]\u001b[A\n",
            " 77% 72/93 [00:09<00:02,  7.70it/s]\u001b[A\n",
            " 78% 73/93 [00:09<00:02,  7.82it/s]\u001b[A\n",
            " 80% 74/93 [00:09<00:02,  7.70it/s]\u001b[A\n",
            " 81% 75/93 [00:09<00:02,  8.12it/s]\u001b[A\n",
            " 82% 76/93 [00:09<00:02,  8.00it/s]\u001b[A\n",
            " 83% 77/93 [00:09<00:02,  7.82it/s]\u001b[A\n",
            " 84% 78/93 [00:09<00:01,  7.91it/s]\u001b[A\n",
            " 85% 79/93 [00:09<00:01,  7.67it/s]\u001b[A\n",
            " 86% 80/93 [00:10<00:01,  7.89it/s]\u001b[A\n",
            " 87% 81/93 [00:10<00:01,  7.96it/s]\u001b[A\n",
            " 88% 82/93 [00:10<00:01,  7.90it/s]\u001b[A\n",
            " 89% 83/93 [00:10<00:01,  7.98it/s]\u001b[A\n",
            " 90% 84/93 [00:10<00:01,  7.59it/s]\u001b[A\n",
            " 91% 85/93 [00:10<00:01,  7.64it/s]\u001b[A\n",
            " 92% 86/93 [00:10<00:00,  7.57it/s]\u001b[A\n",
            " 94% 87/93 [00:10<00:00,  7.50it/s]\u001b[A\n",
            " 95% 88/93 [00:11<00:00,  7.39it/s]\u001b[A\n",
            " 96% 89/93 [00:11<00:00,  7.50it/s]\u001b[A\n",
            " 97% 90/93 [00:11<00:00,  7.47it/s]\u001b[A\n",
            " 98% 91/93 [00:11<00:00,  7.45it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.8092650175094604, 'eval_accuracy': 0.7333, 'eval_runtime': 11.8243, 'eval_samples_per_second': 78.313, 'eval_steps_per_second': 7.865, 'epoch': 5.0}\n",
            "100% 2715/2715 [29:03<00:00,  2.02it/s]\n",
            "100% 93/93 [00:11<00:00,  7.66it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-27 12:54:33,064 >> Saving model checkpoint to saved/qasc/mcq/deberta-large/checkpoint-2715\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 12:54:33,065 >> Configuration saved in saved/qasc/mcq/deberta-large/checkpoint-2715/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 12:54:35,731 >> Model weights saved in saved/qasc/mcq/deberta-large/checkpoint-2715/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 12:54:35,732 >> tokenizer config file saved in saved/qasc/mcq/deberta-large/checkpoint-2715/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 12:54:35,732 >> Special tokens file saved in saved/qasc/mcq/deberta-large/checkpoint-2715/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-27 12:54:44,354 >> Deleting older checkpoint [saved/qasc/mcq/deberta-large/checkpoint-2172] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2023-01-27 12:54:45,034 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2378.4538, 'train_samples_per_second': 17.099, 'train_steps_per_second': 1.141, 'train_loss': 1.0264310720875778, 'epoch': 5.0}\n",
            "100% 2715/2715 [29:15<00:00,  1.55it/s]\n",
            "[INFO|trainer.py:2640] 2023-01-27 12:54:45,037 >> Saving model checkpoint to saved/qasc/mcq/deberta-large\n",
            "[INFO|configuration_utils.py:451] 2023-01-27 12:54:45,039 >> Configuration saved in saved/qasc/mcq/deberta-large/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-27 12:54:48,414 >> Model weights saved in saved/qasc/mcq/deberta-large/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-27 12:54:48,415 >> tokenizer config file saved in saved/qasc/mcq/deberta-large/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-27 12:54:48,416 >> Special tokens file saved in saved/qasc/mcq/deberta-large/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     1.0264\n",
            "  train_runtime            = 0:39:38.45\n",
            "  train_samples            =       8134\n",
            "  train_samples_per_second =     17.099\n",
            "  train_steps_per_second   =      1.141\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-27 12:54:48,592 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3. If choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:54:48,595 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:54:48,595 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:54:48,595 >>   Batch size = 10\n",
            "100% 93/93 [00:11<00:00,  7.95it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.7333\n",
            "  eval_loss               =     0.8093\n",
            "  eval_runtime            = 0:00:11.85\n",
            "  eval_samples            =        926\n",
            "  eval_samples_per_second =     78.078\n",
            "  eval_steps_per_second   =      7.842\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:722] 2023-01-27 12:55:00,457 >> The following columns in the test set don't have a corresponding argument in `DebertaV2ForMultipleChoice.forward` and have been ignored: choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3. If choice5, choice7, choice2, context, choice6, choice4, choice0, choice1, choice3 are not expected by `DebertaV2ForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 12:55:00,459 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 12:55:00,459 >>   Num examples = 920\n",
            "[INFO|trainer.py:2896] 2023-01-27 12:55:00,460 >>   Batch size = 10\n",
            "100% 92/92 [00:11<00:00,  8.01it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▆█▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▁▂▂▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁▁▄▁▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ██▅█▆▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ██▅█▆▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▃▃▄▅▆▆▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▃▃▄▅▆▆▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▆▄▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▄▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.7333\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.80927\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 11.86\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 78.078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 7.842\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 2715\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.7231\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.3194124875048288e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.02643\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2378.4538\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 17.099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.141\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/TEAM/wandb/offline-run-20230127_122529-24i3palv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230127_122529-24i3palv/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7BrIL2W05kJ"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y0vy1Gz05kK"
      },
      "source": [
        "| Dataset | Model | Instance Accuracy (%)  | CLS Accuracy (%)  |\n",
        "|---|---|---|---|\n",
        "| CSQA  | TEAM RoBERTa  | 0.7428 | 0.8314 |\n",
        "| CSQA  | TEAM DeBEARTa  | 0.7666 | 0.8537 |\n",
        "| CSQA  | SCORE RoBEARTa  | 0.6536 | |\n",
        "| CSQA  | SCORE DeBEARTa  | 0.8206 | |\n",
        "| CSQA2  | TEAM RoBERTa  | 0.5171 | 0.5075 |\n",
        "| CSQA2  | TEAM DeBEARTa  | 0.669 | 0.671 |\n",
        "| CSQA2  | SCORE RoBERTa  | 0.5462 | |\n",
        "| CSQA2  | SCORE DeBERTa  | 0.6336 | |\n",
        "| QASC  | TEAM RoBERTa  | 0.4752 | 0.8371 |\n",
        "| QASC  | TEAM DeBEARTa  | 0.5594 | 0.8506 |\n",
        "| QASC  | SCORE RoBERTa  | 0.4914 | |\n",
        "| QASC  | SCORE DeBERTa  | 0.7333 | |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_qaHvcB05kK"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}