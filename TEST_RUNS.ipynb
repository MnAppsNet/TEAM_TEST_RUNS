{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg4p5NliPGAJ",
        "outputId": "62738421-0ad1-45a2-a048-54043c623c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'TEAM'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 121 (delta 9), reused 15 (delta 4), pack-reused 98\u001b[K\n",
            "Receiving objects: 100% (121/121), 100.31 MiB | 16.95 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "Checking out files: 100% (87/87), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/declare-lab/TEAM.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNXJCXYfPJvT",
        "outputId": "c269e438-75c2-4a3a-fd7f-420afdcce7b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/TEAM\n"
          ]
        }
      ],
      "source": [
        "%cd TEAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fab50uCFPYGi",
        "outputId": "6f7a72e9-b3ad-466b-a3d6-3ebf6bfcb232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==2.5.2\n",
            "  Downloading datasets-2.5.2-py3-none-any.whl (432 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.7/432.7 KB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.3.5)\n",
            "Collecting scikit_learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (4.64.1)\n",
            "Collecting transformers==4.21.2\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb==0.13.3\n",
            "  Downloading wandb-0.13.3-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (9.0.0)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (3.8.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (21.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.5.2->-r requirements.txt (line 1)) (2.25.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.5->-r requirements.txt (line 3)) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.5->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.3->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.3->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.3->-r requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.11.0->-r requirements.txt (line 5)) (4.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21.2->-r requirements.txt (line 7)) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.21.2->-r requirements.txt (line 7)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.21.2->-r requirements.txt (line 7)) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (3.19.6)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (1.15.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.3->-r requirements.txt (line 8)) (57.4.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.13.0-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.5.2->-r requirements.txt (line 1)) (22.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets==2.5.2->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->-r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==2.5.2->-r requirements.txt (line 1)) (2.10)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=bc7c7cef825aa167746dd72c42ac15efac7c5c03efebed26b6a5bdae92895a81\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, pathtools, xxhash, urllib3, torch, smmap, shortuuid, setproctitle, numpy, docker-pycreds, dill, sentry-sdk, multiprocess, gitdb, scikit_learn, responses, huggingface-hub, GitPython, wandb, transformers, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "xarray-einstats 0.4.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.11.0 which is incompatible.\n",
            "tifffile 2022.10.10 requires numpy>=1.19.2, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.25 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.30 datasets-2.5.2 dill-0.3.5.1 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.11.1 multiprocess-0.70.13 numpy-1.18.5 pathtools-0.1.2 responses-0.18.0 scikit_learn-1.1.3 sentry-sdk-1.13.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 tokenizers-0.12.1 torch-1.11.0 transformers-4.21.2 urllib3-1.26.14 wandb-0.13.3 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nShsyU2Gh8TQ",
        "outputId": "3f53b5c6-e5ac-409e-975a-c52314a144b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.21.0\n",
            "  Using cached numpy-1.21.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.0\n",
            "    Uninstalling numpy-1.21.0:\n",
            "      Successfully uninstalled numpy-1.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.21.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m890.2/890.2 MB\u001b[0m \u001b[31m154.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1112711168 bytes == 0x380f4000 @  0x7efd85563615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.2/890.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wheel\n",
            "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-65.7.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wheel, typing-extensions, setuptools, nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.38.4\n",
            "    Uninstalling wheel-0.38.4:\n",
            "      Successfully uninstalled wheel-0.38.4\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 65.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 setuptools-65.7.0 torch-1.13.0 typing-extensions-4.4.0 wheel-0.38.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pio\n",
            "  Downloading pio-0.0.3-py3-none-any.whl (3.4 kB)\n",
            "Collecting prin\n",
            "  Downloading prin-1.1.0.zip (710 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: prin\n",
            "  Building wheel for prin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prin: filename=prin-1.1.0-py3-none-any.whl size=1154 sha256=deca2bdab62563b91eefd01840a3a9f094515a01f78ec625bb3bfe2725053367\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/34/08/b94e5795d376b15ac366057a948257efa0a078b57c640fd2bc\n",
            "Successfully built prin\n",
            "Installing collected packages: sentencepiece, prin, pio\n",
            "Successfully installed pio-0.0.3 prin-1.1.0 sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install --force-reinstall numpy==1.21.0\n",
        "!pip install --force-reinstall torch==1.13.0\n",
        "!pip install sentencepiece pio prin"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CSQA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TEAM - RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbS4s3e2PZJI",
        "outputId": "0e75726b-ff1d-411d-c03e-80228f4708e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=15, epochs=5, eval_bs=15, input_format='1', lr=1e-06, name='roberta-large', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Downloading config.json: 100% 482/482 [00:00<00:00, 444kB/s]\n",
            "Downloading vocab.json: 100% 878k/878k [00:01<00:00, 670kB/s]\n",
            "Downloading merges.txt: 100% 446k/446k [00:01<00:00, 393kB/s]\n",
            "Downloading tokenizer.json: 100% 1.29M/1.29M [00:01<00:00, 1.19MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.33G/1.33G [00:16<00:00, 86.5MB/s]\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230113_095401-t06ov0mv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mkind-sea-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-roberta-large\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-roberta-large/runs/t06ov0mv\u001b[0m\n",
            "Test preds frequency: {'E': 239, 'A': 232, 'C': 232, 'B': 224, 'D': 213}\n",
            "Epoch 1: Loss: Train 0.4836; Val 0.3934\n",
            "Classification Acc: Train 0.7994; Val 0.8028\n",
            "Classification Macro F1: Train 0.4581; Val 0.465\n",
            "Instance Acc: Val 0.6216\n",
            "Test preds frequency: {'A': 241, 'E': 235, 'B': 225, 'D': 222, 'C': 217}\n",
            "Epoch 2: Loss: Train 0.4094; Val 0.3627\n",
            "Classification Acc: Train 0.8108; Val 0.8318\n",
            "Classification Macro F1: Train 0.603; Val 0.7514\n",
            "Instance Acc: Val 0.6773\n",
            "Test preds frequency: {'D': 232, 'C': 231, 'E': 228, 'A': 228, 'B': 221}\n",
            "Epoch 3: Loss: Train 0.372; Val 0.3356\n",
            "Classification Acc: Train 0.8286; Val 0.847\n",
            "Classification Macro F1: Train 0.687; Val 0.7545\n",
            "Instance Acc: Val 0.7191\n",
            "Test preds frequency: {'C': 235, 'B': 229, 'A': 229, 'E': 226, 'D': 221}\n",
            "Epoch 4: Loss: Train 0.3459; Val 0.3434\n",
            "Classification Acc: Train 0.84; Val 0.8424\n",
            "Classification Macro F1: Train 0.7196; Val 0.7707\n",
            "Instance Acc: Val 0.7314\n",
            "Test preds frequency: {'B': 232, 'A': 232, 'C': 228, 'D': 225, 'E': 223}\n",
            "Epoch 5: Loss: Train 0.3193; Val 0.3485\n",
            "Classification Acc: Train 0.8553; Val 0.8314\n",
            "Classification Macro F1: Train 0.7549; Val 0.7657\n",
            "Instance Acc: Val 0.7428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▂▅▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▅▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁▆█▇▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁▄▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▄▁▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.8553\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.3193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.7428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.3485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mkind-sea-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-roberta-large/runs/t06ov0mv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230113_095401-t06ov0mv/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_csqa.py --epochs 5 --lr 1e-6 --shuffle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### SCORE - RoBEERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g3dhLF-8vUx",
        "outputId": "e087426f-16f0-4c2e-9ccb-b9c7d74a0724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved/csqa/mcq/roberta-large/runs/Jan15_15-13-39_50285f0c5623,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=saved/csqa/mcq/roberta-large,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=10,\n",
            "per_device_train_batch_size=5,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=MCQA CSQA ROBERTA,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.005,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-097216e0329ca922\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "100% 3/3 [00:00<00:00, 749.56it/s]\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:13:41,354 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:13:41,356 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:404] 2023-01-15 15:13:42,271 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:13:43,184 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:13:43,185 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,586 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:13:49,587 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:13:50,503 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:13:50,504 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2041] 2023-01-15 15:13:51,486 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[WARNING|modeling_utils.py:2425] 2023-01-15 15:13:55,418 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2437] 2023-01-15 15:13:55,418 >> Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-416c95ae5e71be9c.arrow\n",
            "  0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a1255d4c19c65799.arrow\n",
            "100% 2/2 [00:00<00:00,  2.95ba/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-097216e0329ca922/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2cb265b6aa621904.arrow\n",
            "Epoch count 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:722] 2023-01-15 15:14:01,014 >> The following columns in the training set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-15 15:14:01,029 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-15 15:14:01,029 >>   Num examples = 9741\n",
            "[INFO|trainer.py:1607] 2023-01-15 15:14:01,029 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2023-01-15 15:14:01,029 >>   Instantaneous batch size per device = 5\n",
            "[INFO|trainer.py:1609] 2023-01-15 15:14:01,029 >>   Total train batch size (w. parallel, distributed & accumulation) = 5\n",
            "[INFO|trainer.py:1610] 2023-01-15 15:14:01,029 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-15 15:14:01,029 >>   Total optimization steps = 9745\n",
            "[INFO|integrations.py:607] 2023-01-15 15:14:01,031 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalyvasman\u001b[0m (\u001b[33mnlpteam_gr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/TEAM/wandb/run-20230115_151403-14r1s8x3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMCQA CSQA ROBERTA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/14r1s8x3\u001b[0m\n",
            "{'loss': 1.6186, 'learning_rate': 9.486916367367881e-07, 'epoch': 0.26}\n",
            "{'loss': 1.6222, 'learning_rate': 8.973832734735761e-07, 'epoch': 0.51}\n",
            "{'loss': 1.6186, 'learning_rate': 8.460749102103642e-07, 'epoch': 0.77}\n",
            " 20% 1949/9745 [19:37<1:06:10,  1.96it/s][INFO|trainer.py:722] 2023-01-15 15:33:41,712 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 15:33:41,716 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 15:33:41,716 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 15:33:41,716 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.39it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.34it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:27,  4.25it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:31,  3.77it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:32,  3.61it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:30,  3.85it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:33,  3.46it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.01it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:39,  2.88it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:38,  2.95it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.73it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:39,  2.79it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.81it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:40,  2.67it/s]\u001b[A\n",
            " 13% 16/123 [00:05<00:38,  2.79it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:35,  2.98it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:31,  3.31it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:32,  3.24it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:35,  2.90it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:33,  3.09it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.40it/s]\u001b[A\n",
            " 19% 23/123 [00:07<00:27,  3.66it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:29,  3.37it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.17it/s]\u001b[A\n",
            " 21% 26/123 [00:08<00:31,  3.09it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:33,  2.90it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:29,  3.27it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:29,  3.21it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.48it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.36it/s]\u001b[A\n",
            " 26% 32/123 [00:10<00:32,  2.82it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:30,  2.99it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:30,  2.95it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.29it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:26,  3.33it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.15it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.30it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:29,  2.89it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.08it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.09it/s]\u001b[A\n",
            " 34% 42/123 [00:13<00:23,  3.40it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.38it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.95it/s]\u001b[A\n",
            " 37% 45/123 [00:14<00:25,  3.00it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:25,  3.05it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.00it/s]\u001b[A\n",
            " 39% 48/123 [00:15<00:25,  2.98it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.03it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.13it/s]\u001b[A\n",
            " 41% 51/123 [00:16<00:22,  3.15it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.16it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.87it/s]\u001b[A\n",
            " 44% 54/123 [00:17<00:23,  2.97it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.07it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.14it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:20,  3.28it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.38it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.29it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:19,  3.30it/s]\u001b[A\n",
            " 50% 61/123 [00:19<00:19,  3.26it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.94it/s]\u001b[A\n",
            " 51% 63/123 [00:20<00:24,  2.49it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:24,  2.45it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.46it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:22,  2.58it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:20,  2.79it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.01it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.74it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.79it/s]\u001b[A\n",
            " 58% 71/123 [00:23<00:19,  2.69it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.81it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.89it/s]\u001b[A\n",
            " 60% 74/123 [00:24<00:17,  2.74it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.75it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.87it/s]\u001b[A\n",
            " 63% 77/123 [00:25<00:14,  3.20it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.05it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.07it/s]\u001b[A\n",
            " 65% 80/123 [00:26<00:13,  3.10it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.25it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.55it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.31it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.98it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.00it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.06it/s]\u001b[A\n",
            " 71% 87/123 [00:28<00:10,  3.40it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.48it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.35it/s]\u001b[A\n",
            " 73% 90/123 [00:29<00:09,  3.30it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.86it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.62it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:12,  2.50it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:10,  2.88it/s]\u001b[A\n",
            " 77% 95/123 [00:31<00:10,  2.73it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.77it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.93it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.23it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.38it/s]\u001b[A\n",
            " 81% 100/123 [00:33<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.72it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.81it/s]\u001b[A\n",
            " 84% 103/123 [00:34<00:06,  2.96it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.75it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.12it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.09it/s]\u001b[A\n",
            " 87% 107/123 [00:35<00:04,  3.20it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.91it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.25it/s]\u001b[A\n",
            " 89% 110/123 [00:36<00:03,  3.37it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.46it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.40it/s]\u001b[A\n",
            " 92% 113/123 [00:37<00:02,  3.48it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.34it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.43it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.24it/s]\u001b[A\n",
            " 95% 117/123 [00:38<00:01,  3.19it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.51it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.07it/s]\u001b[A\n",
            " 98% 120/123 [00:39<00:00,  3.08it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.11it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 1.4447888135910034, 'eval_accuracy': 0.4046, 'eval_runtime': 40.2733, 'eval_samples_per_second': 30.318, 'eval_steps_per_second': 3.054, 'epoch': 1.0}\n",
            " 20% 1949/9745 [20:18<1:06:10,  1.96it/s]\n",
            "100% 123/123 [00:39<00:00,  3.04it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 15:34:22,002 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-1949\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 15:34:22,003 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-1949/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 15:34:27,256 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-1949/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 15:34:27,257 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-1949/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 15:34:27,258 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-1949/special_tokens_map.json\n",
            "{'loss': 1.5761, 'learning_rate': 7.947665469471524e-07, 'epoch': 1.03}\n",
            "{'loss': 1.5258, 'learning_rate': 7.434581836839404e-07, 'epoch': 1.28}\n",
            "{'loss': 1.4954, 'learning_rate': 6.921498204207286e-07, 'epoch': 1.54}\n",
            "{'loss': 1.4479, 'learning_rate': 6.408414571575167e-07, 'epoch': 1.8}\n",
            " 40% 3898/9745 [40:11<49:49,  1.96it/s]  [INFO|trainer.py:722] 2023-01-15 15:54:15,010 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 15:54:15,013 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 15:54:15,013 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 15:54:15,013 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.23it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.27it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:28,  4.22it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:31,  3.77it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:32,  3.61it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:30,  3.86it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:33,  3.47it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.02it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:39,  2.88it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:38,  2.95it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.74it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:39,  2.80it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.82it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:40,  2.68it/s]\u001b[A\n",
            " 13% 16/123 [00:05<00:38,  2.79it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:35,  2.99it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:31,  3.32it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:32,  3.24it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:35,  2.90it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:32,  3.09it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.42it/s]\u001b[A\n",
            " 19% 23/123 [00:06<00:27,  3.68it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:29,  3.41it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.20it/s]\u001b[A\n",
            " 21% 26/123 [00:08<00:31,  3.10it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:32,  2.92it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:28,  3.28it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:29,  3.22it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.50it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.38it/s]\u001b[A\n",
            " 26% 32/123 [00:10<00:32,  2.83it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:30,  2.99it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:29,  2.97it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.32it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:25,  3.36it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.15it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.29it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:28,  2.90it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.08it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.09it/s]\u001b[A\n",
            " 34% 42/123 [00:13<00:23,  3.42it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.41it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.96it/s]\u001b[A\n",
            " 37% 45/123 [00:14<00:26,  3.00it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:25,  3.05it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.01it/s]\u001b[A\n",
            " 39% 48/123 [00:15<00:25,  2.99it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.04it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.14it/s]\u001b[A\n",
            " 41% 51/123 [00:16<00:22,  3.16it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.17it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.87it/s]\u001b[A\n",
            " 44% 54/123 [00:17<00:23,  2.96it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.06it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.14it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:20,  3.28it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.39it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.29it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:19,  3.30it/s]\u001b[A\n",
            " 50% 61/123 [00:19<00:19,  3.26it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.94it/s]\u001b[A\n",
            " 51% 63/123 [00:20<00:24,  2.48it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:24,  2.45it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.45it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:22,  2.58it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:20,  2.79it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.01it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.74it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.79it/s]\u001b[A\n",
            " 58% 71/123 [00:23<00:19,  2.69it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.81it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.90it/s]\u001b[A\n",
            " 60% 74/123 [00:24<00:17,  2.74it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.76it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.87it/s]\u001b[A\n",
            " 63% 77/123 [00:24<00:14,  3.20it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.05it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.07it/s]\u001b[A\n",
            " 65% 80/123 [00:25<00:13,  3.10it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.25it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.54it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.28it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.97it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.00it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.04it/s]\u001b[A\n",
            " 71% 87/123 [00:28<00:10,  3.39it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.48it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.37it/s]\u001b[A\n",
            " 73% 90/123 [00:29<00:09,  3.31it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.86it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.63it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:11,  2.50it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:10,  2.89it/s]\u001b[A\n",
            " 77% 95/123 [00:31<00:10,  2.72it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.78it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.94it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.23it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.39it/s]\u001b[A\n",
            " 81% 100/123 [00:33<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.73it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.82it/s]\u001b[A\n",
            " 84% 103/123 [00:33<00:06,  2.96it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.75it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.12it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.10it/s]\u001b[A\n",
            " 87% 107/123 [00:35<00:04,  3.20it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.91it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.27it/s]\u001b[A\n",
            " 89% 110/123 [00:36<00:03,  3.38it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.46it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.39it/s]\u001b[A\n",
            " 92% 113/123 [00:36<00:02,  3.48it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.34it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.42it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.24it/s]\u001b[A\n",
            " 95% 117/123 [00:38<00:01,  3.20it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.53it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.09it/s]\u001b[A\n",
            " 98% 120/123 [00:39<00:00,  3.09it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.09it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1734435558319092, 'eval_accuracy': 0.5364, 'eval_runtime': 40.2211, 'eval_samples_per_second': 30.357, 'eval_steps_per_second': 3.058, 'epoch': 2.0}\n",
            " 40% 3898/9745 [40:51<49:49,  1.96it/s]\n",
            "100% 123/123 [00:39<00:00,  3.02it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 15:54:55,237 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-3898\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 15:54:55,238 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-3898/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 15:55:00,546 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-3898/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 15:55:00,547 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-3898/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 15:55:00,548 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-3898/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 15:55:12,120 >> Deleting older checkpoint [saved/csqa/mcq/roberta-large/checkpoint-1949] due to args.save_total_limit\n",
            "{'loss': 1.4014, 'learning_rate': 5.895330938943047e-07, 'epoch': 2.05}\n",
            "{'loss': 1.3271, 'learning_rate': 5.382247306310928e-07, 'epoch': 2.31}\n",
            "{'loss': 1.3053, 'learning_rate': 4.869163673678809e-07, 'epoch': 2.57}\n",
            "{'loss': 1.245, 'learning_rate': 4.35608004104669e-07, 'epoch': 2.82}\n",
            " 60% 5847/9745 [1:00:44<30:40,  2.12it/s][INFO|trainer.py:722] 2023-01-15 16:14:48,037 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:14:48,039 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:14:48,039 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:14:48,040 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.47it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.38it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:27,  4.28it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:30,  3.83it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:31,  3.68it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:29,  3.91it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:32,  3.52it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.07it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:38,  2.92it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:37,  2.99it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.77it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:38,  2.83it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.85it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:39,  2.71it/s]\u001b[A\n",
            " 13% 16/123 [00:04<00:37,  2.83it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:34,  3.04it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:30,  3.39it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:31,  3.30it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:34,  2.96it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:32,  3.15it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.46it/s]\u001b[A\n",
            " 19% 23/123 [00:06<00:26,  3.71it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:28,  3.45it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.22it/s]\u001b[A\n",
            " 21% 26/123 [00:07<00:30,  3.13it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:32,  2.96it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:28,  3.32it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:28,  3.25it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.54it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.40it/s]\u001b[A\n",
            " 26% 32/123 [00:09<00:32,  2.83it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:29,  3.01it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:29,  2.99it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.32it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:25,  3.36it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.18it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.32it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:28,  2.92it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.10it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.12it/s]\u001b[A\n",
            " 34% 42/123 [00:12<00:23,  3.44it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.41it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.98it/s]\u001b[A\n",
            " 37% 45/123 [00:13<00:25,  3.03it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:25,  3.07it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.04it/s]\u001b[A\n",
            " 39% 48/123 [00:14<00:24,  3.01it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.07it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.17it/s]\u001b[A\n",
            " 41% 51/123 [00:15<00:22,  3.19it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.20it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.90it/s]\u001b[A\n",
            " 44% 54/123 [00:16<00:22,  3.00it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.09it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.16it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:19,  3.30it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.40it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.31it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:18,  3.33it/s]\u001b[A\n",
            " 50% 61/123 [00:18<00:18,  3.29it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.96it/s]\u001b[A\n",
            " 51% 63/123 [00:19<00:23,  2.50it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:23,  2.47it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.46it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:22,  2.59it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:19,  2.80it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.02it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.76it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.81it/s]\u001b[A\n",
            " 58% 71/123 [00:22<00:19,  2.70it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.82it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.91it/s]\u001b[A\n",
            " 60% 74/123 [00:23<00:17,  2.75it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.76it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.88it/s]\u001b[A\n",
            " 63% 77/123 [00:24<00:14,  3.19it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.06it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.09it/s]\u001b[A\n",
            " 65% 80/123 [00:25<00:13,  3.11it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.25it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.56it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.29it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.99it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.01it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.06it/s]\u001b[A\n",
            " 71% 87/123 [00:27<00:10,  3.41it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.48it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.37it/s]\u001b[A\n",
            " 73% 90/123 [00:28<00:09,  3.31it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.86it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.63it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:11,  2.51it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:09,  2.91it/s]\u001b[A\n",
            " 77% 95/123 [00:30<00:10,  2.74it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.79it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.95it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.24it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.39it/s]\u001b[A\n",
            " 81% 100/123 [00:32<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.73it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.82it/s]\u001b[A\n",
            " 84% 103/123 [00:33<00:06,  2.97it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.76it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.13it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.12it/s]\u001b[A\n",
            " 87% 107/123 [00:34<00:04,  3.23it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.93it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.28it/s]\u001b[A\n",
            " 89% 110/123 [00:35<00:03,  3.38it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.47it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.39it/s]\u001b[A\n",
            " 92% 113/123 [00:36<00:02,  3.48it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.36it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.45it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.25it/s]\u001b[A\n",
            " 95% 117/123 [00:37<00:01,  3.20it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.52it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.07it/s]\u001b[A\n",
            " 98% 120/123 [00:38<00:00,  3.09it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.11it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.9874638319015503, 'eval_accuracy': 0.6208, 'eval_runtime': 39.9617, 'eval_samples_per_second': 30.554, 'eval_steps_per_second': 3.078, 'epoch': 3.0}\n",
            " 60% 5847/9745 [1:01:24<30:40,  2.12it/s]\n",
            "100% 123/123 [00:39<00:00,  3.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:15:28,004 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-5847\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:15:28,006 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-5847/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:15:33,302 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-5847/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:15:33,304 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-5847/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:15:33,304 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-5847/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:15:44,635 >> Deleting older checkpoint [saved/csqa/mcq/roberta-large/checkpoint-3898] due to args.save_total_limit\n",
            "{'loss': 1.2381, 'learning_rate': 3.842996408414572e-07, 'epoch': 3.08}\n",
            "{'loss': 1.1718, 'learning_rate': 3.3299127757824525e-07, 'epoch': 3.34}\n",
            "{'loss': 1.1427, 'learning_rate': 2.816829143150333e-07, 'epoch': 3.59}\n",
            "{'loss': 1.1515, 'learning_rate': 2.3037455105182143e-07, 'epoch': 3.85}\n",
            " 80% 7796/9745 [1:21:19<15:51,  2.05it/s][INFO|trainer.py:722] 2023-01-15 16:35:23,204 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:35:23,207 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:35:23,208 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:35:23,208 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.26it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.41it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:27,  4.32it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:30,  3.82it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:31,  3.67it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:29,  3.91it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:32,  3.51it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.05it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:38,  2.92it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:37,  2.98it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.76it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:38,  2.84it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.84it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:40,  2.70it/s]\u001b[A\n",
            " 13% 16/123 [00:04<00:37,  2.82it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:34,  3.04it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:31,  3.38it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:31,  3.30it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:34,  2.95it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:32,  3.13it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.46it/s]\u001b[A\n",
            " 19% 23/123 [00:06<00:26,  3.72it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:28,  3.44it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.21it/s]\u001b[A\n",
            " 21% 26/123 [00:07<00:30,  3.14it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:32,  2.95it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:28,  3.30it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:28,  3.25it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.52it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.39it/s]\u001b[A\n",
            " 26% 32/123 [00:09<00:32,  2.83it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:29,  3.00it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:29,  2.98it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.32it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:25,  3.37it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.17it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.31it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:28,  2.92it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.10it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.12it/s]\u001b[A\n",
            " 34% 42/123 [00:12<00:23,  3.44it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.41it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.99it/s]\u001b[A\n",
            " 37% 45/123 [00:13<00:25,  3.03it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:24,  3.08it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.04it/s]\u001b[A\n",
            " 39% 48/123 [00:14<00:24,  3.01it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.08it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.16it/s]\u001b[A\n",
            " 41% 51/123 [00:15<00:22,  3.18it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.19it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.89it/s]\u001b[A\n",
            " 44% 54/123 [00:16<00:23,  2.98it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.08it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.15it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:20,  3.30it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.39it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.31it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:18,  3.34it/s]\u001b[A\n",
            " 50% 61/123 [00:18<00:18,  3.28it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.97it/s]\u001b[A\n",
            " 51% 63/123 [00:19<00:23,  2.51it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:23,  2.47it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.47it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:21,  2.60it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:19,  2.81it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.03it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.76it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.81it/s]\u001b[A\n",
            " 58% 71/123 [00:22<00:19,  2.70it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.81it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.89it/s]\u001b[A\n",
            " 60% 74/123 [00:23<00:17,  2.75it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.76it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.87it/s]\u001b[A\n",
            " 63% 77/123 [00:24<00:14,  3.20it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.05it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.08it/s]\u001b[A\n",
            " 65% 80/123 [00:25<00:13,  3.11it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.25it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.56it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.30it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.98it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.01it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.06it/s]\u001b[A\n",
            " 71% 87/123 [00:27<00:10,  3.40it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.48it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.35it/s]\u001b[A\n",
            " 73% 90/123 [00:28<00:10,  3.30it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.87it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.62it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:11,  2.51it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:09,  2.90it/s]\u001b[A\n",
            " 77% 95/123 [00:30<00:10,  2.73it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.78it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.94it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.23it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.38it/s]\u001b[A\n",
            " 81% 100/123 [00:32<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.73it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.82it/s]\u001b[A\n",
            " 84% 103/123 [00:33<00:06,  2.98it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.76it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.13it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.10it/s]\u001b[A\n",
            " 87% 107/123 [00:35<00:04,  3.22it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.91it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.25it/s]\u001b[A\n",
            " 89% 110/123 [00:35<00:03,  3.36it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.45it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.37it/s]\u001b[A\n",
            " 92% 113/123 [00:36<00:02,  3.46it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.34it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.41it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.25it/s]\u001b[A\n",
            " 95% 117/123 [00:38<00:01,  3.21it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.53it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.08it/s]\u001b[A\n",
            " 98% 120/123 [00:39<00:00,  3.08it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.09it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.939306378364563, 'eval_accuracy': 0.6495, 'eval_runtime': 40.0248, 'eval_samples_per_second': 30.506, 'eval_steps_per_second': 3.073, 'epoch': 4.0}\n",
            " 80% 7796/9745 [1:21:59<15:51,  2.05it/s]\n",
            "100% 123/123 [00:39<00:00,  3.01it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:36:03,236 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-7796\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:36:03,237 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-7796/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:36:08,582 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-7796/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:36:08,583 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-7796/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:36:08,584 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-7796/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:36:19,898 >> Deleting older checkpoint [saved/csqa/mcq/roberta-large/checkpoint-5847] due to args.save_total_limit\n",
            "{'loss': 1.1419, 'learning_rate': 1.7906618778860955e-07, 'epoch': 4.1}\n",
            "{'loss': 1.119, 'learning_rate': 1.2775782452539762e-07, 'epoch': 4.36}\n",
            "{'loss': 1.1087, 'learning_rate': 7.644946126218574e-08, 'epoch': 4.62}\n",
            "{'loss': 1.0924, 'learning_rate': 2.514109799897383e-08, 'epoch': 4.87}\n",
            "100% 9745/9745 [1:41:52<00:00,  2.06it/s][INFO|trainer.py:722] 2023-01-15 16:55:56,240 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:55:56,243 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:55:56,243 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:55:56,243 >>   Batch size = 10\n",
            "\n",
            "  0% 0/123 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/123 [00:00<00:14,  8.39it/s]\u001b[A\n",
            "  2% 3/123 [00:00<00:22,  5.38it/s]\u001b[A\n",
            "  3% 4/123 [00:00<00:27,  4.28it/s]\u001b[A\n",
            "  4% 5/123 [00:01<00:31,  3.80it/s]\u001b[A\n",
            "  5% 6/123 [00:01<00:32,  3.65it/s]\u001b[A\n",
            "  6% 7/123 [00:01<00:29,  3.90it/s]\u001b[A\n",
            "  7% 8/123 [00:02<00:32,  3.49it/s]\u001b[A\n",
            "  7% 9/123 [00:02<00:37,  3.03it/s]\u001b[A\n",
            "  8% 10/123 [00:02<00:38,  2.90it/s]\u001b[A\n",
            "  9% 11/123 [00:03<00:37,  2.97it/s]\u001b[A\n",
            " 10% 12/123 [00:03<00:40,  2.75it/s]\u001b[A\n",
            " 11% 13/123 [00:03<00:39,  2.81it/s]\u001b[A\n",
            " 11% 14/123 [00:04<00:38,  2.84it/s]\u001b[A\n",
            " 12% 15/123 [00:04<00:40,  2.70it/s]\u001b[A\n",
            " 13% 16/123 [00:04<00:38,  2.81it/s]\u001b[A\n",
            " 14% 17/123 [00:05<00:35,  3.01it/s]\u001b[A\n",
            " 15% 18/123 [00:05<00:31,  3.35it/s]\u001b[A\n",
            " 15% 19/123 [00:05<00:31,  3.28it/s]\u001b[A\n",
            " 16% 20/123 [00:06<00:35,  2.94it/s]\u001b[A\n",
            " 17% 21/123 [00:06<00:32,  3.12it/s]\u001b[A\n",
            " 18% 22/123 [00:06<00:29,  3.45it/s]\u001b[A\n",
            " 19% 23/123 [00:06<00:27,  3.70it/s]\u001b[A\n",
            " 20% 24/123 [00:07<00:28,  3.43it/s]\u001b[A\n",
            " 20% 25/123 [00:07<00:30,  3.21it/s]\u001b[A\n",
            " 21% 26/123 [00:07<00:31,  3.11it/s]\u001b[A\n",
            " 22% 27/123 [00:08<00:32,  2.93it/s]\u001b[A\n",
            " 23% 28/123 [00:08<00:28,  3.29it/s]\u001b[A\n",
            " 24% 29/123 [00:08<00:29,  3.22it/s]\u001b[A\n",
            " 24% 30/123 [00:09<00:26,  3.50it/s]\u001b[A\n",
            " 25% 31/123 [00:09<00:27,  3.37it/s]\u001b[A\n",
            " 26% 32/123 [00:09<00:32,  2.83it/s]\u001b[A\n",
            " 27% 33/123 [00:10<00:29,  3.00it/s]\u001b[A\n",
            " 28% 34/123 [00:10<00:29,  2.98it/s]\u001b[A\n",
            " 28% 35/123 [00:10<00:26,  3.32it/s]\u001b[A\n",
            " 29% 36/123 [00:11<00:25,  3.36it/s]\u001b[A\n",
            " 30% 37/123 [00:11<00:27,  3.17it/s]\u001b[A\n",
            " 31% 38/123 [00:11<00:25,  3.31it/s]\u001b[A\n",
            " 32% 39/123 [00:12<00:28,  2.91it/s]\u001b[A\n",
            " 33% 40/123 [00:12<00:26,  3.09it/s]\u001b[A\n",
            " 33% 41/123 [00:12<00:26,  3.11it/s]\u001b[A\n",
            " 34% 42/123 [00:12<00:23,  3.44it/s]\u001b[A\n",
            " 35% 43/123 [00:13<00:23,  3.41it/s]\u001b[A\n",
            " 36% 44/123 [00:13<00:26,  2.96it/s]\u001b[A\n",
            " 37% 45/123 [00:14<00:25,  3.01it/s]\u001b[A\n",
            " 37% 46/123 [00:14<00:25,  3.07it/s]\u001b[A\n",
            " 38% 47/123 [00:14<00:25,  3.02it/s]\u001b[A\n",
            " 39% 48/123 [00:15<00:25,  2.99it/s]\u001b[A\n",
            " 40% 49/123 [00:15<00:24,  3.06it/s]\u001b[A\n",
            " 41% 50/123 [00:15<00:23,  3.14it/s]\u001b[A\n",
            " 41% 51/123 [00:15<00:22,  3.17it/s]\u001b[A\n",
            " 42% 52/123 [00:16<00:22,  3.18it/s]\u001b[A\n",
            " 43% 53/123 [00:16<00:24,  2.89it/s]\u001b[A\n",
            " 44% 54/123 [00:16<00:23,  2.99it/s]\u001b[A\n",
            " 45% 55/123 [00:17<00:22,  3.08it/s]\u001b[A\n",
            " 46% 56/123 [00:17<00:21,  3.15it/s]\u001b[A\n",
            " 46% 57/123 [00:17<00:19,  3.30it/s]\u001b[A\n",
            " 47% 58/123 [00:18<00:19,  3.41it/s]\u001b[A\n",
            " 48% 59/123 [00:18<00:19,  3.31it/s]\u001b[A\n",
            " 49% 60/123 [00:18<00:18,  3.33it/s]\u001b[A\n",
            " 50% 61/123 [00:19<00:18,  3.29it/s]\u001b[A\n",
            " 50% 62/123 [00:19<00:20,  2.95it/s]\u001b[A\n",
            " 51% 63/123 [00:20<00:24,  2.50it/s]\u001b[A\n",
            " 52% 64/123 [00:20<00:23,  2.47it/s]\u001b[A\n",
            " 53% 65/123 [00:20<00:23,  2.46it/s]\u001b[A\n",
            " 54% 66/123 [00:21<00:22,  2.59it/s]\u001b[A\n",
            " 54% 67/123 [00:21<00:19,  2.81it/s]\u001b[A\n",
            " 55% 68/123 [00:21<00:18,  3.02it/s]\u001b[A\n",
            " 56% 69/123 [00:22<00:19,  2.76it/s]\u001b[A\n",
            " 57% 70/123 [00:22<00:18,  2.81it/s]\u001b[A\n",
            " 58% 71/123 [00:22<00:19,  2.69it/s]\u001b[A\n",
            " 59% 72/123 [00:23<00:18,  2.80it/s]\u001b[A\n",
            " 59% 73/123 [00:23<00:17,  2.89it/s]\u001b[A\n",
            " 60% 74/123 [00:23<00:17,  2.74it/s]\u001b[A\n",
            " 61% 75/123 [00:24<00:17,  2.75it/s]\u001b[A\n",
            " 62% 76/123 [00:24<00:16,  2.86it/s]\u001b[A\n",
            " 63% 77/123 [00:24<00:14,  3.20it/s]\u001b[A\n",
            " 63% 78/123 [00:25<00:14,  3.06it/s]\u001b[A\n",
            " 64% 79/123 [00:25<00:14,  3.09it/s]\u001b[A\n",
            " 65% 80/123 [00:25<00:13,  3.11it/s]\u001b[A\n",
            " 66% 81/123 [00:26<00:12,  3.27it/s]\u001b[A\n",
            " 67% 82/123 [00:26<00:11,  3.56it/s]\u001b[A\n",
            " 67% 83/123 [00:26<00:12,  3.29it/s]\u001b[A\n",
            " 68% 84/123 [00:27<00:13,  2.99it/s]\u001b[A\n",
            " 69% 85/123 [00:27<00:12,  3.02it/s]\u001b[A\n",
            " 70% 86/123 [00:27<00:12,  3.06it/s]\u001b[A\n",
            " 71% 87/123 [00:27<00:10,  3.41it/s]\u001b[A\n",
            " 72% 88/123 [00:28<00:10,  3.49it/s]\u001b[A\n",
            " 72% 89/123 [00:28<00:10,  3.37it/s]\u001b[A\n",
            " 73% 90/123 [00:28<00:09,  3.30it/s]\u001b[A\n",
            " 74% 91/123 [00:29<00:11,  2.86it/s]\u001b[A\n",
            " 75% 92/123 [00:29<00:11,  2.62it/s]\u001b[A\n",
            " 76% 93/123 [00:30<00:11,  2.50it/s]\u001b[A\n",
            " 76% 94/123 [00:30<00:10,  2.89it/s]\u001b[A\n",
            " 77% 95/123 [00:30<00:10,  2.72it/s]\u001b[A\n",
            " 78% 96/123 [00:31<00:09,  2.78it/s]\u001b[A\n",
            " 79% 97/123 [00:31<00:08,  2.95it/s]\u001b[A\n",
            " 80% 98/123 [00:32<00:11,  2.24it/s]\u001b[A\n",
            " 80% 99/123 [00:32<00:10,  2.39it/s]\u001b[A\n",
            " 81% 100/123 [00:32<00:08,  2.58it/s]\u001b[A\n",
            " 82% 101/123 [00:33<00:08,  2.73it/s]\u001b[A\n",
            " 83% 102/123 [00:33<00:07,  2.82it/s]\u001b[A\n",
            " 84% 103/123 [00:33<00:06,  2.96it/s]\u001b[A\n",
            " 85% 104/123 [00:34<00:06,  2.76it/s]\u001b[A\n",
            " 85% 105/123 [00:34<00:05,  3.13it/s]\u001b[A\n",
            " 86% 106/123 [00:34<00:05,  3.11it/s]\u001b[A\n",
            " 87% 107/123 [00:35<00:04,  3.24it/s]\u001b[A\n",
            " 88% 108/123 [00:35<00:05,  2.93it/s]\u001b[A\n",
            " 89% 109/123 [00:35<00:04,  3.27it/s]\u001b[A\n",
            " 89% 110/123 [00:35<00:03,  3.37it/s]\u001b[A\n",
            " 90% 111/123 [00:36<00:03,  3.46it/s]\u001b[A\n",
            " 91% 112/123 [00:36<00:03,  3.39it/s]\u001b[A\n",
            " 92% 113/123 [00:36<00:02,  3.47it/s]\u001b[A\n",
            " 93% 114/123 [00:37<00:02,  3.35it/s]\u001b[A\n",
            " 93% 115/123 [00:37<00:02,  3.45it/s]\u001b[A\n",
            " 94% 116/123 [00:37<00:02,  3.26it/s]\u001b[A\n",
            " 95% 117/123 [00:38<00:01,  3.19it/s]\u001b[A\n",
            " 96% 118/123 [00:38<00:01,  3.52it/s]\u001b[A\n",
            " 97% 119/123 [00:38<00:01,  3.07it/s]\u001b[A\n",
            " 98% 120/123 [00:39<00:00,  3.08it/s]\u001b[A\n",
            " 98% 121/123 [00:39<00:00,  3.10it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.9197953939437866, 'eval_accuracy': 0.6536, 'eval_runtime': 40.069, 'eval_samples_per_second': 30.472, 'eval_steps_per_second': 3.07, 'epoch': 5.0}\n",
            "100% 9745/9745 [1:42:32<00:00,  2.06it/s]\n",
            "100% 123/123 [00:39<00:00,  3.04it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:56:36,318 >> Saving model checkpoint to saved/csqa/mcq/roberta-large/checkpoint-9745\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:56:36,319 >> Configuration saved in saved/csqa/mcq/roberta-large/checkpoint-9745/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:56:41,624 >> Model weights saved in saved/csqa/mcq/roberta-large/checkpoint-9745/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:56:41,625 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/checkpoint-9745/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:56:41,626 >> Special tokens file saved in saved/csqa/mcq/roberta-large/checkpoint-9745/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:56:52,992 >> Deleting older checkpoint [saved/csqa/mcq/roberta-large/checkpoint-7796] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2023-01-15 16:56:53,063 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 6172.0336, 'train_samples_per_second': 7.891, 'train_steps_per_second': 1.579, 'train_loss': 1.3281526646777995, 'epoch': 5.0}\n",
            "100% 9745/9745 [1:42:49<00:00,  1.58it/s]\n",
            "[INFO|trainer.py:2640] 2023-01-15 16:56:53,066 >> Saving model checkpoint to saved/csqa/mcq/roberta-large\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:56:53,067 >> Configuration saved in saved/csqa/mcq/roberta-large/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:56:58,584 >> Model weights saved in saved/csqa/mcq/roberta-large/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:56:58,585 >> tokenizer config file saved in saved/csqa/mcq/roberta-large/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:56:58,586 >> Special tokens file saved in saved/csqa/mcq/roberta-large/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     1.3282\n",
            "  train_runtime            = 1:42:52.03\n",
            "  train_samples            =       9741\n",
            "  train_samples_per_second =      7.891\n",
            "  train_steps_per_second   =      1.579\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-15 16:56:58,721 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:56:58,724 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:56:58,725 >>   Num examples = 1221\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:56:58,726 >>   Batch size = 10\n",
            "100% 123/123 [00:40<00:00,  3.01it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.6536\n",
            "  eval_loss               =     0.9198\n",
            "  eval_runtime            = 0:00:41.28\n",
            "  eval_samples            =       1221\n",
            "  eval_samples_per_second =     29.577\n",
            "  eval_steps_per_second   =       2.98\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:722] 2023-01-15 16:57:40,010 >> The following columns in the test set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice0, choice3, choice2, choice1, context, choice4. If choice0, choice3, choice2, choice1, context, choice4 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:57:40,012 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:57:40,012 >>   Num examples = 1140\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:57:40,013 >>   Batch size = 10\n",
            "100% 114/114 [00:38<00:00,  2.93it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▅▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▄▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▃▂▁▁▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▆▇██▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▆▇██▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ██▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ███▇▇▆▆▅▄▄▃▃▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.6536\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.9198\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 41.2818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 29.577\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 2.98\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 9745\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 1.0924\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.375459608354597e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.32815\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 6172.0336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 7.891\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMCQA CSQA ROBERTA\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/14r1s8x3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230115_151403-14r1s8x3/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run_mcqa_score.py --learning_rate=1e-6 --num_train_epochs 5 --seed 42 \\\n",
        "--train_file=\"data/csqa/mcq_train.json\" --validation_file=\"data/csqa/mcq_valid.json\" --test_file=\"data/csqa/mcq_test.json\" \\\n",
        "--output_dir=\"saved/csqa/mcq/roberta-large\" --model_name_or_path=\"roberta-large\" \\\n",
        "--per_device_train_batch_size=5 --per_device_eval_batch_size=10 --weight_decay=0.005 \\\n",
        "--do_train True --do_eval True --do_predict True --evaluation_strategy=\"epoch\" --save_strategy=\"epoch\" \\\n",
        "--report_to \"wandb\" --run_name \"MCQA CSQA ROBERTA\" --save_total_limit=1 --overwrite_output_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TEAM - DeBEERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIvTULClilxd",
        "outputId": "810865e1-28d8-497c-acca-089f5c23d2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=15, epochs=5, eval_bs=15, input_format='1', lr=1e-06, name='microsoft/deberta-v3-base', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading pytorch_model.bin: 100% 354M/354M [00:12<00:00, 30.9MB/s]\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalyvasman\u001b[0m (\u001b[33mnlpteam_gr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230113_123521-f4hmo5sk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mproud-moon-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-deberta-v3-base\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-deberta-v3-base/runs/f4hmo5sk\u001b[0m\n",
            "Test preds frequency: {'A': 241, 'C': 237, 'D': 233, 'B': 218, 'E': 211}\n",
            "Epoch 1: Loss: Train 0.4582; Val 0.3906\n",
            "Classification Acc: Train 0.7974; Val 0.8\n",
            "Classification Macro F1: Train 0.4505; Val 0.4444\n",
            "Instance Acc: Val 0.6773\n",
            "Test preds frequency: {'A': 247, 'C': 234, 'D': 224, 'B': 222, 'E': 213}\n",
            "Epoch 2: Loss: Train 0.3948; Val 0.3539\n",
            "Classification Acc: Train 0.8143; Val 0.8473\n",
            "Classification Macro F1: Train 0.5727; Val 0.7409\n",
            "Instance Acc: Val 0.7232\n",
            "Test preds frequency: {'A': 250, 'B': 229, 'D': 226, 'C': 222, 'E': 213}\n",
            "Epoch 3: Loss: Train 0.3646; Val 0.3393\n",
            "Classification Acc: Train 0.8325; Val 0.8393\n",
            "Classification Macro F1: Train 0.7011; Val 0.7631\n",
            "Instance Acc: Val 0.7437\n",
            "Test preds frequency: {'A': 243, 'C': 231, 'B': 227, 'D': 226, 'E': 213}\n",
            "Epoch 4: Loss: Train 0.341; Val 0.3312\n",
            "Classification Acc: Train 0.8456; Val 0.8423\n",
            "Classification Macro F1: Train 0.7416; Val 0.7715\n",
            "Instance Acc: Val 0.7576\n",
            "Test preds frequency: {'A': 240, 'C': 229, 'D': 229, 'B': 226, 'E': 216}\n",
            "Epoch 5: Loss: Train 0.324; Val 0.3181\n",
            "Classification Acc: Train 0.856; Val 0.8537\n",
            "Classification Macro F1: Train 0.7614; Val 0.7781\n",
            "Instance Acc: Val 0.7666\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▃▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▅▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁▇▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▄▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.324\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8537\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.7666\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.3181\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mproud-moon-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA-deberta-v3-base/runs/f4hmo5sk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230113_123521-f4hmo5sk/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_csqa.py --name \"microsoft/deberta-v3-base\" --epochs 5 --lr 1e-6 --shuffle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CSQA2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TEAM - RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB2ZsCT9tv1N",
        "outputId": "b99bb975-0573-4371-de0a-a902d231dd5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=4, epochs=5, eval_bs=4, input_format='0', lr=3e-06, name='roberta-large', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230113_104749-1sbovkar\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpious-lake-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA2-roberta-large\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA2-roberta-large/runs/1sbovkar\u001b[0m\n",
            "Test preds frequency: {'yes': 1426, 'no': 1047}\n",
            "Epoch 1: Loss: Train 0.7014; Val 0.6936\n",
            "Classification Acc: Train 0.4975; Val 0.5\n",
            "Instance Acc: Val 0.5187\n",
            "Test preds frequency: {'no': 1589, 'yes': 884}\n",
            "Epoch 2: Loss: Train 0.6971; Val 0.6935\n",
            "Classification Acc: Train 0.4978; Val 0.5\n",
            "Instance Acc: Val 0.486\n",
            "Test preds frequency: {'no': 2258, 'yes': 215}\n",
            "Epoch 3: Loss: Train 0.6967; Val 0.6932\n",
            "Classification Acc: Train 0.5005; Val 0.4996\n",
            "Instance Acc: Val 0.5238\n",
            "Test preds frequency: {'yes': 1510, 'no': 963}\n",
            "Epoch 4: Loss: Train 0.6974; Val 0.6936\n",
            "Classification Acc: Train 0.4919; Val 0.5\n",
            "Instance Acc: Val 0.4746\n",
            "Test preds frequency: {'no': 1793, 'yes': 680}\n",
            "Epoch 5: Loss: Train 0.6957; Val 0.6935\n",
            "Classification Acc: Train 0.5012; Val 0.5\n",
            "Instance Acc: Val 0.5222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▅▅▇▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▃▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ██▁██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▇▃█▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▆▁█▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.5012\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.6957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.5222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.6935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mpious-lake-2\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA2-roberta-large/runs/1sbovkar\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230113_104749-1sbovkar/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_csqa2.py --name \"roberta-large\" --epochs 5 --lr 3e-6 --bs 4 --eval-bs 4 --shuffle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### SCORE - RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-lwmXvzPZYA",
        "outputId": "5d7a7d90-7ba6-4258-a5a1-c198b5fb4550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved/csqa2/mcq/roberta-large/runs/Jan15_16-32-29_322a8f8a85e4,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=saved/csqa2/mcq/roberta-large,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=10,\n",
            "per_device_train_batch_size=5,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=MCQA CSQA2 ROBERTA,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.005,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-383765ffc621a3df\n",
            "INFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-383765ffc621a3df/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-383765ffc621a3df/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 9784.53it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1639.89it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-383765ffc621a3df/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 924.67it/s]\n",
            "[INFO|hub.py:600] 2023-01-15 16:32:29,670 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpw7b6vnpb\n",
            "Downloading config.json: 100% 482/482 [00:00<00:00, 465kB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 16:32:29,935 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|hub.py:621] 2023-01-15 16:32:29,935 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 16:32:29,935 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 16:32:29,937 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:404] 2023-01-15 16:32:30,202 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 16:32:30,467 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 16:32:30,468 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2023-01-15 16:32:30,998 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpirp74_ja\n",
            "Downloading vocab.json: 100% 878k/878k [00:00<00:00, 2.41MB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 16:32:31,661 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|hub.py:621] 2023-01-15 16:32:31,661 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|hub.py:600] 2023-01-15 16:32:31,926 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpls82hudf\n",
            "Downloading merges.txt: 100% 446k/446k [00:00<00:00, 1.47MB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 16:32:32,510 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:621] 2023-01-15 16:32:32,510 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:600] 2023-01-15 16:32:32,774 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbdo_fcmw\n",
            "Downloading tokenizer.json: 100% 1.29M/1.29M [00:00<00:00, 3.62MB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 16:32:33,453 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|hub.py:621] 2023-01-15 16:32:33,453 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 16:32:34,248 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 16:32:34,248 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 16:32:34,248 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 16:32:34,248 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 16:32:34,248 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 16:32:34,248 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 16:32:34,539 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 16:32:34,540 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2023-01-15 16:32:34,883 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6af5uhb1\n",
            "Downloading pytorch_model.bin: 100% 1.33G/1.33G [00:15<00:00, 90.7MB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 16:32:50,654 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[INFO|hub.py:621] 2023-01-15 16:32:50,654 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[INFO|modeling_utils.py:2041] 2023-01-15 16:32:50,655 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[WARNING|modeling_utils.py:2425] 2023-01-15 16:32:54,596 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2437] 2023-01-15 16:32:54,596 >> Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0% 0/10 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-383765ffc621a3df/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-cad98fb0eb7bc939.arrow\n",
            "100% 10/10 [00:01<00:00,  8.44ba/s]\n",
            "  0% 0/3 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-383765ffc621a3df/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fc959a2837ce0312.arrow\n",
            "100% 3/3 [00:00<00:00,  9.94ba/s]\n",
            "  0% 0/3 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-383765ffc621a3df/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e7536082fe00a4e1.arrow\n",
            "100% 3/3 [00:00<00:00,  7.56ba/s]\n",
            "Epoch count 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:722] 2023-01-15 16:33:01,043 >> The following columns in the training set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice1, context, choice0. If choice1, context, choice0 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-15 16:33:01,053 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-15 16:33:01,053 >>   Num examples = 9264\n",
            "[INFO|trainer.py:1607] 2023-01-15 16:33:01,053 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2023-01-15 16:33:01,053 >>   Instantaneous batch size per device = 5\n",
            "[INFO|trainer.py:1609] 2023-01-15 16:33:01,053 >>   Total train batch size (w. parallel, distributed & accumulation) = 5\n",
            "[INFO|trainer.py:1610] 2023-01-15 16:33:01,053 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-15 16:33:01,053 >>   Total optimization steps = 9265\n",
            "[INFO|integrations.py:607] 2023-01-15 16:33:01,056 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalyvasman\u001b[0m (\u001b[33mnlpteam_gr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230115_163302-3c45qx8d\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMCQA CSQA2 ROBERTA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/3c45qx8d\u001b[0m\n",
            "{'loss': 0.6989, 'learning_rate': 9.460334592552617e-07, 'epoch': 0.27}\n",
            "{'loss': 0.6955, 'learning_rate': 8.920669185105234e-07, 'epoch': 0.54}\n",
            "{'loss': 0.6963, 'learning_rate': 8.381003777657851e-07, 'epoch': 0.81}\n",
            " 20% 1853/9265 [10:36<44:58,  2.75it/s][INFO|trainer.py:722] 2023-01-15 16:43:39,861 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice1, context, choice0. If choice1, context, choice0 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:43:39,864 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:43:39,864 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:43:39,864 >>   Batch size = 10\n",
            "\n",
            "  0% 0/255 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/255 [00:00<00:22, 11.39it/s]\u001b[A\n",
            "  2% 4/255 [00:00<00:33,  7.39it/s]\u001b[A\n",
            "  2% 5/255 [00:00<00:37,  6.63it/s]\u001b[A\n",
            "  2% 6/255 [00:00<00:38,  6.55it/s]\u001b[A\n",
            "  3% 7/255 [00:01<00:38,  6.44it/s]\u001b[A\n",
            "  3% 8/255 [00:01<00:39,  6.26it/s]\u001b[A\n",
            "  4% 9/255 [00:01<00:35,  6.97it/s]\u001b[A\n",
            "  4% 10/255 [00:01<00:33,  7.31it/s]\u001b[A\n",
            "  4% 11/255 [00:01<00:36,  6.72it/s]\u001b[A\n",
            "  5% 12/255 [00:01<00:34,  7.05it/s]\u001b[A\n",
            "  5% 13/255 [00:01<00:36,  6.56it/s]\u001b[A\n",
            "  5% 14/255 [00:02<00:38,  6.25it/s]\u001b[A\n",
            "  6% 15/255 [00:02<00:35,  6.75it/s]\u001b[A\n",
            "  6% 16/255 [00:02<00:33,  7.13it/s]\u001b[A\n",
            "  7% 17/255 [00:02<00:32,  7.39it/s]\u001b[A\n",
            "  7% 18/255 [00:02<00:30,  7.68it/s]\u001b[A\n",
            "  7% 19/255 [00:02<00:30,  7.78it/s]\u001b[A\n",
            "  8% 20/255 [00:02<00:32,  7.34it/s]\u001b[A\n",
            "  8% 21/255 [00:02<00:32,  7.09it/s]\u001b[A\n",
            "  9% 22/255 [00:03<00:33,  6.90it/s]\u001b[A\n",
            "  9% 23/255 [00:03<00:34,  6.77it/s]\u001b[A\n",
            "  9% 24/255 [00:03<00:34,  6.66it/s]\u001b[A\n",
            " 10% 25/255 [00:03<00:32,  7.18it/s]\u001b[A\n",
            " 10% 26/255 [00:03<00:29,  7.77it/s]\u001b[A\n",
            " 11% 27/255 [00:03<00:28,  7.93it/s]\u001b[A\n",
            " 11% 28/255 [00:03<00:27,  8.32it/s]\u001b[A\n",
            " 11% 29/255 [00:04<00:27,  8.32it/s]\u001b[A\n",
            " 12% 30/255 [00:04<00:29,  7.70it/s]\u001b[A\n",
            " 12% 31/255 [00:04<00:33,  6.63it/s]\u001b[A\n",
            " 13% 32/255 [00:04<00:37,  5.96it/s]\u001b[A\n",
            " 13% 33/255 [00:04<00:34,  6.52it/s]\u001b[A\n",
            " 13% 34/255 [00:04<00:34,  6.48it/s]\u001b[A\n",
            " 14% 35/255 [00:04<00:32,  6.78it/s]\u001b[A\n",
            " 14% 36/255 [00:05<00:30,  7.20it/s]\u001b[A\n",
            " 15% 37/255 [00:05<00:32,  6.68it/s]\u001b[A\n",
            " 15% 38/255 [00:05<00:33,  6.54it/s]\u001b[A\n",
            " 15% 39/255 [00:05<00:31,  6.89it/s]\u001b[A\n",
            " 16% 40/255 [00:05<00:31,  6.74it/s]\u001b[A\n",
            " 16% 41/255 [00:05<00:34,  6.24it/s]\u001b[A\n",
            " 16% 42/255 [00:06<00:30,  6.92it/s]\u001b[A\n",
            " 17% 43/255 [00:06<00:29,  7.20it/s]\u001b[A\n",
            " 17% 44/255 [00:06<00:31,  6.65it/s]\u001b[A\n",
            " 18% 45/255 [00:06<00:32,  6.53it/s]\u001b[A\n",
            " 18% 46/255 [00:06<00:33,  6.25it/s]\u001b[A\n",
            " 18% 47/255 [00:06<00:30,  6.80it/s]\u001b[A\n",
            " 19% 48/255 [00:06<00:29,  7.02it/s]\u001b[A\n",
            " 19% 49/255 [00:07<00:30,  6.76it/s]\u001b[A\n",
            " 20% 50/255 [00:07<00:27,  7.36it/s]\u001b[A\n",
            " 20% 51/255 [00:07<00:28,  7.16it/s]\u001b[A\n",
            " 20% 52/255 [00:07<00:29,  6.90it/s]\u001b[A\n",
            " 21% 53/255 [00:07<00:27,  7.46it/s]\u001b[A\n",
            " 21% 54/255 [00:07<00:31,  6.44it/s]\u001b[A\n",
            " 22% 55/255 [00:07<00:31,  6.40it/s]\u001b[A\n",
            " 22% 56/255 [00:08<00:31,  6.27it/s]\u001b[A\n",
            " 22% 57/255 [00:08<00:31,  6.34it/s]\u001b[A\n",
            " 23% 58/255 [00:08<00:32,  6.08it/s]\u001b[A\n",
            " 23% 59/255 [00:08<00:28,  6.79it/s]\u001b[A\n",
            " 24% 60/255 [00:08<00:26,  7.24it/s]\u001b[A\n",
            " 24% 61/255 [00:08<00:28,  6.70it/s]\u001b[A\n",
            " 24% 62/255 [00:09<00:29,  6.54it/s]\u001b[A\n",
            " 25% 63/255 [00:09<00:28,  6.83it/s]\u001b[A\n",
            " 25% 64/255 [00:09<00:27,  7.04it/s]\u001b[A\n",
            " 25% 65/255 [00:09<00:28,  6.75it/s]\u001b[A\n",
            " 26% 66/255 [00:09<00:26,  7.19it/s]\u001b[A\n",
            " 26% 67/255 [00:09<00:24,  7.52it/s]\u001b[A\n",
            " 27% 68/255 [00:09<00:30,  6.11it/s]\u001b[A\n",
            " 27% 69/255 [00:10<00:31,  5.95it/s]\u001b[A\n",
            " 27% 70/255 [00:10<00:28,  6.56it/s]\u001b[A\n",
            " 28% 71/255 [00:10<00:26,  6.84it/s]\u001b[A\n",
            " 28% 72/255 [00:10<00:25,  7.26it/s]\u001b[A\n",
            " 29% 73/255 [00:10<00:26,  6.93it/s]\u001b[A\n",
            " 29% 74/255 [00:10<00:24,  7.30it/s]\u001b[A\n",
            " 29% 75/255 [00:10<00:25,  7.04it/s]\u001b[A\n",
            " 30% 76/255 [00:11<00:24,  7.36it/s]\u001b[A\n",
            " 30% 77/255 [00:11<00:25,  6.89it/s]\u001b[A\n",
            " 31% 78/255 [00:11<00:23,  7.51it/s]\u001b[A\n",
            " 31% 79/255 [00:11<00:22,  7.78it/s]\u001b[A\n",
            " 31% 80/255 [00:11<00:22,  7.94it/s]\u001b[A\n",
            " 32% 81/255 [00:11<00:22,  7.87it/s]\u001b[A\n",
            " 32% 82/255 [00:11<00:20,  8.28it/s]\u001b[A\n",
            " 33% 83/255 [00:11<00:24,  6.97it/s]\u001b[A\n",
            " 33% 84/255 [00:12<00:23,  7.33it/s]\u001b[A\n",
            " 33% 85/255 [00:12<00:24,  7.04it/s]\u001b[A\n",
            " 34% 86/255 [00:12<00:22,  7.36it/s]\u001b[A\n",
            " 34% 87/255 [00:12<00:24,  6.85it/s]\u001b[A\n",
            " 35% 88/255 [00:12<00:23,  7.25it/s]\u001b[A\n",
            " 35% 89/255 [00:12<00:21,  7.56it/s]\u001b[A\n",
            " 35% 90/255 [00:12<00:26,  6.19it/s]\u001b[A\n",
            " 36% 91/255 [00:13<00:24,  6.75it/s]\u001b[A\n",
            " 36% 92/255 [00:13<00:24,  6.58it/s]\u001b[A\n",
            " 36% 93/255 [00:13<00:25,  6.30it/s]\u001b[A\n",
            " 37% 94/255 [00:13<00:24,  6.70it/s]\u001b[A\n",
            " 37% 95/255 [00:13<00:23,  6.95it/s]\u001b[A\n",
            " 38% 96/255 [00:13<00:23,  6.75it/s]\u001b[A\n",
            " 38% 97/255 [00:13<00:22,  7.15it/s]\u001b[A\n",
            " 38% 98/255 [00:14<00:22,  6.89it/s]\u001b[A\n",
            " 39% 99/255 [00:14<00:21,  7.17it/s]\u001b[A\n",
            " 39% 100/255 [00:14<00:20,  7.39it/s]\u001b[A\n",
            " 40% 101/255 [00:14<00:20,  7.65it/s]\u001b[A\n",
            " 40% 102/255 [00:14<00:21,  7.17it/s]\u001b[A\n",
            " 40% 103/255 [00:14<00:20,  7.47it/s]\u001b[A\n",
            " 41% 104/255 [00:14<00:20,  7.52it/s]\u001b[A\n",
            " 41% 105/255 [00:15<00:19,  7.80it/s]\u001b[A\n",
            " 42% 106/255 [00:15<00:19,  7.75it/s]\u001b[A\n",
            " 42% 107/255 [00:15<00:18,  7.86it/s]\u001b[A\n",
            " 42% 108/255 [00:15<00:20,  7.19it/s]\u001b[A\n",
            " 43% 109/255 [00:15<00:19,  7.52it/s]\u001b[A\n",
            " 43% 110/255 [00:15<00:18,  8.00it/s]\u001b[A\n",
            " 44% 111/255 [00:15<00:19,  7.53it/s]\u001b[A\n",
            " 44% 112/255 [00:15<00:19,  7.18it/s]\u001b[A\n",
            " 44% 113/255 [00:16<00:19,  7.47it/s]\u001b[A\n",
            " 45% 114/255 [00:16<00:18,  7.76it/s]\u001b[A\n",
            " 45% 115/255 [00:16<00:20,  6.87it/s]\u001b[A\n",
            " 45% 116/255 [00:16<00:21,  6.40it/s]\u001b[A\n",
            " 46% 117/255 [00:16<00:19,  7.04it/s]\u001b[A\n",
            " 46% 118/255 [00:16<00:21,  6.26it/s]\u001b[A\n",
            " 47% 119/255 [00:17<00:21,  6.33it/s]\u001b[A\n",
            " 47% 120/255 [00:18<01:22,  1.64it/s]\u001b[A\n",
            " 47% 121/255 [00:18<01:01,  2.16it/s]\u001b[A\n",
            " 48% 122/255 [00:18<00:47,  2.78it/s]\u001b[A\n",
            " 48% 123/255 [00:19<00:37,  3.48it/s]\u001b[A\n",
            " 49% 124/255 [00:19<00:30,  4.23it/s]\u001b[A\n",
            " 49% 125/255 [00:19<00:26,  4.97it/s]\u001b[A\n",
            " 49% 126/255 [00:19<00:23,  5.55it/s]\u001b[A\n",
            " 50% 127/255 [00:19<00:23,  5.53it/s]\u001b[A\n",
            " 50% 128/255 [00:19<00:22,  5.57it/s]\u001b[A\n",
            " 51% 129/255 [00:19<00:20,  6.22it/s]\u001b[A\n",
            " 51% 130/255 [00:20<00:20,  6.23it/s]\u001b[A\n",
            " 51% 131/255 [00:20<00:18,  6.65it/s]\u001b[A\n",
            " 52% 132/255 [00:20<00:17,  7.09it/s]\u001b[A\n",
            " 52% 133/255 [00:20<00:17,  6.89it/s]\u001b[A\n",
            " 53% 134/255 [00:20<00:17,  6.74it/s]\u001b[A\n",
            " 53% 135/255 [00:20<00:17,  7.01it/s]\u001b[A\n",
            " 53% 136/255 [00:20<00:19,  6.20it/s]\u001b[A\n",
            " 54% 137/255 [00:21<00:21,  5.50it/s]\u001b[A\n",
            " 54% 138/255 [00:21<00:21,  5.34it/s]\u001b[A\n",
            " 55% 139/255 [00:21<00:21,  5.34it/s]\u001b[A\n",
            " 55% 140/255 [00:21<00:20,  5.64it/s]\u001b[A\n",
            " 55% 141/255 [00:21<00:20,  5.63it/s]\u001b[A\n",
            " 56% 142/255 [00:22<00:20,  5.61it/s]\u001b[A\n",
            " 56% 143/255 [00:22<00:19,  5.65it/s]\u001b[A\n",
            " 56% 144/255 [00:22<00:20,  5.40it/s]\u001b[A\n",
            " 57% 145/255 [00:22<00:19,  5.65it/s]\u001b[A\n",
            " 57% 146/255 [00:22<00:17,  6.25it/s]\u001b[A\n",
            " 58% 147/255 [00:22<00:15,  6.76it/s]\u001b[A\n",
            " 58% 148/255 [00:22<00:14,  7.16it/s]\u001b[A\n",
            " 58% 149/255 [00:23<00:15,  6.95it/s]\u001b[A\n",
            " 59% 150/255 [00:23<00:15,  6.79it/s]\u001b[A\n",
            " 59% 151/255 [00:23<00:14,  7.19it/s]\u001b[A\n",
            " 60% 152/255 [00:23<00:15,  6.69it/s]\u001b[A\n",
            " 60% 153/255 [00:23<00:16,  6.30it/s]\u001b[A\n",
            " 60% 154/255 [00:23<00:16,  6.30it/s]\u001b[A\n",
            " 61% 155/255 [00:24<00:15,  6.37it/s]\u001b[A\n",
            " 61% 156/255 [00:24<00:15,  6.28it/s]\u001b[A\n",
            " 62% 157/255 [00:24<00:14,  6.71it/s]\u001b[A\n",
            " 62% 158/255 [00:24<00:15,  6.26it/s]\u001b[A\n",
            " 63% 160/255 [00:24<00:13,  6.98it/s]\u001b[A\n",
            " 63% 161/255 [00:24<00:13,  7.18it/s]\u001b[A\n",
            " 64% 162/255 [00:25<00:14,  6.44it/s]\u001b[A\n",
            " 64% 163/255 [00:25<00:13,  6.87it/s]\u001b[A\n",
            " 64% 164/255 [00:25<00:12,  7.11it/s]\u001b[A\n",
            " 65% 165/255 [00:25<00:13,  6.80it/s]\u001b[A\n",
            " 65% 166/255 [00:25<00:13,  6.51it/s]\u001b[A\n",
            " 65% 167/255 [00:25<00:15,  5.72it/s]\u001b[A\n",
            " 66% 168/255 [00:26<00:13,  6.28it/s]\u001b[A\n",
            " 66% 169/255 [00:26<00:12,  6.73it/s]\u001b[A\n",
            " 67% 170/255 [00:26<00:12,  6.74it/s]\u001b[A\n",
            " 67% 171/255 [00:26<00:12,  6.60it/s]\u001b[A\n",
            " 67% 172/255 [00:26<00:12,  6.46it/s]\u001b[A\n",
            " 68% 173/255 [00:26<00:12,  6.36it/s]\u001b[A\n",
            " 68% 174/255 [00:26<00:12,  6.36it/s]\u001b[A\n",
            " 69% 175/255 [00:27<00:11,  6.80it/s]\u001b[A\n",
            " 69% 176/255 [00:27<00:11,  6.65it/s]\u001b[A\n",
            " 69% 177/255 [00:27<00:11,  6.55it/s]\u001b[A\n",
            " 70% 178/255 [00:27<00:11,  6.53it/s]\u001b[A\n",
            " 70% 179/255 [00:27<00:11,  6.49it/s]\u001b[A\n",
            " 71% 180/255 [00:27<00:10,  6.96it/s]\u001b[A\n",
            " 71% 181/255 [00:28<00:11,  6.44it/s]\u001b[A\n",
            " 71% 182/255 [00:28<00:11,  6.10it/s]\u001b[A\n",
            " 72% 183/255 [00:28<00:11,  6.00it/s]\u001b[A\n",
            " 72% 184/255 [00:28<00:11,  6.40it/s]\u001b[A\n",
            " 73% 185/255 [00:28<00:10,  6.93it/s]\u001b[A\n",
            " 73% 186/255 [00:28<00:10,  6.59it/s]\u001b[A\n",
            " 73% 187/255 [00:28<00:10,  6.32it/s]\u001b[A\n",
            " 74% 188/255 [00:29<00:09,  6.99it/s]\u001b[A\n",
            " 74% 189/255 [00:29<00:09,  6.78it/s]\u001b[A\n",
            " 75% 190/255 [00:29<00:09,  7.09it/s]\u001b[A\n",
            " 75% 191/255 [00:29<00:08,  7.37it/s]\u001b[A\n",
            " 75% 192/255 [00:29<00:08,  7.08it/s]\u001b[A\n",
            " 76% 193/255 [00:29<00:08,  6.89it/s]\u001b[A\n",
            " 76% 194/255 [00:29<00:09,  6.75it/s]\u001b[A\n",
            " 76% 195/255 [00:30<00:08,  7.31it/s]\u001b[A\n",
            " 77% 196/255 [00:30<00:08,  6.74it/s]\u001b[A\n",
            " 77% 197/255 [00:30<00:08,  6.70it/s]\u001b[A\n",
            " 78% 198/255 [00:30<00:09,  6.30it/s]\u001b[A\n",
            " 78% 199/255 [00:30<00:08,  6.82it/s]\u001b[A\n",
            " 78% 200/255 [00:30<00:07,  7.42it/s]\u001b[A\n",
            " 79% 201/255 [00:30<00:07,  7.06it/s]\u001b[A\n",
            " 79% 202/255 [00:31<00:07,  7.41it/s]\u001b[A\n",
            " 80% 203/255 [00:31<00:06,  7.70it/s]\u001b[A\n",
            " 80% 204/255 [00:31<00:07,  7.22it/s]\u001b[A\n",
            " 80% 205/255 [00:31<00:06,  7.46it/s]\u001b[A\n",
            " 81% 206/255 [00:31<00:06,  7.68it/s]\u001b[A\n",
            " 81% 207/255 [00:31<00:07,  6.83it/s]\u001b[A\n",
            " 82% 208/255 [00:31<00:07,  6.06it/s]\u001b[A\n",
            " 82% 209/255 [00:32<00:06,  6.61it/s]\u001b[A\n",
            " 82% 210/255 [00:32<00:06,  6.95it/s]\u001b[A\n",
            " 83% 211/255 [00:32<00:06,  7.28it/s]\u001b[A\n",
            " 83% 212/255 [00:32<00:06,  6.70it/s]\u001b[A\n",
            " 84% 213/255 [00:32<00:05,  7.12it/s]\u001b[A\n",
            " 84% 214/255 [00:32<00:05,  7.45it/s]\u001b[A\n",
            " 84% 215/255 [00:32<00:05,  7.44it/s]\u001b[A\n",
            " 85% 216/255 [00:33<00:05,  6.97it/s]\u001b[A\n",
            " 85% 217/255 [00:33<00:05,  6.80it/s]\u001b[A\n",
            " 85% 218/255 [00:33<00:05,  7.21it/s]\u001b[A\n",
            " 86% 220/255 [00:33<00:04,  8.02it/s]\u001b[A\n",
            " 87% 221/255 [00:33<00:04,  7.22it/s]\u001b[A\n",
            " 87% 222/255 [00:33<00:04,  7.45it/s]\u001b[A\n",
            " 87% 223/255 [00:34<00:04,  6.98it/s]\u001b[A\n",
            " 88% 224/255 [00:34<00:04,  6.82it/s]\u001b[A\n",
            " 88% 225/255 [00:34<00:04,  6.71it/s]\u001b[A\n",
            " 89% 226/255 [00:34<00:04,  6.71it/s]\u001b[A\n",
            " 89% 227/255 [00:34<00:04,  6.40it/s]\u001b[A\n",
            " 89% 228/255 [00:34<00:03,  7.01it/s]\u001b[A\n",
            " 90% 229/255 [00:34<00:03,  7.59it/s]\u001b[A\n",
            " 90% 230/255 [00:35<00:03,  6.51it/s]\u001b[A\n",
            " 91% 231/255 [00:35<00:03,  7.09it/s]\u001b[A\n",
            " 91% 232/255 [00:35<00:03,  7.62it/s]\u001b[A\n",
            " 91% 233/255 [00:35<00:02,  7.79it/s]\u001b[A\n",
            " 92% 234/255 [00:35<00:02,  7.03it/s]\u001b[A\n",
            " 92% 235/255 [00:35<00:02,  6.78it/s]\u001b[A\n",
            " 93% 236/255 [00:35<00:02,  6.56it/s]\u001b[A\n",
            " 93% 237/255 [00:36<00:02,  6.19it/s]\u001b[A\n",
            " 93% 238/255 [00:36<00:02,  6.66it/s]\u001b[A\n",
            " 94% 239/255 [00:36<00:02,  6.39it/s]\u001b[A\n",
            " 94% 240/255 [00:36<00:02,  7.06it/s]\u001b[A\n",
            " 95% 241/255 [00:36<00:02,  6.89it/s]\u001b[A\n",
            " 95% 242/255 [00:36<00:02,  6.16it/s]\u001b[A\n",
            " 95% 243/255 [00:36<00:01,  6.77it/s]\u001b[A\n",
            " 96% 244/255 [00:37<00:01,  6.57it/s]\u001b[A\n",
            " 96% 245/255 [00:37<00:01,  7.02it/s]\u001b[A\n",
            " 96% 246/255 [00:37<00:01,  7.39it/s]\u001b[A\n",
            " 97% 247/255 [00:37<00:01,  6.77it/s]\u001b[A\n",
            " 97% 248/255 [00:37<00:00,  7.19it/s]\u001b[A\n",
            " 98% 249/255 [00:37<00:00,  6.97it/s]\u001b[A\n",
            " 98% 250/255 [00:37<00:00,  7.37it/s]\u001b[A\n",
            " 98% 251/255 [00:38<00:00,  6.98it/s]\u001b[A\n",
            " 99% 252/255 [00:38<00:00,  7.27it/s]\u001b[A\n",
            " 99% 253/255 [00:38<00:00,  6.38it/s]\u001b[A\n",
            "100% 254/255 [00:38<00:00,  6.35it/s]\u001b[A\n",
            "{'eval_loss': 0.6929551362991333, 'eval_accuracy': 0.5368, 'eval_runtime': 38.7604, 'eval_samples_per_second': 65.557, 'eval_steps_per_second': 6.579, 'epoch': 1.0}\n",
            "\n",
            " 20% 1853/9265 [11:15<44:58,  2.75it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:44:18,627 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-1853\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:44:18,628 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-1853/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:44:23,969 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-1853/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:44:23,970 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-1853/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:44:23,971 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-1853/special_tokens_map.json\n",
            "{'loss': 0.7005, 'learning_rate': 7.841338370210469e-07, 'epoch': 1.08}\n",
            "{'loss': 0.6962, 'learning_rate': 7.301672962763087e-07, 'epoch': 1.35}\n",
            "{'loss': 0.6968, 'learning_rate': 6.762007555315704e-07, 'epoch': 1.62}\n",
            "{'loss': 0.6973, 'learning_rate': 6.222342147868322e-07, 'epoch': 1.89}\n",
            " 40% 3706/9265 [22:15<29:38,  3.13it/s][INFO|trainer.py:722] 2023-01-15 16:55:18,537 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice1, context, choice0. If choice1, context, choice0 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:55:18,540 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:55:18,540 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:55:18,540 >>   Batch size = 10\n",
            "\n",
            "  0% 0/255 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/255 [00:00<00:24, 10.47it/s]\u001b[A\n",
            "  2% 4/255 [00:00<00:34,  7.26it/s]\u001b[A\n",
            "  2% 5/255 [00:00<00:37,  6.64it/s]\u001b[A\n",
            "  2% 6/255 [00:00<00:37,  6.59it/s]\u001b[A\n",
            "  3% 7/255 [00:01<00:37,  6.59it/s]\u001b[A\n",
            "  3% 8/255 [00:01<00:38,  6.34it/s]\u001b[A\n",
            "  4% 9/255 [00:01<00:34,  7.03it/s]\u001b[A\n",
            "  4% 10/255 [00:01<00:33,  7.34it/s]\u001b[A\n",
            "  4% 11/255 [00:01<00:36,  6.75it/s]\u001b[A\n",
            "  5% 12/255 [00:01<00:34,  7.08it/s]\u001b[A\n",
            "  5% 13/255 [00:01<00:36,  6.58it/s]\u001b[A\n",
            "  5% 14/255 [00:02<00:38,  6.31it/s]\u001b[A\n",
            "  6% 15/255 [00:02<00:35,  6.84it/s]\u001b[A\n",
            "  6% 16/255 [00:02<00:32,  7.25it/s]\u001b[A\n",
            "  7% 17/255 [00:02<00:31,  7.52it/s]\u001b[A\n",
            "  7% 18/255 [00:02<00:30,  7.83it/s]\u001b[A\n",
            "  7% 19/255 [00:02<00:29,  7.97it/s]\u001b[A\n",
            "  8% 20/255 [00:02<00:31,  7.48it/s]\u001b[A\n",
            "  8% 21/255 [00:02<00:32,  7.20it/s]\u001b[A\n",
            "  9% 22/255 [00:03<00:33,  6.94it/s]\u001b[A\n",
            "  9% 23/255 [00:03<00:34,  6.81it/s]\u001b[A\n",
            "  9% 24/255 [00:03<00:34,  6.67it/s]\u001b[A\n",
            " 10% 25/255 [00:03<00:31,  7.25it/s]\u001b[A\n",
            " 10% 26/255 [00:03<00:29,  7.79it/s]\u001b[A\n",
            " 11% 27/255 [00:03<00:28,  7.99it/s]\u001b[A\n",
            " 11% 28/255 [00:03<00:27,  8.37it/s]\u001b[A\n",
            " 11% 29/255 [00:03<00:26,  8.46it/s]\u001b[A\n",
            " 12% 30/255 [00:04<00:28,  7.78it/s]\u001b[A\n",
            " 12% 31/255 [00:04<00:34,  6.59it/s]\u001b[A\n",
            " 13% 32/255 [00:04<00:37,  5.92it/s]\u001b[A\n",
            " 13% 33/255 [00:04<00:34,  6.49it/s]\u001b[A\n",
            " 13% 34/255 [00:04<00:34,  6.43it/s]\u001b[A\n",
            " 14% 35/255 [00:04<00:32,  6.81it/s]\u001b[A\n",
            " 14% 36/255 [00:05<00:30,  7.24it/s]\u001b[A\n",
            " 15% 37/255 [00:05<00:32,  6.70it/s]\u001b[A\n",
            " 15% 38/255 [00:05<00:32,  6.60it/s]\u001b[A\n",
            " 15% 39/255 [00:05<00:30,  7.00it/s]\u001b[A\n",
            " 16% 40/255 [00:05<00:31,  6.86it/s]\u001b[A\n",
            " 16% 41/255 [00:05<00:33,  6.36it/s]\u001b[A\n",
            " 16% 42/255 [00:05<00:30,  7.01it/s]\u001b[A\n",
            " 17% 43/255 [00:06<00:29,  7.31it/s]\u001b[A\n",
            " 17% 44/255 [00:06<00:31,  6.74it/s]\u001b[A\n",
            " 18% 45/255 [00:06<00:31,  6.57it/s]\u001b[A\n",
            " 18% 46/255 [00:06<00:33,  6.22it/s]\u001b[A\n",
            " 18% 47/255 [00:06<00:30,  6.78it/s]\u001b[A\n",
            " 19% 48/255 [00:06<00:29,  7.01it/s]\u001b[A\n",
            " 19% 49/255 [00:07<00:30,  6.80it/s]\u001b[A\n",
            " 20% 50/255 [00:07<00:27,  7.37it/s]\u001b[A\n",
            " 20% 51/255 [00:07<00:28,  7.21it/s]\u001b[A\n",
            " 20% 52/255 [00:07<00:29,  6.96it/s]\u001b[A\n",
            " 21% 53/255 [00:07<00:27,  7.46it/s]\u001b[A\n",
            " 21% 54/255 [00:07<00:30,  6.51it/s]\u001b[A\n",
            " 22% 55/255 [00:07<00:30,  6.48it/s]\u001b[A\n",
            " 22% 56/255 [00:08<00:30,  6.44it/s]\u001b[A\n",
            " 22% 57/255 [00:08<00:30,  6.45it/s]\u001b[A\n",
            " 23% 58/255 [00:08<00:31,  6.16it/s]\u001b[A\n",
            " 23% 59/255 [00:08<00:28,  6.81it/s]\u001b[A\n",
            " 24% 60/255 [00:08<00:26,  7.26it/s]\u001b[A\n",
            " 24% 61/255 [00:08<00:28,  6.77it/s]\u001b[A\n",
            " 24% 62/255 [00:08<00:29,  6.60it/s]\u001b[A\n",
            " 25% 63/255 [00:09<00:27,  6.90it/s]\u001b[A\n",
            " 25% 64/255 [00:09<00:26,  7.09it/s]\u001b[A\n",
            " 25% 65/255 [00:09<00:28,  6.74it/s]\u001b[A\n",
            " 26% 66/255 [00:09<00:26,  7.15it/s]\u001b[A\n",
            " 26% 67/255 [00:09<00:25,  7.45it/s]\u001b[A\n",
            " 27% 68/255 [00:09<00:30,  6.08it/s]\u001b[A\n",
            " 27% 69/255 [00:10<00:31,  5.95it/s]\u001b[A\n",
            " 27% 70/255 [00:10<00:28,  6.56it/s]\u001b[A\n",
            " 28% 71/255 [00:10<00:26,  6.86it/s]\u001b[A\n",
            " 28% 72/255 [00:10<00:25,  7.22it/s]\u001b[A\n",
            " 29% 73/255 [00:10<00:26,  6.88it/s]\u001b[A\n",
            " 29% 74/255 [00:10<00:25,  7.24it/s]\u001b[A\n",
            " 29% 75/255 [00:10<00:25,  7.05it/s]\u001b[A\n",
            " 30% 76/255 [00:10<00:24,  7.42it/s]\u001b[A\n",
            " 30% 77/255 [00:11<00:25,  7.03it/s]\u001b[A\n",
            " 31% 78/255 [00:11<00:23,  7.59it/s]\u001b[A\n",
            " 31% 79/255 [00:11<00:22,  7.85it/s]\u001b[A\n",
            " 31% 80/255 [00:11<00:21,  8.03it/s]\u001b[A\n",
            " 32% 81/255 [00:11<00:22,  7.84it/s]\u001b[A\n",
            " 32% 82/255 [00:11<00:20,  8.26it/s]\u001b[A\n",
            " 33% 83/255 [00:11<00:24,  6.94it/s]\u001b[A\n",
            " 33% 84/255 [00:12<00:23,  7.32it/s]\u001b[A\n",
            " 33% 85/255 [00:12<00:23,  7.10it/s]\u001b[A\n",
            " 34% 86/255 [00:12<00:22,  7.43it/s]\u001b[A\n",
            " 34% 87/255 [00:12<00:24,  6.84it/s]\u001b[A\n",
            " 35% 88/255 [00:12<00:22,  7.27it/s]\u001b[A\n",
            " 35% 89/255 [00:12<00:21,  7.62it/s]\u001b[A\n",
            " 35% 90/255 [00:12<00:26,  6.25it/s]\u001b[A\n",
            " 36% 91/255 [00:13<00:24,  6.82it/s]\u001b[A\n",
            " 36% 92/255 [00:13<00:24,  6.65it/s]\u001b[A\n",
            " 36% 93/255 [00:13<00:25,  6.34it/s]\u001b[A\n",
            " 37% 94/255 [00:13<00:24,  6.69it/s]\u001b[A\n",
            " 37% 95/255 [00:13<00:22,  7.00it/s]\u001b[A\n",
            " 38% 96/255 [00:13<00:23,  6.75it/s]\u001b[A\n",
            " 38% 97/255 [00:13<00:22,  7.16it/s]\u001b[A\n",
            " 38% 98/255 [00:14<00:22,  6.87it/s]\u001b[A\n",
            " 39% 99/255 [00:14<00:21,  7.16it/s]\u001b[A\n",
            " 39% 100/255 [00:14<00:20,  7.43it/s]\u001b[A\n",
            " 40% 101/255 [00:14<00:20,  7.66it/s]\u001b[A\n",
            " 40% 102/255 [00:14<00:21,  7.20it/s]\u001b[A\n",
            " 40% 103/255 [00:14<00:20,  7.52it/s]\u001b[A\n",
            " 41% 104/255 [00:14<00:19,  7.63it/s]\u001b[A\n",
            " 41% 105/255 [00:14<00:19,  7.80it/s]\u001b[A\n",
            " 42% 106/255 [00:15<00:19,  7.70it/s]\u001b[A\n",
            " 42% 107/255 [00:15<00:18,  7.86it/s]\u001b[A\n",
            " 42% 108/255 [00:15<00:20,  7.27it/s]\u001b[A\n",
            " 43% 109/255 [00:15<00:19,  7.61it/s]\u001b[A\n",
            " 43% 110/255 [00:15<00:17,  8.13it/s]\u001b[A\n",
            " 44% 111/255 [00:15<00:18,  7.65it/s]\u001b[A\n",
            " 44% 112/255 [00:15<00:19,  7.32it/s]\u001b[A\n",
            " 44% 113/255 [00:16<00:19,  7.47it/s]\u001b[A\n",
            " 45% 114/255 [00:16<00:18,  7.76it/s]\u001b[A\n",
            " 45% 115/255 [00:16<00:20,  6.90it/s]\u001b[A\n",
            " 45% 116/255 [00:16<00:21,  6.43it/s]\u001b[A\n",
            " 46% 117/255 [00:16<00:19,  7.07it/s]\u001b[A\n",
            " 46% 118/255 [00:16<00:21,  6.30it/s]\u001b[A\n",
            " 47% 119/255 [00:16<00:21,  6.33it/s]\u001b[A\n",
            " 47% 120/255 [00:18<01:22,  1.64it/s]\u001b[A\n",
            " 47% 121/255 [00:18<01:01,  2.17it/s]\u001b[A\n",
            " 48% 122/255 [00:18<00:47,  2.78it/s]\u001b[A\n",
            " 48% 123/255 [00:18<00:38,  3.47it/s]\u001b[A\n",
            " 49% 124/255 [00:19<00:31,  4.21it/s]\u001b[A\n",
            " 49% 125/255 [00:19<00:26,  4.95it/s]\u001b[A\n",
            " 49% 126/255 [00:19<00:23,  5.52it/s]\u001b[A\n",
            " 50% 127/255 [00:19<00:23,  5.49it/s]\u001b[A\n",
            " 50% 128/255 [00:19<00:22,  5.55it/s]\u001b[A\n",
            " 51% 129/255 [00:19<00:20,  6.21it/s]\u001b[A\n",
            " 51% 130/255 [00:19<00:20,  6.22it/s]\u001b[A\n",
            " 51% 131/255 [00:20<00:18,  6.73it/s]\u001b[A\n",
            " 52% 132/255 [00:20<00:17,  7.19it/s]\u001b[A\n",
            " 52% 133/255 [00:20<00:17,  6.98it/s]\u001b[A\n",
            " 53% 134/255 [00:20<00:17,  6.79it/s]\u001b[A\n",
            " 53% 135/255 [00:20<00:16,  7.09it/s]\u001b[A\n",
            " 53% 136/255 [00:20<00:18,  6.29it/s]\u001b[A\n",
            " 54% 137/255 [00:21<00:21,  5.59it/s]\u001b[A\n",
            " 54% 138/255 [00:21<00:21,  5.38it/s]\u001b[A\n",
            " 55% 139/255 [00:21<00:21,  5.35it/s]\u001b[A\n",
            " 55% 140/255 [00:21<00:20,  5.63it/s]\u001b[A\n",
            " 55% 141/255 [00:21<00:20,  5.64it/s]\u001b[A\n",
            " 56% 142/255 [00:21<00:19,  5.66it/s]\u001b[A\n",
            " 56% 143/255 [00:22<00:19,  5.69it/s]\u001b[A\n",
            " 56% 144/255 [00:22<00:20,  5.46it/s]\u001b[A\n",
            " 57% 145/255 [00:22<00:19,  5.74it/s]\u001b[A\n",
            " 57% 146/255 [00:22<00:17,  6.33it/s]\u001b[A\n",
            " 58% 147/255 [00:22<00:15,  6.80it/s]\u001b[A\n",
            " 58% 148/255 [00:22<00:14,  7.22it/s]\u001b[A\n",
            " 58% 149/255 [00:23<00:15,  6.99it/s]\u001b[A\n",
            " 59% 150/255 [00:23<00:15,  6.78it/s]\u001b[A\n",
            " 59% 151/255 [00:23<00:14,  7.18it/s]\u001b[A\n",
            " 60% 152/255 [00:23<00:15,  6.69it/s]\u001b[A\n",
            " 60% 153/255 [00:23<00:16,  6.32it/s]\u001b[A\n",
            " 60% 154/255 [00:23<00:15,  6.32it/s]\u001b[A\n",
            " 61% 155/255 [00:23<00:15,  6.39it/s]\u001b[A\n",
            " 61% 156/255 [00:24<00:15,  6.30it/s]\u001b[A\n",
            " 62% 157/255 [00:24<00:14,  6.72it/s]\u001b[A\n",
            " 62% 158/255 [00:24<00:15,  6.32it/s]\u001b[A\n",
            " 63% 160/255 [00:24<00:13,  7.00it/s]\u001b[A\n",
            " 63% 161/255 [00:24<00:13,  7.20it/s]\u001b[A\n",
            " 64% 162/255 [00:25<00:14,  6.44it/s]\u001b[A\n",
            " 64% 163/255 [00:25<00:13,  6.88it/s]\u001b[A\n",
            " 64% 164/255 [00:25<00:12,  7.13it/s]\u001b[A\n",
            " 65% 165/255 [00:25<00:13,  6.86it/s]\u001b[A\n",
            " 65% 166/255 [00:25<00:13,  6.64it/s]\u001b[A\n",
            " 65% 167/255 [00:25<00:15,  5.80it/s]\u001b[A\n",
            " 66% 168/255 [00:25<00:13,  6.30it/s]\u001b[A\n",
            " 66% 169/255 [00:26<00:12,  6.73it/s]\u001b[A\n",
            " 67% 170/255 [00:26<00:12,  6.68it/s]\u001b[A\n",
            " 67% 171/255 [00:26<00:12,  6.58it/s]\u001b[A\n",
            " 67% 172/255 [00:26<00:12,  6.50it/s]\u001b[A\n",
            " 68% 173/255 [00:26<00:12,  6.39it/s]\u001b[A\n",
            " 68% 174/255 [00:26<00:12,  6.31it/s]\u001b[A\n",
            " 69% 175/255 [00:26<00:11,  6.75it/s]\u001b[A\n",
            " 69% 176/255 [00:27<00:11,  6.60it/s]\u001b[A\n",
            " 69% 177/255 [00:27<00:12,  6.48it/s]\u001b[A\n",
            " 70% 178/255 [00:27<00:11,  6.48it/s]\u001b[A\n",
            " 70% 179/255 [00:27<00:11,  6.46it/s]\u001b[A\n",
            " 71% 180/255 [00:27<00:10,  6.98it/s]\u001b[A\n",
            " 71% 181/255 [00:27<00:11,  6.43it/s]\u001b[A\n",
            " 71% 182/255 [00:28<00:11,  6.11it/s]\u001b[A\n",
            " 72% 183/255 [00:28<00:12,  6.00it/s]\u001b[A\n",
            " 72% 184/255 [00:28<00:11,  6.40it/s]\u001b[A\n",
            " 73% 185/255 [00:28<00:10,  6.95it/s]\u001b[A\n",
            " 73% 186/255 [00:28<00:10,  6.64it/s]\u001b[A\n",
            " 73% 187/255 [00:28<00:10,  6.33it/s]\u001b[A\n",
            " 74% 188/255 [00:28<00:09,  7.04it/s]\u001b[A\n",
            " 74% 189/255 [00:29<00:09,  6.80it/s]\u001b[A\n",
            " 75% 190/255 [00:29<00:09,  7.13it/s]\u001b[A\n",
            " 75% 191/255 [00:29<00:08,  7.42it/s]\u001b[A\n",
            " 75% 192/255 [00:29<00:08,  7.15it/s]\u001b[A\n",
            " 76% 193/255 [00:29<00:08,  7.04it/s]\u001b[A\n",
            " 76% 194/255 [00:29<00:09,  6.76it/s]\u001b[A\n",
            " 76% 195/255 [00:29<00:08,  7.28it/s]\u001b[A\n",
            " 77% 196/255 [00:30<00:08,  6.67it/s]\u001b[A\n",
            " 77% 197/255 [00:30<00:08,  6.65it/s]\u001b[A\n",
            " 78% 198/255 [00:30<00:09,  6.29it/s]\u001b[A\n",
            " 78% 199/255 [00:30<00:08,  6.81it/s]\u001b[A\n",
            " 78% 200/255 [00:30<00:07,  7.47it/s]\u001b[A\n",
            " 79% 201/255 [00:30<00:07,  7.08it/s]\u001b[A\n",
            " 79% 202/255 [00:30<00:07,  7.42it/s]\u001b[A\n",
            " 80% 203/255 [00:31<00:06,  7.71it/s]\u001b[A\n",
            " 80% 204/255 [00:31<00:06,  7.32it/s]\u001b[A\n",
            " 80% 205/255 [00:31<00:06,  7.63it/s]\u001b[A\n",
            " 81% 206/255 [00:31<00:06,  7.85it/s]\u001b[A\n",
            " 81% 207/255 [00:31<00:06,  6.98it/s]\u001b[A\n",
            " 82% 208/255 [00:31<00:07,  6.16it/s]\u001b[A\n",
            " 82% 209/255 [00:31<00:06,  6.69it/s]\u001b[A\n",
            " 82% 210/255 [00:32<00:06,  7.02it/s]\u001b[A\n",
            " 83% 211/255 [00:32<00:05,  7.36it/s]\u001b[A\n",
            " 83% 212/255 [00:32<00:06,  6.75it/s]\u001b[A\n",
            " 84% 213/255 [00:32<00:05,  7.16it/s]\u001b[A\n",
            " 84% 214/255 [00:32<00:05,  7.52it/s]\u001b[A\n",
            " 84% 215/255 [00:32<00:05,  7.51it/s]\u001b[A\n",
            " 85% 216/255 [00:32<00:05,  6.98it/s]\u001b[A\n",
            " 85% 217/255 [00:33<00:05,  6.81it/s]\u001b[A\n",
            " 85% 218/255 [00:33<00:05,  7.22it/s]\u001b[A\n",
            " 86% 220/255 [00:33<00:04,  8.04it/s]\u001b[A\n",
            " 87% 221/255 [00:33<00:04,  7.23it/s]\u001b[A\n",
            " 87% 222/255 [00:33<00:04,  7.48it/s]\u001b[A\n",
            " 87% 223/255 [00:33<00:04,  7.06it/s]\u001b[A\n",
            " 88% 224/255 [00:34<00:04,  6.90it/s]\u001b[A\n",
            " 88% 225/255 [00:34<00:04,  6.77it/s]\u001b[A\n",
            " 89% 226/255 [00:34<00:04,  6.76it/s]\u001b[A\n",
            " 89% 227/255 [00:34<00:04,  6.42it/s]\u001b[A\n",
            " 89% 228/255 [00:34<00:03,  7.01it/s]\u001b[A\n",
            " 90% 229/255 [00:34<00:03,  7.57it/s]\u001b[A\n",
            " 90% 230/255 [00:34<00:03,  6.56it/s]\u001b[A\n",
            " 91% 231/255 [00:35<00:03,  7.25it/s]\u001b[A\n",
            " 91% 232/255 [00:35<00:02,  7.80it/s]\u001b[A\n",
            " 91% 233/255 [00:35<00:02,  7.89it/s]\u001b[A\n",
            " 92% 234/255 [00:35<00:02,  7.06it/s]\u001b[A\n",
            " 92% 235/255 [00:35<00:02,  6.88it/s]\u001b[A\n",
            " 93% 236/255 [00:35<00:02,  6.63it/s]\u001b[A\n",
            " 93% 237/255 [00:35<00:02,  6.24it/s]\u001b[A\n",
            " 93% 238/255 [00:36<00:02,  6.71it/s]\u001b[A\n",
            " 94% 239/255 [00:36<00:02,  6.45it/s]\u001b[A\n",
            " 94% 240/255 [00:36<00:02,  7.13it/s]\u001b[A\n",
            " 95% 241/255 [00:36<00:02,  6.94it/s]\u001b[A\n",
            " 95% 242/255 [00:36<00:02,  6.18it/s]\u001b[A\n",
            " 95% 243/255 [00:36<00:01,  6.76it/s]\u001b[A\n",
            " 96% 244/255 [00:36<00:01,  6.61it/s]\u001b[A\n",
            " 96% 245/255 [00:37<00:01,  7.05it/s]\u001b[A\n",
            " 96% 246/255 [00:37<00:01,  7.42it/s]\u001b[A\n",
            " 97% 247/255 [00:37<00:01,  6.81it/s]\u001b[A\n",
            " 97% 248/255 [00:37<00:00,  7.24it/s]\u001b[A\n",
            " 98% 249/255 [00:37<00:00,  6.97it/s]\u001b[A\n",
            " 98% 250/255 [00:37<00:00,  7.34it/s]\u001b[A\n",
            " 98% 251/255 [00:37<00:00,  6.95it/s]\u001b[A\n",
            " 99% 252/255 [00:38<00:00,  7.25it/s]\u001b[A\n",
            " 99% 253/255 [00:38<00:00,  6.39it/s]\u001b[A\n",
            "100% 254/255 [00:38<00:00,  6.33it/s]\u001b[A\n",
            "{'eval_loss': 0.6930314302444458, 'eval_accuracy': 0.5254, 'eval_runtime': 38.5731, 'eval_samples_per_second': 65.875, 'eval_steps_per_second': 6.611, 'epoch': 2.0}\n",
            "\n",
            " 40% 3706/9265 [22:54<29:38,  3.13it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:55:57,116 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-3706\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:55:57,117 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-3706/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:56:02,480 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-3706/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:56:02,481 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-3706/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:56:02,481 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-3706/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:56:14,005 >> Deleting older checkpoint [saved/csqa2/mcq/roberta-large/checkpoint-1853] due to args.save_total_limit\n",
            "{'loss': 0.6973, 'learning_rate': 5.682676740420939e-07, 'epoch': 2.16}\n",
            "{'loss': 0.6967, 'learning_rate': 5.143011332973555e-07, 'epoch': 2.43}\n",
            "{'loss': 0.6945, 'learning_rate': 4.6033459255261735e-07, 'epoch': 2.7}\n",
            "{'loss': 0.6944, 'learning_rate': 4.063680518078791e-07, 'epoch': 2.97}\n",
            " 60% 5559/9265 [33:55<20:54,  2.95it/s][INFO|trainer.py:722] 2023-01-15 17:06:58,456 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice1, context, choice0. If choice1, context, choice0 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 17:06:58,459 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 17:06:58,459 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-15 17:06:58,459 >>   Batch size = 10\n",
            "\n",
            "  0% 0/255 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/255 [00:00<00:23, 10.93it/s]\u001b[A\n",
            "  2% 4/255 [00:00<00:34,  7.24it/s]\u001b[A\n",
            "  2% 5/255 [00:00<00:38,  6.51it/s]\u001b[A\n",
            "  2% 6/255 [00:00<00:38,  6.49it/s]\u001b[A\n",
            "  3% 7/255 [00:01<00:38,  6.46it/s]\u001b[A\n",
            "  3% 8/255 [00:01<00:39,  6.31it/s]\u001b[A\n",
            "  4% 9/255 [00:01<00:35,  6.99it/s]\u001b[A\n",
            "  4% 10/255 [00:01<00:33,  7.24it/s]\u001b[A\n",
            "  4% 11/255 [00:01<00:36,  6.64it/s]\u001b[A\n",
            "  5% 12/255 [00:01<00:35,  6.94it/s]\u001b[A\n",
            "  5% 13/255 [00:01<00:37,  6.50it/s]\u001b[A\n",
            "  5% 14/255 [00:02<00:38,  6.28it/s]\u001b[A\n",
            "  6% 15/255 [00:02<00:35,  6.80it/s]\u001b[A\n",
            "  6% 16/255 [00:02<00:33,  7.18it/s]\u001b[A\n",
            "  7% 17/255 [00:02<00:32,  7.43it/s]\u001b[A\n",
            "  7% 18/255 [00:02<00:30,  7.68it/s]\u001b[A\n",
            "  7% 19/255 [00:02<00:30,  7.78it/s]\u001b[A\n",
            "  8% 20/255 [00:02<00:32,  7.33it/s]\u001b[A\n",
            "  8% 21/255 [00:02<00:32,  7.10it/s]\u001b[A\n",
            "  9% 22/255 [00:03<00:33,  6.92it/s]\u001b[A\n",
            "  9% 23/255 [00:03<00:34,  6.78it/s]\u001b[A\n",
            "  9% 24/255 [00:03<00:34,  6.66it/s]\u001b[A\n",
            " 10% 25/255 [00:03<00:32,  7.17it/s]\u001b[A\n",
            " 10% 26/255 [00:03<00:29,  7.73it/s]\u001b[A\n",
            " 11% 27/255 [00:03<00:29,  7.84it/s]\u001b[A\n",
            " 11% 28/255 [00:03<00:27,  8.22it/s]\u001b[A\n",
            " 11% 29/255 [00:04<00:27,  8.30it/s]\u001b[A\n",
            " 12% 30/255 [00:04<00:29,  7.62it/s]\u001b[A\n",
            " 12% 31/255 [00:04<00:34,  6.56it/s]\u001b[A\n",
            " 13% 32/255 [00:04<00:37,  5.92it/s]\u001b[A\n",
            " 13% 33/255 [00:04<00:34,  6.47it/s]\u001b[A\n",
            " 13% 34/255 [00:04<00:34,  6.41it/s]\u001b[A\n",
            " 14% 35/255 [00:05<00:32,  6.74it/s]\u001b[A\n",
            " 14% 36/255 [00:05<00:30,  7.15it/s]\u001b[A\n",
            " 15% 37/255 [00:05<00:32,  6.62it/s]\u001b[A\n",
            " 15% 38/255 [00:05<00:33,  6.51it/s]\u001b[A\n",
            " 15% 39/255 [00:05<00:31,  6.87it/s]\u001b[A\n",
            " 16% 40/255 [00:05<00:32,  6.70it/s]\u001b[A\n",
            " 16% 41/255 [00:05<00:34,  6.25it/s]\u001b[A\n",
            " 16% 42/255 [00:06<00:30,  6.89it/s]\u001b[A\n",
            " 17% 43/255 [00:06<00:29,  7.21it/s]\u001b[A\n",
            " 17% 44/255 [00:06<00:31,  6.66it/s]\u001b[A\n",
            " 18% 45/255 [00:06<00:32,  6.49it/s]\u001b[A\n",
            " 18% 46/255 [00:06<00:33,  6.22it/s]\u001b[A\n",
            " 18% 47/255 [00:06<00:30,  6.77it/s]\u001b[A\n",
            " 19% 48/255 [00:06<00:29,  6.99it/s]\u001b[A\n",
            " 19% 49/255 [00:07<00:30,  6.74it/s]\u001b[A\n",
            " 20% 50/255 [00:07<00:27,  7.34it/s]\u001b[A\n",
            " 20% 51/255 [00:07<00:28,  7.15it/s]\u001b[A\n",
            " 20% 52/255 [00:07<00:29,  6.87it/s]\u001b[A\n",
            " 21% 53/255 [00:07<00:27,  7.42it/s]\u001b[A\n",
            " 21% 54/255 [00:07<00:31,  6.45it/s]\u001b[A\n",
            " 22% 55/255 [00:07<00:31,  6.41it/s]\u001b[A\n",
            " 22% 56/255 [00:08<00:31,  6.29it/s]\u001b[A\n",
            " 22% 57/255 [00:08<00:30,  6.39it/s]\u001b[A\n",
            " 23% 58/255 [00:08<00:32,  6.11it/s]\u001b[A\n",
            " 23% 59/255 [00:08<00:28,  6.79it/s]\u001b[A\n",
            " 24% 60/255 [00:08<00:26,  7.23it/s]\u001b[A\n",
            " 24% 61/255 [00:08<00:28,  6.70it/s]\u001b[A\n",
            " 24% 62/255 [00:09<00:29,  6.53it/s]\u001b[A\n",
            " 25% 63/255 [00:09<00:28,  6.81it/s]\u001b[A\n",
            " 25% 64/255 [00:09<00:27,  7.03it/s]\u001b[A\n",
            " 25% 65/255 [00:09<00:28,  6.73it/s]\u001b[A\n",
            " 26% 66/255 [00:09<00:26,  7.18it/s]\u001b[A\n",
            " 26% 67/255 [00:09<00:25,  7.50it/s]\u001b[A\n",
            " 27% 68/255 [00:09<00:30,  6.07it/s]\u001b[A\n",
            " 27% 69/255 [00:10<00:31,  5.88it/s]\u001b[A\n",
            " 27% 70/255 [00:10<00:28,  6.49it/s]\u001b[A\n",
            " 28% 71/255 [00:10<00:26,  6.84it/s]\u001b[A\n",
            " 28% 72/255 [00:10<00:25,  7.17it/s]\u001b[A\n",
            " 29% 73/255 [00:10<00:26,  6.83it/s]\u001b[A\n",
            " 29% 74/255 [00:10<00:25,  7.16it/s]\u001b[A\n",
            " 29% 75/255 [00:10<00:25,  6.93it/s]\u001b[A\n",
            " 30% 76/255 [00:11<00:24,  7.27it/s]\u001b[A\n",
            " 30% 77/255 [00:11<00:26,  6.84it/s]\u001b[A\n",
            " 31% 78/255 [00:11<00:23,  7.41it/s]\u001b[A\n",
            " 31% 79/255 [00:11<00:22,  7.71it/s]\u001b[A\n",
            " 31% 80/255 [00:11<00:22,  7.87it/s]\u001b[A\n",
            " 32% 81/255 [00:11<00:22,  7.80it/s]\u001b[A\n",
            " 32% 82/255 [00:11<00:20,  8.25it/s]\u001b[A\n",
            " 33% 83/255 [00:12<00:24,  6.95it/s]\u001b[A\n",
            " 33% 84/255 [00:12<00:23,  7.32it/s]\u001b[A\n",
            " 33% 85/255 [00:12<00:24,  7.03it/s]\u001b[A\n",
            " 34% 86/255 [00:12<00:22,  7.37it/s]\u001b[A\n",
            " 34% 87/255 [00:12<00:24,  6.83it/s]\u001b[A\n",
            " 35% 88/255 [00:12<00:23,  7.25it/s]\u001b[A\n",
            " 35% 89/255 [00:12<00:21,  7.58it/s]\u001b[A\n",
            " 35% 90/255 [00:13<00:26,  6.22it/s]\u001b[A\n",
            " 36% 91/255 [00:13<00:24,  6.77it/s]\u001b[A\n",
            " 36% 92/255 [00:13<00:24,  6.61it/s]\u001b[A\n",
            " 36% 93/255 [00:13<00:25,  6.31it/s]\u001b[A\n",
            " 37% 94/255 [00:13<00:24,  6.69it/s]\u001b[A\n",
            " 37% 95/255 [00:13<00:23,  6.96it/s]\u001b[A\n",
            " 38% 96/255 [00:13<00:23,  6.74it/s]\u001b[A\n",
            " 38% 97/255 [00:14<00:21,  7.18it/s]\u001b[A\n",
            " 38% 98/255 [00:14<00:22,  6.89it/s]\u001b[A\n",
            " 39% 99/255 [00:14<00:21,  7.17it/s]\u001b[A\n",
            " 39% 100/255 [00:14<00:20,  7.43it/s]\u001b[A\n",
            " 40% 101/255 [00:14<00:20,  7.67it/s]\u001b[A\n",
            " 40% 102/255 [00:14<00:21,  7.20it/s]\u001b[A\n",
            " 40% 103/255 [00:14<00:20,  7.47it/s]\u001b[A\n",
            " 41% 104/255 [00:14<00:20,  7.54it/s]\u001b[A\n",
            " 41% 105/255 [00:15<00:19,  7.76it/s]\u001b[A\n",
            " 42% 106/255 [00:15<00:19,  7.71it/s]\u001b[A\n",
            " 42% 107/255 [00:15<00:18,  7.81it/s]\u001b[A\n",
            " 42% 108/255 [00:15<00:20,  7.20it/s]\u001b[A\n",
            " 43% 109/255 [00:15<00:19,  7.53it/s]\u001b[A\n",
            " 43% 110/255 [00:15<00:18,  7.99it/s]\u001b[A\n",
            " 44% 111/255 [00:15<00:19,  7.54it/s]\u001b[A\n",
            " 44% 112/255 [00:16<00:19,  7.17it/s]\u001b[A\n",
            " 44% 113/255 [00:16<00:19,  7.42it/s]\u001b[A\n",
            " 45% 114/255 [00:16<00:18,  7.70it/s]\u001b[A\n",
            " 45% 115/255 [00:16<00:20,  6.89it/s]\u001b[A\n",
            " 45% 116/255 [00:16<00:21,  6.42it/s]\u001b[A\n",
            " 46% 117/255 [00:16<00:19,  7.04it/s]\u001b[A\n",
            " 46% 118/255 [00:16<00:21,  6.30it/s]\u001b[A\n",
            " 47% 119/255 [00:17<00:21,  6.33it/s]\u001b[A\n",
            " 47% 120/255 [00:18<01:22,  1.64it/s]\u001b[A\n",
            " 47% 121/255 [00:18<01:02,  2.16it/s]\u001b[A\n",
            " 48% 122/255 [00:19<00:47,  2.78it/s]\u001b[A\n",
            " 48% 123/255 [00:19<00:38,  3.47it/s]\u001b[A\n",
            " 49% 124/255 [00:19<00:31,  4.20it/s]\u001b[A\n",
            " 49% 125/255 [00:19<00:26,  4.96it/s]\u001b[A\n",
            " 49% 126/255 [00:19<00:23,  5.58it/s]\u001b[A\n",
            " 50% 127/255 [00:19<00:23,  5.56it/s]\u001b[A\n",
            " 50% 128/255 [00:19<00:22,  5.60it/s]\u001b[A\n",
            " 51% 129/255 [00:19<00:20,  6.25it/s]\u001b[A\n",
            " 51% 130/255 [00:20<00:20,  6.24it/s]\u001b[A\n",
            " 51% 131/255 [00:20<00:18,  6.60it/s]\u001b[A\n",
            " 52% 132/255 [00:20<00:17,  7.06it/s]\u001b[A\n",
            " 52% 133/255 [00:20<00:17,  6.89it/s]\u001b[A\n",
            " 53% 134/255 [00:20<00:17,  6.74it/s]\u001b[A\n",
            " 53% 135/255 [00:20<00:17,  7.04it/s]\u001b[A\n",
            " 53% 136/255 [00:21<00:19,  6.25it/s]\u001b[A\n",
            " 54% 137/255 [00:21<00:21,  5.50it/s]\u001b[A\n",
            " 54% 138/255 [00:21<00:21,  5.35it/s]\u001b[A\n",
            " 55% 139/255 [00:21<00:21,  5.37it/s]\u001b[A\n",
            " 55% 140/255 [00:21<00:20,  5.67it/s]\u001b[A\n",
            " 55% 141/255 [00:21<00:20,  5.65it/s]\u001b[A\n",
            " 56% 142/255 [00:22<00:19,  5.65it/s]\u001b[A\n",
            " 56% 143/255 [00:22<00:19,  5.66it/s]\u001b[A\n",
            " 56% 144/255 [00:22<00:20,  5.41it/s]\u001b[A\n",
            " 57% 145/255 [00:22<00:19,  5.69it/s]\u001b[A\n",
            " 57% 146/255 [00:22<00:17,  6.30it/s]\u001b[A\n",
            " 58% 147/255 [00:22<00:15,  6.83it/s]\u001b[A\n",
            " 58% 148/255 [00:23<00:14,  7.27it/s]\u001b[A\n",
            " 58% 149/255 [00:23<00:15,  7.01it/s]\u001b[A\n",
            " 59% 150/255 [00:23<00:15,  6.73it/s]\u001b[A\n",
            " 59% 151/255 [00:23<00:14,  7.13it/s]\u001b[A\n",
            " 60% 152/255 [00:23<00:15,  6.64it/s]\u001b[A\n",
            " 60% 153/255 [00:23<00:16,  6.31it/s]\u001b[A\n",
            " 60% 154/255 [00:23<00:16,  6.25it/s]\u001b[A\n",
            " 61% 155/255 [00:24<00:15,  6.32it/s]\u001b[A\n",
            " 61% 156/255 [00:24<00:15,  6.20it/s]\u001b[A\n",
            " 62% 157/255 [00:24<00:14,  6.65it/s]\u001b[A\n",
            " 62% 158/255 [00:24<00:15,  6.27it/s]\u001b[A\n",
            " 63% 160/255 [00:24<00:13,  7.00it/s]\u001b[A\n",
            " 63% 161/255 [00:24<00:13,  7.19it/s]\u001b[A\n",
            " 64% 162/255 [00:25<00:14,  6.43it/s]\u001b[A\n",
            " 64% 163/255 [00:25<00:13,  6.85it/s]\u001b[A\n",
            " 64% 164/255 [00:25<00:12,  7.08it/s]\u001b[A\n",
            " 65% 165/255 [00:25<00:13,  6.79it/s]\u001b[A\n",
            " 65% 166/255 [00:25<00:13,  6.56it/s]\u001b[A\n",
            " 65% 167/255 [00:25<00:15,  5.74it/s]\u001b[A\n",
            " 66% 168/255 [00:26<00:13,  6.29it/s]\u001b[A\n",
            " 66% 169/255 [00:26<00:12,  6.77it/s]\u001b[A\n",
            " 67% 170/255 [00:26<00:12,  6.74it/s]\u001b[A\n",
            " 67% 171/255 [00:26<00:12,  6.53it/s]\u001b[A\n",
            " 67% 172/255 [00:26<00:12,  6.40it/s]\u001b[A\n",
            " 68% 173/255 [00:26<00:13,  6.30it/s]\u001b[A\n",
            " 68% 174/255 [00:27<00:13,  6.23it/s]\u001b[A\n",
            " 69% 175/255 [00:27<00:12,  6.66it/s]\u001b[A\n",
            " 69% 176/255 [00:27<00:11,  6.65it/s]\u001b[A\n",
            " 69% 177/255 [00:27<00:11,  6.52it/s]\u001b[A\n",
            " 70% 178/255 [00:27<00:11,  6.49it/s]\u001b[A\n",
            " 70% 179/255 [00:27<00:11,  6.50it/s]\u001b[A\n",
            " 71% 180/255 [00:27<00:10,  6.96it/s]\u001b[A\n",
            " 71% 181/255 [00:28<00:11,  6.39it/s]\u001b[A\n",
            " 71% 182/255 [00:28<00:11,  6.11it/s]\u001b[A\n",
            " 72% 183/255 [00:28<00:12,  5.99it/s]\u001b[A\n",
            " 72% 184/255 [00:28<00:11,  6.40it/s]\u001b[A\n",
            " 73% 185/255 [00:28<00:10,  6.95it/s]\u001b[A\n",
            " 73% 186/255 [00:28<00:10,  6.68it/s]\u001b[A\n",
            " 73% 187/255 [00:29<00:10,  6.32it/s]\u001b[A\n",
            " 74% 188/255 [00:29<00:09,  7.04it/s]\u001b[A\n",
            " 74% 189/255 [00:29<00:09,  6.85it/s]\u001b[A\n",
            " 75% 190/255 [00:29<00:09,  7.20it/s]\u001b[A\n",
            " 75% 191/255 [00:29<00:08,  7.48it/s]\u001b[A\n",
            " 75% 192/255 [00:29<00:08,  7.23it/s]\u001b[A\n",
            " 76% 193/255 [00:29<00:08,  7.08it/s]\u001b[A\n",
            " 76% 194/255 [00:29<00:08,  6.82it/s]\u001b[A\n",
            " 76% 195/255 [00:30<00:08,  7.35it/s]\u001b[A\n",
            " 77% 196/255 [00:30<00:08,  6.72it/s]\u001b[A\n",
            " 77% 197/255 [00:30<00:08,  6.72it/s]\u001b[A\n",
            " 78% 198/255 [00:30<00:09,  6.28it/s]\u001b[A\n",
            " 78% 199/255 [00:30<00:08,  6.75it/s]\u001b[A\n",
            " 78% 200/255 [00:30<00:07,  7.40it/s]\u001b[A\n",
            " 79% 201/255 [00:31<00:07,  6.97it/s]\u001b[A\n",
            " 79% 202/255 [00:31<00:07,  7.29it/s]\u001b[A\n",
            " 80% 203/255 [00:31<00:06,  7.57it/s]\u001b[A\n",
            " 80% 204/255 [00:31<00:07,  7.16it/s]\u001b[A\n",
            " 80% 205/255 [00:31<00:06,  7.44it/s]\u001b[A\n",
            " 81% 206/255 [00:31<00:06,  7.68it/s]\u001b[A\n",
            " 81% 207/255 [00:31<00:06,  6.87it/s]\u001b[A\n",
            " 82% 208/255 [00:32<00:07,  6.07it/s]\u001b[A\n",
            " 82% 209/255 [00:32<00:06,  6.59it/s]\u001b[A\n",
            " 82% 210/255 [00:32<00:06,  6.96it/s]\u001b[A\n",
            " 83% 211/255 [00:32<00:06,  7.27it/s]\u001b[A\n",
            " 83% 212/255 [00:32<00:06,  6.72it/s]\u001b[A\n",
            " 84% 213/255 [00:32<00:05,  7.15it/s]\u001b[A\n",
            " 84% 214/255 [00:32<00:05,  7.49it/s]\u001b[A\n",
            " 84% 215/255 [00:32<00:05,  7.52it/s]\u001b[A\n",
            " 85% 216/255 [00:33<00:05,  7.03it/s]\u001b[A\n",
            " 85% 217/255 [00:33<00:05,  6.80it/s]\u001b[A\n",
            " 85% 218/255 [00:33<00:05,  7.20it/s]\u001b[A\n",
            " 86% 220/255 [00:33<00:04,  8.05it/s]\u001b[A\n",
            " 87% 221/255 [00:33<00:04,  7.29it/s]\u001b[A\n",
            " 87% 222/255 [00:33<00:04,  7.59it/s]\u001b[A\n",
            " 87% 223/255 [00:34<00:04,  7.09it/s]\u001b[A\n",
            " 88% 224/255 [00:34<00:04,  6.92it/s]\u001b[A\n",
            " 88% 225/255 [00:34<00:04,  6.77it/s]\u001b[A\n",
            " 89% 226/255 [00:34<00:04,  6.78it/s]\u001b[A\n",
            " 89% 227/255 [00:34<00:04,  6.43it/s]\u001b[A\n",
            " 89% 228/255 [00:34<00:03,  7.08it/s]\u001b[A\n",
            " 90% 229/255 [00:34<00:03,  7.57it/s]\u001b[A\n",
            " 90% 230/255 [00:35<00:03,  6.54it/s]\u001b[A\n",
            " 91% 231/255 [00:35<00:03,  7.17it/s]\u001b[A\n",
            " 91% 232/255 [00:35<00:02,  7.70it/s]\u001b[A\n",
            " 91% 233/255 [00:35<00:02,  7.83it/s]\u001b[A\n",
            " 92% 234/255 [00:35<00:02,  7.01it/s]\u001b[A\n",
            " 92% 235/255 [00:35<00:02,  6.85it/s]\u001b[A\n",
            " 93% 236/255 [00:35<00:02,  6.60it/s]\u001b[A\n",
            " 93% 237/255 [00:36<00:02,  6.25it/s]\u001b[A\n",
            " 93% 238/255 [00:36<00:02,  6.70it/s]\u001b[A\n",
            " 94% 239/255 [00:36<00:02,  6.42it/s]\u001b[A\n",
            " 94% 240/255 [00:36<00:02,  7.06it/s]\u001b[A\n",
            " 95% 241/255 [00:36<00:02,  6.95it/s]\u001b[A\n",
            " 95% 242/255 [00:36<00:02,  6.21it/s]\u001b[A\n",
            " 95% 243/255 [00:37<00:01,  6.79it/s]\u001b[A\n",
            " 96% 244/255 [00:37<00:01,  6.62it/s]\u001b[A\n",
            " 96% 245/255 [00:37<00:01,  7.03it/s]\u001b[A\n",
            " 96% 246/255 [00:37<00:01,  7.41it/s]\u001b[A\n",
            " 97% 247/255 [00:37<00:01,  6.80it/s]\u001b[A\n",
            " 97% 248/255 [00:37<00:00,  7.22it/s]\u001b[A\n",
            " 98% 249/255 [00:37<00:00,  6.97it/s]\u001b[A\n",
            " 98% 250/255 [00:37<00:00,  7.36it/s]\u001b[A\n",
            " 98% 251/255 [00:38<00:00,  7.02it/s]\u001b[A\n",
            " 99% 252/255 [00:38<00:00,  7.39it/s]\u001b[A\n",
            " 99% 253/255 [00:38<00:00,  6.53it/s]\u001b[A\n",
            "100% 254/255 [00:38<00:00,  6.45it/s]\u001b[A\n",
            "{'eval_loss': 0.6921427845954895, 'eval_accuracy': 0.5435, 'eval_runtime': 38.7551, 'eval_samples_per_second': 65.566, 'eval_steps_per_second': 6.58, 'epoch': 3.0}\n",
            "\n",
            " 60% 5559/9265 [34:34<20:54,  2.95it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 17:07:37,217 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-5559\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 17:07:37,218 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-5559/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 17:07:42,587 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-5559/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 17:07:42,588 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-5559/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 17:07:42,589 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-5559/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 17:07:53,958 >> Deleting older checkpoint [saved/csqa2/mcq/roberta-large/checkpoint-3706] due to args.save_total_limit\n",
            "{'loss': 0.6961, 'learning_rate': 3.5240151106314083e-07, 'epoch': 3.24}\n",
            "{'loss': 0.694, 'learning_rate': 2.984349703184026e-07, 'epoch': 3.51}\n",
            "{'loss': 0.6957, 'learning_rate': 2.444684295736643e-07, 'epoch': 3.78}\n",
            " 80% 7412/9265 [45:34<10:22,  2.97it/s][INFO|trainer.py:722] 2023-01-15 17:18:37,869 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice1, context, choice0. If choice1, context, choice0 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 17:18:37,871 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 17:18:37,872 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-15 17:18:37,872 >>   Batch size = 10\n",
            "\n",
            "  0% 0/255 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/255 [00:00<00:22, 11.14it/s]\u001b[A\n",
            "  2% 4/255 [00:00<00:34,  7.22it/s]\u001b[A\n",
            "  2% 5/255 [00:00<00:38,  6.53it/s]\u001b[A\n",
            "  2% 6/255 [00:00<00:38,  6.54it/s]\u001b[A\n",
            "  3% 7/255 [00:01<00:38,  6.51it/s]\u001b[A\n",
            "  3% 8/255 [00:01<00:39,  6.30it/s]\u001b[A\n",
            "  4% 9/255 [00:01<00:35,  7.02it/s]\u001b[A\n",
            "  4% 10/255 [00:01<00:33,  7.34it/s]\u001b[A\n",
            "  4% 11/255 [00:01<00:36,  6.65it/s]\u001b[A\n",
            "  5% 12/255 [00:01<00:34,  7.01it/s]\u001b[A\n",
            "  5% 13/255 [00:01<00:37,  6.53it/s]\u001b[A\n",
            "  5% 14/255 [00:02<00:38,  6.26it/s]\u001b[A\n",
            "  6% 15/255 [00:02<00:35,  6.76it/s]\u001b[A\n",
            "  6% 16/255 [00:02<00:33,  7.14it/s]\u001b[A\n",
            "  7% 17/255 [00:02<00:32,  7.39it/s]\u001b[A\n",
            "  7% 18/255 [00:02<00:31,  7.63it/s]\u001b[A\n",
            "  7% 19/255 [00:02<00:30,  7.77it/s]\u001b[A\n",
            "  8% 20/255 [00:02<00:32,  7.34it/s]\u001b[A\n",
            "  8% 21/255 [00:03<00:33,  6.97it/s]\u001b[A\n",
            "  9% 22/255 [00:03<00:34,  6.82it/s]\u001b[A\n",
            "  9% 23/255 [00:03<00:34,  6.73it/s]\u001b[A\n",
            "  9% 24/255 [00:03<00:35,  6.53it/s]\u001b[A\n",
            " 10% 25/255 [00:03<00:32,  7.06it/s]\u001b[A\n",
            " 10% 26/255 [00:03<00:29,  7.65it/s]\u001b[A\n",
            " 11% 27/255 [00:03<00:29,  7.81it/s]\u001b[A\n",
            " 11% 28/255 [00:03<00:27,  8.14it/s]\u001b[A\n",
            " 11% 29/255 [00:04<00:27,  8.24it/s]\u001b[A\n",
            " 12% 30/255 [00:04<00:29,  7.62it/s]\u001b[A\n",
            " 12% 31/255 [00:04<00:34,  6.53it/s]\u001b[A\n",
            " 13% 32/255 [00:04<00:38,  5.83it/s]\u001b[A\n",
            " 13% 33/255 [00:04<00:34,  6.44it/s]\u001b[A\n",
            " 13% 34/255 [00:04<00:34,  6.40it/s]\u001b[A\n",
            " 14% 35/255 [00:05<00:32,  6.73it/s]\u001b[A\n",
            " 14% 36/255 [00:05<00:30,  7.16it/s]\u001b[A\n",
            " 15% 37/255 [00:05<00:32,  6.67it/s]\u001b[A\n",
            " 15% 38/255 [00:05<00:33,  6.50it/s]\u001b[A\n",
            " 15% 39/255 [00:05<00:31,  6.86it/s]\u001b[A\n",
            " 16% 40/255 [00:05<00:31,  6.73it/s]\u001b[A\n",
            " 16% 41/255 [00:05<00:34,  6.24it/s]\u001b[A\n",
            " 16% 42/255 [00:06<00:31,  6.87it/s]\u001b[A\n",
            " 17% 43/255 [00:06<00:29,  7.15it/s]\u001b[A\n",
            " 17% 44/255 [00:06<00:32,  6.57it/s]\u001b[A\n",
            " 18% 45/255 [00:06<00:32,  6.41it/s]\u001b[A\n",
            " 18% 46/255 [00:06<00:34,  6.12it/s]\u001b[A\n",
            " 18% 47/255 [00:06<00:31,  6.67it/s]\u001b[A\n",
            " 19% 48/255 [00:06<00:30,  6.89it/s]\u001b[A\n",
            " 19% 49/255 [00:07<00:30,  6.71it/s]\u001b[A\n",
            " 20% 50/255 [00:07<00:27,  7.33it/s]\u001b[A\n",
            " 20% 51/255 [00:07<00:28,  7.15it/s]\u001b[A\n",
            " 20% 52/255 [00:07<00:29,  6.84it/s]\u001b[A\n",
            " 21% 53/255 [00:07<00:27,  7.37it/s]\u001b[A\n",
            " 21% 54/255 [00:07<00:31,  6.39it/s]\u001b[A\n",
            " 22% 55/255 [00:08<00:31,  6.36it/s]\u001b[A\n",
            " 22% 56/255 [00:08<00:31,  6.32it/s]\u001b[A\n",
            " 22% 57/255 [00:08<00:31,  6.38it/s]\u001b[A\n",
            " 23% 58/255 [00:08<00:32,  6.08it/s]\u001b[A\n",
            " 23% 59/255 [00:08<00:28,  6.76it/s]\u001b[A\n",
            " 24% 60/255 [00:08<00:27,  7.22it/s]\u001b[A\n",
            " 24% 61/255 [00:08<00:29,  6.64it/s]\u001b[A\n",
            " 24% 62/255 [00:09<00:29,  6.44it/s]\u001b[A\n",
            " 25% 63/255 [00:09<00:28,  6.76it/s]\u001b[A\n",
            " 25% 64/255 [00:09<00:27,  7.04it/s]\u001b[A\n",
            " 25% 65/255 [00:09<00:28,  6.70it/s]\u001b[A\n",
            " 26% 66/255 [00:09<00:26,  7.09it/s]\u001b[A\n",
            " 26% 67/255 [00:09<00:25,  7.40it/s]\u001b[A\n",
            " 27% 68/255 [00:09<00:31,  5.99it/s]\u001b[A\n",
            " 27% 69/255 [00:10<00:32,  5.80it/s]\u001b[A\n",
            " 27% 70/255 [00:10<00:28,  6.43it/s]\u001b[A\n",
            " 28% 71/255 [00:10<00:27,  6.73it/s]\u001b[A\n",
            " 28% 72/255 [00:10<00:25,  7.13it/s]\u001b[A\n",
            " 29% 73/255 [00:10<00:26,  6.87it/s]\u001b[A\n",
            " 29% 74/255 [00:10<00:25,  7.19it/s]\u001b[A\n",
            " 29% 75/255 [00:10<00:25,  6.95it/s]\u001b[A\n",
            " 30% 76/255 [00:11<00:24,  7.23it/s]\u001b[A\n",
            " 30% 77/255 [00:11<00:25,  6.85it/s]\u001b[A\n",
            " 31% 78/255 [00:11<00:23,  7.43it/s]\u001b[A\n",
            " 31% 79/255 [00:11<00:22,  7.72it/s]\u001b[A\n",
            " 31% 80/255 [00:11<00:22,  7.88it/s]\u001b[A\n",
            " 32% 81/255 [00:11<00:22,  7.84it/s]\u001b[A\n",
            " 32% 82/255 [00:11<00:20,  8.24it/s]\u001b[A\n",
            " 33% 83/255 [00:12<00:24,  6.88it/s]\u001b[A\n",
            " 33% 84/255 [00:12<00:23,  7.24it/s]\u001b[A\n",
            " 33% 85/255 [00:12<00:24,  6.99it/s]\u001b[A\n",
            " 34% 86/255 [00:12<00:22,  7.35it/s]\u001b[A\n",
            " 34% 87/255 [00:12<00:24,  6.81it/s]\u001b[A\n",
            " 35% 88/255 [00:12<00:23,  7.24it/s]\u001b[A\n",
            " 35% 89/255 [00:12<00:21,  7.62it/s]\u001b[A\n",
            " 35% 90/255 [00:13<00:26,  6.24it/s]\u001b[A\n",
            " 36% 91/255 [00:13<00:24,  6.83it/s]\u001b[A\n",
            " 36% 92/255 [00:13<00:24,  6.65it/s]\u001b[A\n",
            " 36% 93/255 [00:13<00:25,  6.33it/s]\u001b[A\n",
            " 37% 94/255 [00:13<00:24,  6.71it/s]\u001b[A\n",
            " 37% 95/255 [00:13<00:22,  6.96it/s]\u001b[A\n",
            " 38% 96/255 [00:13<00:23,  6.76it/s]\u001b[A\n",
            " 38% 97/255 [00:14<00:21,  7.19it/s]\u001b[A\n",
            " 38% 98/255 [00:14<00:22,  6.93it/s]\u001b[A\n",
            " 39% 99/255 [00:14<00:21,  7.16it/s]\u001b[A\n",
            " 39% 100/255 [00:14<00:20,  7.42it/s]\u001b[A\n",
            " 40% 101/255 [00:14<00:20,  7.67it/s]\u001b[A\n",
            " 40% 102/255 [00:14<00:21,  7.17it/s]\u001b[A\n",
            " 40% 103/255 [00:14<00:20,  7.45it/s]\u001b[A\n",
            " 41% 104/255 [00:15<00:20,  7.52it/s]\u001b[A\n",
            " 41% 105/255 [00:15<00:19,  7.77it/s]\u001b[A\n",
            " 42% 106/255 [00:15<00:19,  7.71it/s]\u001b[A\n",
            " 42% 107/255 [00:15<00:18,  7.82it/s]\u001b[A\n",
            " 42% 108/255 [00:15<00:20,  7.22it/s]\u001b[A\n",
            " 43% 109/255 [00:15<00:19,  7.54it/s]\u001b[A\n",
            " 43% 110/255 [00:15<00:18,  8.00it/s]\u001b[A\n",
            " 44% 111/255 [00:15<00:18,  7.59it/s]\u001b[A\n",
            " 44% 112/255 [00:16<00:19,  7.22it/s]\u001b[A\n",
            " 44% 113/255 [00:16<00:18,  7.47it/s]\u001b[A\n",
            " 45% 114/255 [00:16<00:18,  7.73it/s]\u001b[A\n",
            " 45% 115/255 [00:16<00:20,  6.87it/s]\u001b[A\n",
            " 45% 116/255 [00:16<00:21,  6.43it/s]\u001b[A\n",
            " 46% 117/255 [00:16<00:19,  7.06it/s]\u001b[A\n",
            " 46% 118/255 [00:16<00:21,  6.26it/s]\u001b[A\n",
            " 47% 119/255 [00:17<00:21,  6.32it/s]\u001b[A\n",
            " 47% 120/255 [00:18<01:22,  1.64it/s]\u001b[A\n",
            " 47% 121/255 [00:18<01:01,  2.17it/s]\u001b[A\n",
            " 48% 122/255 [00:19<00:47,  2.79it/s]\u001b[A\n",
            " 48% 123/255 [00:19<00:37,  3.49it/s]\u001b[A\n",
            " 49% 124/255 [00:19<00:31,  4.22it/s]\u001b[A\n",
            " 49% 125/255 [00:19<00:26,  4.96it/s]\u001b[A\n",
            " 49% 126/255 [00:19<00:23,  5.53it/s]\u001b[A\n",
            " 50% 127/255 [00:19<00:23,  5.51it/s]\u001b[A\n",
            " 50% 128/255 [00:19<00:22,  5.54it/s]\u001b[A\n",
            " 51% 129/255 [00:20<00:20,  6.21it/s]\u001b[A\n",
            " 51% 130/255 [00:20<00:20,  6.23it/s]\u001b[A\n",
            " 51% 131/255 [00:20<00:18,  6.67it/s]\u001b[A\n",
            " 52% 132/255 [00:20<00:17,  7.10it/s]\u001b[A\n",
            " 52% 133/255 [00:20<00:17,  6.83it/s]\u001b[A\n",
            " 53% 134/255 [00:20<00:18,  6.69it/s]\u001b[A\n",
            " 53% 135/255 [00:20<00:17,  7.01it/s]\u001b[A\n",
            " 53% 136/255 [00:21<00:18,  6.26it/s]\u001b[A\n",
            " 54% 137/255 [00:21<00:21,  5.59it/s]\u001b[A\n",
            " 54% 138/255 [00:21<00:21,  5.41it/s]\u001b[A\n",
            " 55% 139/255 [00:21<00:21,  5.42it/s]\u001b[A\n",
            " 55% 140/255 [00:21<00:20,  5.71it/s]\u001b[A\n",
            " 55% 141/255 [00:21<00:19,  5.70it/s]\u001b[A\n",
            " 56% 142/255 [00:22<00:19,  5.70it/s]\u001b[A\n",
            " 56% 143/255 [00:22<00:19,  5.74it/s]\u001b[A\n",
            " 56% 144/255 [00:22<00:20,  5.42it/s]\u001b[A\n",
            " 57% 145/255 [00:22<00:19,  5.67it/s]\u001b[A\n",
            " 57% 146/255 [00:22<00:17,  6.29it/s]\u001b[A\n",
            " 58% 147/255 [00:22<00:15,  6.78it/s]\u001b[A\n",
            " 58% 148/255 [00:23<00:14,  7.19it/s]\u001b[A\n",
            " 58% 149/255 [00:23<00:15,  7.02it/s]\u001b[A\n",
            " 59% 150/255 [00:23<00:15,  6.82it/s]\u001b[A\n",
            " 59% 151/255 [00:23<00:14,  7.20it/s]\u001b[A\n",
            " 60% 152/255 [00:23<00:15,  6.67it/s]\u001b[A\n",
            " 60% 153/255 [00:23<00:16,  6.32it/s]\u001b[A\n",
            " 60% 154/255 [00:24<00:16,  6.29it/s]\u001b[A\n",
            " 61% 155/255 [00:24<00:15,  6.37it/s]\u001b[A\n",
            " 61% 156/255 [00:24<00:15,  6.32it/s]\u001b[A\n",
            " 62% 157/255 [00:24<00:14,  6.77it/s]\u001b[A\n",
            " 62% 158/255 [00:24<00:15,  6.33it/s]\u001b[A\n",
            " 63% 160/255 [00:24<00:13,  7.09it/s]\u001b[A\n",
            " 63% 161/255 [00:24<00:12,  7.29it/s]\u001b[A\n",
            " 64% 162/255 [00:25<00:14,  6.49it/s]\u001b[A\n",
            " 64% 163/255 [00:25<00:13,  6.91it/s]\u001b[A\n",
            " 64% 164/255 [00:25<00:12,  7.15it/s]\u001b[A\n",
            " 65% 165/255 [00:25<00:13,  6.81it/s]\u001b[A\n",
            " 65% 166/255 [00:25<00:13,  6.56it/s]\u001b[A\n",
            " 65% 167/255 [00:25<00:15,  5.76it/s]\u001b[A\n",
            " 66% 168/255 [00:26<00:13,  6.36it/s]\u001b[A\n",
            " 66% 169/255 [00:26<00:12,  6.82it/s]\u001b[A\n",
            " 67% 170/255 [00:26<00:12,  6.77it/s]\u001b[A\n",
            " 67% 171/255 [00:26<00:12,  6.63it/s]\u001b[A\n",
            " 67% 172/255 [00:26<00:12,  6.53it/s]\u001b[A\n",
            " 68% 173/255 [00:26<00:12,  6.42it/s]\u001b[A\n",
            " 68% 174/255 [00:27<00:12,  6.36it/s]\u001b[A\n",
            " 69% 175/255 [00:27<00:11,  6.80it/s]\u001b[A\n",
            " 69% 176/255 [00:27<00:11,  6.67it/s]\u001b[A\n",
            " 69% 177/255 [00:27<00:11,  6.54it/s]\u001b[A\n",
            " 70% 178/255 [00:27<00:11,  6.52it/s]\u001b[A\n",
            " 70% 179/255 [00:27<00:11,  6.52it/s]\u001b[A\n",
            " 71% 180/255 [00:27<00:10,  6.98it/s]\u001b[A\n",
            " 71% 181/255 [00:28<00:11,  6.48it/s]\u001b[A\n",
            " 71% 182/255 [00:28<00:11,  6.14it/s]\u001b[A\n",
            " 72% 183/255 [00:28<00:11,  6.01it/s]\u001b[A\n",
            " 72% 184/255 [00:28<00:10,  6.46it/s]\u001b[A\n",
            " 73% 185/255 [00:28<00:09,  7.01it/s]\u001b[A\n",
            " 73% 186/255 [00:28<00:10,  6.74it/s]\u001b[A\n",
            " 73% 187/255 [00:29<00:10,  6.44it/s]\u001b[A\n",
            " 74% 188/255 [00:29<00:09,  7.13it/s]\u001b[A\n",
            " 74% 189/255 [00:29<00:09,  6.90it/s]\u001b[A\n",
            " 75% 190/255 [00:29<00:08,  7.23it/s]\u001b[A\n",
            " 75% 191/255 [00:29<00:08,  7.53it/s]\u001b[A\n",
            " 75% 192/255 [00:29<00:08,  7.27it/s]\u001b[A\n",
            " 76% 193/255 [00:29<00:08,  7.11it/s]\u001b[A\n",
            " 76% 194/255 [00:29<00:08,  6.87it/s]\u001b[A\n",
            " 76% 195/255 [00:30<00:08,  7.39it/s]\u001b[A\n",
            " 77% 196/255 [00:30<00:08,  6.76it/s]\u001b[A\n",
            " 77% 197/255 [00:30<00:08,  6.70it/s]\u001b[A\n",
            " 78% 198/255 [00:30<00:09,  6.32it/s]\u001b[A\n",
            " 78% 199/255 [00:30<00:08,  6.85it/s]\u001b[A\n",
            " 78% 200/255 [00:30<00:07,  7.51it/s]\u001b[A\n",
            " 79% 201/255 [00:30<00:07,  7.15it/s]\u001b[A\n",
            " 79% 202/255 [00:31<00:07,  7.47it/s]\u001b[A\n",
            " 80% 203/255 [00:31<00:06,  7.73it/s]\u001b[A\n",
            " 80% 204/255 [00:31<00:06,  7.33it/s]\u001b[A\n",
            " 80% 205/255 [00:31<00:06,  7.61it/s]\u001b[A\n",
            " 81% 206/255 [00:31<00:06,  7.81it/s]\u001b[A\n",
            " 81% 207/255 [00:31<00:06,  6.99it/s]\u001b[A\n",
            " 82% 208/255 [00:31<00:07,  6.18it/s]\u001b[A\n",
            " 82% 209/255 [00:32<00:06,  6.69it/s]\u001b[A\n",
            " 82% 210/255 [00:32<00:06,  6.98it/s]\u001b[A\n",
            " 83% 211/255 [00:32<00:06,  7.26it/s]\u001b[A\n",
            " 83% 212/255 [00:32<00:06,  6.70it/s]\u001b[A\n",
            " 84% 213/255 [00:32<00:05,  7.14it/s]\u001b[A\n",
            " 84% 214/255 [00:32<00:05,  7.51it/s]\u001b[A\n",
            " 84% 215/255 [00:32<00:05,  7.60it/s]\u001b[A\n",
            " 85% 216/255 [00:33<00:05,  7.05it/s]\u001b[A\n",
            " 85% 217/255 [00:33<00:05,  6.81it/s]\u001b[A\n",
            " 85% 218/255 [00:33<00:05,  7.18it/s]\u001b[A\n",
            " 86% 220/255 [00:33<00:04,  7.97it/s]\u001b[A\n",
            " 87% 221/255 [00:33<00:04,  7.26it/s]\u001b[A\n",
            " 87% 222/255 [00:33<00:04,  7.60it/s]\u001b[A\n",
            " 87% 223/255 [00:34<00:04,  7.13it/s]\u001b[A\n",
            " 88% 224/255 [00:34<00:04,  6.91it/s]\u001b[A\n",
            " 88% 225/255 [00:34<00:04,  6.79it/s]\u001b[A\n",
            " 89% 226/255 [00:34<00:04,  6.79it/s]\u001b[A\n",
            " 89% 227/255 [00:34<00:04,  6.49it/s]\u001b[A\n",
            " 89% 228/255 [00:34<00:03,  7.08it/s]\u001b[A\n",
            " 90% 229/255 [00:34<00:03,  7.67it/s]\u001b[A\n",
            " 90% 230/255 [00:35<00:03,  6.62it/s]\u001b[A\n",
            " 91% 231/255 [00:35<00:03,  7.29it/s]\u001b[A\n",
            " 91% 232/255 [00:35<00:02,  7.84it/s]\u001b[A\n",
            " 91% 233/255 [00:35<00:02,  7.96it/s]\u001b[A\n",
            " 92% 234/255 [00:35<00:02,  7.13it/s]\u001b[A\n",
            " 92% 235/255 [00:35<00:02,  6.88it/s]\u001b[A\n",
            " 93% 236/255 [00:35<00:02,  6.67it/s]\u001b[A\n",
            " 93% 237/255 [00:36<00:02,  6.29it/s]\u001b[A\n",
            " 93% 238/255 [00:36<00:02,  6.75it/s]\u001b[A\n",
            " 94% 239/255 [00:36<00:02,  6.40it/s]\u001b[A\n",
            " 94% 240/255 [00:36<00:02,  7.11it/s]\u001b[A\n",
            " 95% 241/255 [00:36<00:02,  6.99it/s]\u001b[A\n",
            " 95% 242/255 [00:36<00:02,  6.18it/s]\u001b[A\n",
            " 95% 243/255 [00:36<00:01,  6.76it/s]\u001b[A\n",
            " 96% 244/255 [00:37<00:01,  6.60it/s]\u001b[A\n",
            " 96% 245/255 [00:37<00:01,  7.09it/s]\u001b[A\n",
            " 96% 246/255 [00:37<00:01,  7.45it/s]\u001b[A\n",
            " 97% 247/255 [00:37<00:01,  6.87it/s]\u001b[A\n",
            " 97% 248/255 [00:37<00:00,  7.27it/s]\u001b[A\n",
            " 98% 249/255 [00:37<00:00,  7.03it/s]\u001b[A\n",
            " 98% 250/255 [00:37<00:00,  7.41it/s]\u001b[A\n",
            " 98% 251/255 [00:38<00:00,  7.05it/s]\u001b[A\n",
            " 99% 252/255 [00:38<00:00,  7.40it/s]\u001b[A\n",
            " 99% 253/255 [00:38<00:00,  6.49it/s]\u001b[A\n",
            "100% 254/255 [00:38<00:00,  6.47it/s]\u001b[A\n",
            "{'eval_loss': 0.6924327611923218, 'eval_accuracy': 0.5403, 'eval_runtime': 38.6681, 'eval_samples_per_second': 65.713, 'eval_steps_per_second': 6.595, 'epoch': 4.0}\n",
            "\n",
            " 80% 7412/9265 [46:13<10:22,  2.97it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 17:19:16,541 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-7412\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 17:19:16,542 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-7412/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 17:19:21,970 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-7412/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 17:19:21,971 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-7412/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 17:19:21,971 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-7412/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 17:19:33,265 >> Deleting older checkpoint [saved/csqa2/mcq/roberta-large/checkpoint-5559] due to args.save_total_limit\n",
            "{'loss': 0.6979, 'learning_rate': 1.9050188882892607e-07, 'epoch': 4.05}\n",
            "{'loss': 0.6906, 'learning_rate': 1.3653534808418779e-07, 'epoch': 4.32}\n",
            "{'loss': 0.6958, 'learning_rate': 8.256880733944954e-08, 'epoch': 4.59}\n",
            "{'loss': 0.6957, 'learning_rate': 2.8602266594711278e-08, 'epoch': 4.86}\n",
            "100% 9265/9265 [57:13<00:00,  2.87it/s][INFO|trainer.py:722] 2023-01-15 17:30:16,930 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice1, context, choice0. If choice1, context, choice0 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 17:30:16,932 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 17:30:16,932 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-15 17:30:16,933 >>   Batch size = 10\n",
            "\n",
            "  0% 0/255 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/255 [00:00<00:22, 11.30it/s]\u001b[A\n",
            "  2% 4/255 [00:00<00:34,  7.26it/s]\u001b[A\n",
            "  2% 5/255 [00:00<00:38,  6.50it/s]\u001b[A\n",
            "  2% 6/255 [00:00<00:38,  6.47it/s]\u001b[A\n",
            "  3% 7/255 [00:01<00:38,  6.48it/s]\u001b[A\n",
            "  3% 8/255 [00:01<00:38,  6.34it/s]\u001b[A\n",
            "  4% 9/255 [00:01<00:35,  7.00it/s]\u001b[A\n",
            "  4% 10/255 [00:01<00:33,  7.26it/s]\u001b[A\n",
            "  4% 11/255 [00:01<00:36,  6.62it/s]\u001b[A\n",
            "  5% 12/255 [00:01<00:34,  6.97it/s]\u001b[A\n",
            "  5% 13/255 [00:01<00:37,  6.53it/s]\u001b[A\n",
            "  5% 14/255 [00:02<00:38,  6.25it/s]\u001b[A\n",
            "  6% 15/255 [00:02<00:35,  6.79it/s]\u001b[A\n",
            "  6% 16/255 [00:02<00:33,  7.21it/s]\u001b[A\n",
            "  7% 17/255 [00:02<00:31,  7.52it/s]\u001b[A\n",
            "  7% 18/255 [00:02<00:30,  7.77it/s]\u001b[A\n",
            "  7% 19/255 [00:02<00:29,  7.87it/s]\u001b[A\n",
            "  8% 20/255 [00:02<00:31,  7.41it/s]\u001b[A\n",
            "  8% 21/255 [00:02<00:33,  7.08it/s]\u001b[A\n",
            "  9% 22/255 [00:03<00:33,  6.93it/s]\u001b[A\n",
            "  9% 23/255 [00:03<00:34,  6.78it/s]\u001b[A\n",
            "  9% 24/255 [00:03<00:34,  6.65it/s]\u001b[A\n",
            " 10% 25/255 [00:03<00:31,  7.23it/s]\u001b[A\n",
            " 10% 26/255 [00:03<00:29,  7.71it/s]\u001b[A\n",
            " 11% 27/255 [00:03<00:28,  7.92it/s]\u001b[A\n",
            " 11% 28/255 [00:03<00:27,  8.26it/s]\u001b[A\n",
            " 11% 29/255 [00:04<00:27,  8.36it/s]\u001b[A\n",
            " 12% 30/255 [00:04<00:29,  7.73it/s]\u001b[A\n",
            " 12% 31/255 [00:04<00:33,  6.62it/s]\u001b[A\n",
            " 13% 32/255 [00:04<00:37,  5.97it/s]\u001b[A\n",
            " 13% 33/255 [00:04<00:34,  6.49it/s]\u001b[A\n",
            " 13% 34/255 [00:04<00:34,  6.39it/s]\u001b[A\n",
            " 14% 35/255 [00:04<00:32,  6.74it/s]\u001b[A\n",
            " 14% 36/255 [00:05<00:30,  7.17it/s]\u001b[A\n",
            " 15% 37/255 [00:05<00:32,  6.64it/s]\u001b[A\n",
            " 15% 38/255 [00:05<00:33,  6.55it/s]\u001b[A\n",
            " 15% 39/255 [00:05<00:31,  6.96it/s]\u001b[A\n",
            " 16% 40/255 [00:05<00:31,  6.83it/s]\u001b[A\n",
            " 16% 41/255 [00:05<00:33,  6.30it/s]\u001b[A\n",
            " 16% 42/255 [00:06<00:30,  7.02it/s]\u001b[A\n",
            " 17% 43/255 [00:06<00:28,  7.33it/s]\u001b[A\n",
            " 17% 44/255 [00:06<00:31,  6.71it/s]\u001b[A\n",
            " 18% 45/255 [00:06<00:31,  6.59it/s]\u001b[A\n",
            " 18% 46/255 [00:06<00:33,  6.25it/s]\u001b[A\n",
            " 18% 47/255 [00:06<00:30,  6.77it/s]\u001b[A\n",
            " 19% 48/255 [00:06<00:29,  6.99it/s]\u001b[A\n",
            " 19% 49/255 [00:07<00:30,  6.77it/s]\u001b[A\n",
            " 20% 50/255 [00:07<00:27,  7.39it/s]\u001b[A\n",
            " 20% 51/255 [00:07<00:28,  7.24it/s]\u001b[A\n",
            " 20% 52/255 [00:07<00:29,  6.99it/s]\u001b[A\n",
            " 21% 53/255 [00:07<00:26,  7.49it/s]\u001b[A\n",
            " 21% 54/255 [00:07<00:31,  6.48it/s]\u001b[A\n",
            " 22% 55/255 [00:07<00:31,  6.40it/s]\u001b[A\n",
            " 22% 56/255 [00:08<00:31,  6.34it/s]\u001b[A\n",
            " 22% 57/255 [00:08<00:30,  6.41it/s]\u001b[A\n",
            " 23% 58/255 [00:08<00:32,  6.12it/s]\u001b[A\n",
            " 23% 59/255 [00:08<00:28,  6.79it/s]\u001b[A\n",
            " 24% 60/255 [00:08<00:26,  7.28it/s]\u001b[A\n",
            " 24% 61/255 [00:08<00:29,  6.68it/s]\u001b[A\n",
            " 24% 62/255 [00:09<00:29,  6.48it/s]\u001b[A\n",
            " 25% 63/255 [00:09<00:28,  6.75it/s]\u001b[A\n",
            " 25% 64/255 [00:09<00:27,  7.04it/s]\u001b[A\n",
            " 25% 65/255 [00:09<00:28,  6.75it/s]\u001b[A\n",
            " 26% 66/255 [00:09<00:26,  7.12it/s]\u001b[A\n",
            " 26% 67/255 [00:09<00:25,  7.43it/s]\u001b[A\n",
            " 27% 68/255 [00:09<00:30,  6.04it/s]\u001b[A\n",
            " 27% 69/255 [00:10<00:31,  5.85it/s]\u001b[A\n",
            " 27% 70/255 [00:10<00:28,  6.49it/s]\u001b[A\n",
            " 28% 71/255 [00:10<00:26,  6.82it/s]\u001b[A\n",
            " 28% 72/255 [00:10<00:25,  7.25it/s]\u001b[A\n",
            " 29% 73/255 [00:10<00:26,  6.92it/s]\u001b[A\n",
            " 29% 74/255 [00:10<00:24,  7.25it/s]\u001b[A\n",
            " 29% 75/255 [00:10<00:25,  7.01it/s]\u001b[A\n",
            " 30% 76/255 [00:11<00:24,  7.27it/s]\u001b[A\n",
            " 30% 77/255 [00:11<00:26,  6.82it/s]\u001b[A\n",
            " 31% 78/255 [00:11<00:23,  7.42it/s]\u001b[A\n",
            " 31% 79/255 [00:11<00:22,  7.75it/s]\u001b[A\n",
            " 31% 80/255 [00:11<00:22,  7.93it/s]\u001b[A\n",
            " 32% 81/255 [00:11<00:22,  7.84it/s]\u001b[A\n",
            " 32% 82/255 [00:11<00:20,  8.27it/s]\u001b[A\n",
            " 33% 83/255 [00:11<00:24,  6.89it/s]\u001b[A\n",
            " 33% 84/255 [00:12<00:23,  7.25it/s]\u001b[A\n",
            " 33% 85/255 [00:12<00:24,  7.02it/s]\u001b[A\n",
            " 34% 86/255 [00:12<00:22,  7.35it/s]\u001b[A\n",
            " 34% 87/255 [00:12<00:24,  6.86it/s]\u001b[A\n",
            " 35% 88/255 [00:12<00:22,  7.28it/s]\u001b[A\n",
            " 35% 89/255 [00:12<00:21,  7.62it/s]\u001b[A\n",
            " 35% 90/255 [00:12<00:26,  6.21it/s]\u001b[A\n",
            " 36% 91/255 [00:13<00:24,  6.75it/s]\u001b[A\n",
            " 36% 92/255 [00:13<00:24,  6.62it/s]\u001b[A\n",
            " 36% 93/255 [00:13<00:25,  6.31it/s]\u001b[A\n",
            " 37% 94/255 [00:13<00:24,  6.70it/s]\u001b[A\n",
            " 37% 95/255 [00:13<00:22,  6.97it/s]\u001b[A\n",
            " 38% 96/255 [00:13<00:23,  6.76it/s]\u001b[A\n",
            " 38% 97/255 [00:13<00:22,  7.15it/s]\u001b[A\n",
            " 38% 98/255 [00:14<00:22,  6.84it/s]\u001b[A\n",
            " 39% 99/255 [00:14<00:21,  7.12it/s]\u001b[A\n",
            " 39% 100/255 [00:14<00:21,  7.36it/s]\u001b[A\n",
            " 40% 101/255 [00:14<00:20,  7.61it/s]\u001b[A\n",
            " 40% 102/255 [00:14<00:21,  7.18it/s]\u001b[A\n",
            " 40% 103/255 [00:14<00:20,  7.47it/s]\u001b[A\n",
            " 41% 104/255 [00:14<00:20,  7.50it/s]\u001b[A\n",
            " 41% 105/255 [00:15<00:19,  7.76it/s]\u001b[A\n",
            " 42% 106/255 [00:15<00:19,  7.71it/s]\u001b[A\n",
            " 42% 107/255 [00:15<00:19,  7.79it/s]\u001b[A\n",
            " 42% 108/255 [00:15<00:20,  7.18it/s]\u001b[A\n",
            " 43% 109/255 [00:15<00:19,  7.51it/s]\u001b[A\n",
            " 43% 110/255 [00:15<00:18,  8.03it/s]\u001b[A\n",
            " 44% 111/255 [00:15<00:18,  7.61it/s]\u001b[A\n",
            " 44% 112/255 [00:15<00:19,  7.17it/s]\u001b[A\n",
            " 44% 113/255 [00:16<00:19,  7.42it/s]\u001b[A\n",
            " 45% 114/255 [00:16<00:18,  7.71it/s]\u001b[A\n",
            " 45% 115/255 [00:16<00:20,  6.85it/s]\u001b[A\n",
            " 45% 116/255 [00:16<00:21,  6.39it/s]\u001b[A\n",
            " 46% 117/255 [00:16<00:19,  7.05it/s]\u001b[A\n",
            " 46% 118/255 [00:16<00:21,  6.30it/s]\u001b[A\n",
            " 47% 119/255 [00:17<00:21,  6.29it/s]\u001b[A\n",
            " 47% 120/255 [00:18<01:22,  1.63it/s]\u001b[A\n",
            " 47% 121/255 [00:18<01:02,  2.15it/s]\u001b[A\n",
            " 48% 122/255 [00:18<00:48,  2.76it/s]\u001b[A\n",
            " 48% 123/255 [00:19<00:38,  3.45it/s]\u001b[A\n",
            " 49% 124/255 [00:19<00:31,  4.16it/s]\u001b[A\n",
            " 49% 125/255 [00:19<00:26,  4.88it/s]\u001b[A\n",
            " 49% 126/255 [00:19<00:23,  5.47it/s]\u001b[A\n",
            " 50% 127/255 [00:19<00:23,  5.46it/s]\u001b[A\n",
            " 50% 128/255 [00:19<00:23,  5.52it/s]\u001b[A\n",
            " 51% 129/255 [00:19<00:20,  6.18it/s]\u001b[A\n",
            " 51% 130/255 [00:20<00:20,  6.19it/s]\u001b[A\n",
            " 51% 131/255 [00:20<00:18,  6.58it/s]\u001b[A\n",
            " 52% 132/255 [00:20<00:17,  7.02it/s]\u001b[A\n",
            " 52% 133/255 [00:20<00:17,  6.82it/s]\u001b[A\n",
            " 53% 134/255 [00:20<00:18,  6.69it/s]\u001b[A\n",
            " 53% 135/255 [00:20<00:17,  6.97it/s]\u001b[A\n",
            " 53% 136/255 [00:21<00:19,  6.23it/s]\u001b[A\n",
            " 54% 137/255 [00:21<00:21,  5.51it/s]\u001b[A\n",
            " 54% 138/255 [00:21<00:22,  5.31it/s]\u001b[A\n",
            " 55% 139/255 [00:21<00:21,  5.34it/s]\u001b[A\n",
            " 55% 140/255 [00:21<00:20,  5.63it/s]\u001b[A\n",
            " 55% 141/255 [00:21<00:20,  5.64it/s]\u001b[A\n",
            " 56% 142/255 [00:22<00:20,  5.61it/s]\u001b[A\n",
            " 56% 143/255 [00:22<00:19,  5.62it/s]\u001b[A\n",
            " 56% 144/255 [00:22<00:20,  5.38it/s]\u001b[A\n",
            " 57% 145/255 [00:22<00:19,  5.68it/s]\u001b[A\n",
            " 57% 146/255 [00:22<00:17,  6.27it/s]\u001b[A\n",
            " 58% 147/255 [00:22<00:15,  6.77it/s]\u001b[A\n",
            " 58% 148/255 [00:23<00:14,  7.19it/s]\u001b[A\n",
            " 58% 149/255 [00:23<00:15,  6.93it/s]\u001b[A\n",
            " 59% 150/255 [00:23<00:15,  6.69it/s]\u001b[A\n",
            " 59% 151/255 [00:23<00:14,  7.15it/s]\u001b[A\n",
            " 60% 152/255 [00:23<00:15,  6.67it/s]\u001b[A\n",
            " 60% 153/255 [00:23<00:16,  6.27it/s]\u001b[A\n",
            " 60% 154/255 [00:23<00:16,  6.23it/s]\u001b[A\n",
            " 61% 155/255 [00:24<00:16,  6.24it/s]\u001b[A\n",
            " 61% 156/255 [00:24<00:16,  6.14it/s]\u001b[A\n",
            " 62% 157/255 [00:24<00:14,  6.56it/s]\u001b[A\n",
            " 62% 158/255 [00:24<00:15,  6.21it/s]\u001b[A\n",
            " 63% 160/255 [00:24<00:13,  6.96it/s]\u001b[A\n",
            " 63% 161/255 [00:24<00:13,  7.19it/s]\u001b[A\n",
            " 64% 162/255 [00:25<00:14,  6.41it/s]\u001b[A\n",
            " 64% 163/255 [00:25<00:13,  6.79it/s]\u001b[A\n",
            " 64% 164/255 [00:25<00:13,  6.98it/s]\u001b[A\n",
            " 65% 165/255 [00:25<00:13,  6.75it/s]\u001b[A\n",
            " 65% 166/255 [00:25<00:13,  6.58it/s]\u001b[A\n",
            " 65% 167/255 [00:26<00:15,  5.77it/s]\u001b[A\n",
            " 66% 168/255 [00:26<00:13,  6.32it/s]\u001b[A\n",
            " 66% 169/255 [00:26<00:12,  6.77it/s]\u001b[A\n",
            " 67% 170/255 [00:26<00:12,  6.72it/s]\u001b[A\n",
            " 67% 171/255 [00:26<00:12,  6.50it/s]\u001b[A\n",
            " 67% 172/255 [00:26<00:12,  6.41it/s]\u001b[A\n",
            " 68% 173/255 [00:26<00:12,  6.33it/s]\u001b[A\n",
            " 68% 174/255 [00:27<00:12,  6.27it/s]\u001b[A\n",
            " 69% 175/255 [00:27<00:11,  6.67it/s]\u001b[A\n",
            " 69% 176/255 [00:27<00:12,  6.56it/s]\u001b[A\n",
            " 69% 177/255 [00:27<00:12,  6.42it/s]\u001b[A\n",
            " 70% 178/255 [00:27<00:11,  6.46it/s]\u001b[A\n",
            " 70% 179/255 [00:27<00:11,  6.44it/s]\u001b[A\n",
            " 71% 180/255 [00:27<00:10,  6.96it/s]\u001b[A\n",
            " 71% 181/255 [00:28<00:11,  6.43it/s]\u001b[A\n",
            " 71% 182/255 [00:28<00:11,  6.12it/s]\u001b[A\n",
            " 72% 183/255 [00:28<00:12,  5.99it/s]\u001b[A\n",
            " 72% 184/255 [00:28<00:11,  6.39it/s]\u001b[A\n",
            " 73% 185/255 [00:28<00:10,  6.94it/s]\u001b[A\n",
            " 73% 186/255 [00:28<00:10,  6.63it/s]\u001b[A\n",
            " 73% 187/255 [00:29<00:10,  6.34it/s]\u001b[A\n",
            " 74% 188/255 [00:29<00:09,  7.03it/s]\u001b[A\n",
            " 74% 189/255 [00:29<00:09,  6.81it/s]\u001b[A\n",
            " 75% 190/255 [00:29<00:09,  7.10it/s]\u001b[A\n",
            " 75% 191/255 [00:29<00:08,  7.38it/s]\u001b[A\n",
            " 75% 192/255 [00:29<00:08,  7.11it/s]\u001b[A\n",
            " 76% 193/255 [00:29<00:08,  6.91it/s]\u001b[A\n",
            " 76% 194/255 [00:30<00:09,  6.70it/s]\u001b[A\n",
            " 76% 195/255 [00:30<00:08,  7.28it/s]\u001b[A\n",
            " 77% 196/255 [00:30<00:08,  6.67it/s]\u001b[A\n",
            " 77% 197/255 [00:30<00:08,  6.64it/s]\u001b[A\n",
            " 78% 198/255 [00:30<00:09,  6.23it/s]\u001b[A\n",
            " 78% 199/255 [00:30<00:08,  6.71it/s]\u001b[A\n",
            " 78% 200/255 [00:30<00:07,  7.31it/s]\u001b[A\n",
            " 79% 201/255 [00:31<00:07,  6.99it/s]\u001b[A\n",
            " 79% 202/255 [00:31<00:07,  7.36it/s]\u001b[A\n",
            " 80% 203/255 [00:31<00:06,  7.64it/s]\u001b[A\n",
            " 80% 204/255 [00:31<00:07,  7.19it/s]\u001b[A\n",
            " 80% 205/255 [00:31<00:06,  7.46it/s]\u001b[A\n",
            " 81% 206/255 [00:31<00:06,  7.67it/s]\u001b[A\n",
            " 81% 207/255 [00:31<00:07,  6.84it/s]\u001b[A\n",
            " 82% 208/255 [00:32<00:07,  6.07it/s]\u001b[A\n",
            " 82% 209/255 [00:32<00:06,  6.61it/s]\u001b[A\n",
            " 82% 210/255 [00:32<00:06,  6.95it/s]\u001b[A\n",
            " 83% 211/255 [00:32<00:06,  7.28it/s]\u001b[A\n",
            " 83% 212/255 [00:32<00:06,  6.71it/s]\u001b[A\n",
            " 84% 213/255 [00:32<00:05,  7.12it/s]\u001b[A\n",
            " 84% 214/255 [00:32<00:05,  7.45it/s]\u001b[A\n",
            " 84% 215/255 [00:32<00:05,  7.48it/s]\u001b[A\n",
            " 85% 216/255 [00:33<00:05,  6.97it/s]\u001b[A\n",
            " 85% 217/255 [00:33<00:05,  6.78it/s]\u001b[A\n",
            " 85% 218/255 [00:33<00:05,  7.20it/s]\u001b[A\n",
            " 86% 220/255 [00:33<00:04,  8.00it/s]\u001b[A\n",
            " 87% 221/255 [00:33<00:04,  7.22it/s]\u001b[A\n",
            " 87% 222/255 [00:33<00:04,  7.47it/s]\u001b[A\n",
            " 87% 223/255 [00:34<00:04,  6.99it/s]\u001b[A\n",
            " 88% 224/255 [00:34<00:04,  6.82it/s]\u001b[A\n",
            " 88% 225/255 [00:34<00:04,  6.67it/s]\u001b[A\n",
            " 89% 226/255 [00:34<00:04,  6.72it/s]\u001b[A\n",
            " 89% 227/255 [00:34<00:04,  6.39it/s]\u001b[A\n",
            " 89% 228/255 [00:34<00:03,  7.03it/s]\u001b[A\n",
            " 90% 229/255 [00:34<00:03,  7.51it/s]\u001b[A\n",
            " 90% 230/255 [00:35<00:03,  6.49it/s]\u001b[A\n",
            " 91% 231/255 [00:35<00:03,  7.17it/s]\u001b[A\n",
            " 91% 232/255 [00:35<00:02,  7.70it/s]\u001b[A\n",
            " 91% 233/255 [00:35<00:02,  7.83it/s]\u001b[A\n",
            " 92% 234/255 [00:35<00:03,  6.99it/s]\u001b[A\n",
            " 92% 235/255 [00:35<00:02,  6.80it/s]\u001b[A\n",
            " 93% 236/255 [00:36<00:02,  6.61it/s]\u001b[A\n",
            " 93% 237/255 [00:36<00:02,  6.24it/s]\u001b[A\n",
            " 93% 238/255 [00:36<00:02,  6.71it/s]\u001b[A\n",
            " 94% 239/255 [00:36<00:02,  6.43it/s]\u001b[A\n",
            " 94% 240/255 [00:36<00:02,  7.05it/s]\u001b[A\n",
            " 95% 241/255 [00:36<00:02,  6.90it/s]\u001b[A\n",
            " 95% 242/255 [00:36<00:02,  6.11it/s]\u001b[A\n",
            " 95% 243/255 [00:37<00:01,  6.65it/s]\u001b[A\n",
            " 96% 244/255 [00:37<00:01,  6.45it/s]\u001b[A\n",
            " 96% 245/255 [00:37<00:01,  6.92it/s]\u001b[A\n",
            " 96% 246/255 [00:37<00:01,  7.32it/s]\u001b[A\n",
            " 97% 247/255 [00:37<00:01,  6.69it/s]\u001b[A\n",
            " 97% 248/255 [00:37<00:00,  7.14it/s]\u001b[A\n",
            " 98% 249/255 [00:37<00:00,  6.93it/s]\u001b[A\n",
            " 98% 250/255 [00:38<00:00,  7.30it/s]\u001b[A\n",
            " 98% 251/255 [00:38<00:00,  6.93it/s]\u001b[A\n",
            " 99% 252/255 [00:38<00:00,  7.24it/s]\u001b[A\n",
            " 99% 253/255 [00:38<00:00,  6.40it/s]\u001b[A\n",
            "100% 254/255 [00:38<00:00,  6.31it/s]\u001b[A\n",
            "{'eval_loss': 0.6929572820663452, 'eval_accuracy': 0.5333, 'eval_runtime': 38.8593, 'eval_samples_per_second': 65.39, 'eval_steps_per_second': 6.562, 'epoch': 5.0}\n",
            "\n",
            "100% 9265/9265 [57:52<00:00,  2.87it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:2640] 2023-01-15 17:30:55,794 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large/checkpoint-9265\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 17:30:55,795 >> Configuration saved in saved/csqa2/mcq/roberta-large/checkpoint-9265/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 17:31:01,219 >> Model weights saved in saved/csqa2/mcq/roberta-large/checkpoint-9265/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 17:31:01,220 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/checkpoint-9265/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 17:31:01,221 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/checkpoint-9265/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 17:31:12,563 >> Deleting older checkpoint [saved/csqa2/mcq/roberta-large/checkpoint-7412] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2023-01-15 17:31:12,633 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3491.581, 'train_samples_per_second': 13.266, 'train_steps_per_second': 2.654, 'train_loss': 0.6961203478891915, 'epoch': 5.0}\n",
            "100% 9265/9265 [58:09<00:00,  2.66it/s]\n",
            "[INFO|trainer.py:2640] 2023-01-15 17:31:12,638 >> Saving model checkpoint to saved/csqa2/mcq/roberta-large\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 17:31:12,639 >> Configuration saved in saved/csqa2/mcq/roberta-large/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 17:31:18,244 >> Model weights saved in saved/csqa2/mcq/roberta-large/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 17:31:18,245 >> tokenizer config file saved in saved/csqa2/mcq/roberta-large/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 17:31:18,245 >> Special tokens file saved in saved/csqa2/mcq/roberta-large/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.6961\n",
            "  train_runtime            = 0:58:11.58\n",
            "  train_samples            =       9264\n",
            "  train_samples_per_second =     13.266\n",
            "  train_steps_per_second   =      2.654\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-15 17:31:18,367 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice1, context, choice0. If choice1, context, choice0 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 17:31:18,369 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 17:31:18,370 >>   Num examples = 2541\n",
            "[INFO|trainer.py:2896] 2023-01-15 17:31:18,370 >>   Batch size = 10\n",
            "100% 255/255 [00:38<00:00,  6.70it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.5333\n",
            "  eval_loss               =      0.693\n",
            "  eval_runtime            = 0:00:38.19\n",
            "  eval_samples            =       2541\n",
            "  eval_samples_per_second =     66.531\n",
            "  eval_steps_per_second   =      6.677\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:722] 2023-01-15 17:31:56,566 >> The following columns in the test set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice1, context, choice0. If choice1, context, choice0 are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 17:31:56,569 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 17:31:56,569 >>   Num examples = 2473\n",
            "[INFO|trainer.py:2896] 2023-01-15 17:31:56,569 >>   Batch size = 10\n",
            "100% 248/248 [00:35<00:00,  6.93it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▅▁█▇▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ▇█▁▃▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▇▅▇▆█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▂▄▂▃▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▂▄▂▃▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▇▄▅█▅▅▆▆▅▄▄▅▃▅▆▁▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.5333\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.69296\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 38.1928\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 66.531\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 6.677\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 9265\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.6957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4927592276384328.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.69612\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 3491.581\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 13.266\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMCQA CSQA2 ROBERTA\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/3c45qx8d\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230115_163302-3c45qx8d/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run_mcqa_score.py --learning_rate=1e-6 --num_train_epochs 5 --seed 42 \\\n",
        "--train_file=\"data/csqa2/mcq_train.json\" --validation_file=\"data/csqa2/mcq_dev.json\" --test_file=\"data/csqa2/mcq_test.json\" \\\n",
        "--output_dir=\"saved/csqa2/mcq/roberta-large\" --model_name_or_path=\"roberta-large\" \\\n",
        "--per_device_train_batch_size=5 --per_device_eval_batch_size=10 --weight_decay=0.005 \\\n",
        "--do_train True --do_eval True --do_predict True --evaluation_strategy=\"epoch\" --save_strategy=\"epoch\" \\\n",
        "--report_to \"wandb\" --run_name \"MCQA CSQA2 ROBERTA\" --save_total_limit=1 --overwrite_output_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TEAM - DeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgY54f7I43aQ",
        "outputId": "abae835f-17b7-4f60-9781-09e73f344563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=4, epochs=5, eval_bs=4, input_format='0', lr=3e-06, name='microsoft/deberta-v3-large', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "\rDownloading tokenizer_config.json:   0% 0.00/52.0 [00:00<?, ?B/s]\rDownloading tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 80.0kB/s]\n",
            "Downloading config.json: 100% 580/580 [00:00<00:00, 1.05MB/s]\n",
            "Downloading spm.model: 100% 2.35M/2.35M [00:00<00:00, 75.5MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading pytorch_model.bin: 100% 833M/833M [00:09<00:00, 88.0MB/s]\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230114_105326-17f1xp5b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrevived-brook-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA2-deberta-v3-large\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA2-deberta-v3-large/runs/17f1xp5b\u001b[0m\n",
            "Test preds frequency: {'yes': 1701, 'no': 772}\n",
            "Epoch 1: Loss: Train 0.6641; Val 0.6953\n",
            "Classification Acc: Train 0.5604; Val 0.6257\n",
            "Instance Acc: Val 0.6305\n",
            "Test preds frequency: {'yes': 1329, 'no': 1144}\n",
            "Epoch 2: Loss: Train 0.3874; Val 0.9017\n",
            "Classification Acc: Train 0.8353; Val 0.6702\n",
            "Instance Acc: Val 0.6749\n",
            "Test preds frequency: {'yes': 1302, 'no': 1171}\n",
            "Epoch 3: Loss: Train 0.1562; Val 1.1955\n",
            "Classification Acc: Train 0.947; Val 0.6688\n",
            "Instance Acc: Val 0.6671\n",
            "Test preds frequency: {'no': 1281, 'yes': 1192}\n",
            "Epoch 4: Loss: Train 0.0748; Val 1.4857\n",
            "Classification Acc: Train 0.9763; Val 0.669\n",
            "Instance Acc: Val 0.6686\n",
            "Test preds frequency: {'yes': 1327, 'no': 1146}\n",
            "Epoch 5: Loss: Train 0.0466; Val 1.459\n",
            "Classification Acc: Train 0.9865; Val 0.671\n",
            "Instance Acc: Val 0.669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▆▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▅▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁█▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss ▁▃▅██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.9865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.0466\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.671\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 1.459\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrevived-brook-3\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/CSQA2-deberta-v3-large/runs/17f1xp5b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230114_105326-17f1xp5b/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_csqa2.py --name \"microsoft/deberta-v3-large\" --epochs 5 --lr 3e-6 --bs 4 --eval-bs 4 --shuffle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### QASC"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TEAM - RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbS4s3e2PZJI",
        "outputId": "04ca8604-16cb-4814-ba3a-32e9b169b8a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=8, epochs=5, eval_bs=8, input_format='1', lr=3e-06, name='roberta-large', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Downloading config.json: 100% 482/482 [00:00<00:00, 622kB/s]\n",
            "Downloading vocab.json: 100% 878k/878k [00:00<00:00, 7.63MB/s]\n",
            "Downloading merges.txt: 100% 446k/446k [00:00<00:00, 4.68MB/s]\n",
            "Downloading tokenizer.json: 100% 1.29M/1.29M [00:00<00:00, 9.78MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.33G/1.33G [00:16<00:00, 86.0MB/s]\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "Test preds frequency: {'A': 126, 'F': 124, 'E': 118, 'H': 112, 'D': 112, 'B': 111, 'G': 110, 'C': 107}\n",
            "Epoch 1: Loss: Train 0.3828; Val 0.3756\n",
            "Classification Acc: Train 0.875; Val 0.875\n",
            "Classification Macro F1: Train 0.4668; Val 0.4667\n",
            "Instance Acc: Val 0.1587\n",
            "Test preds frequency: {'F': 134, 'G': 126, 'A': 123, 'C': 116, 'D': 114, 'H': 112, 'E': 109, 'B': 86}\n",
            "Epoch 2: Loss: Train 0.3619; Val 0.3325\n",
            "Classification Acc: Train 0.875; Val 0.8737\n",
            "Classification Macro F1: Train 0.4765; Val 0.5381\n",
            "Instance Acc: Val 0.4546\n",
            "Test preds frequency: {'F': 126, 'C': 123, 'A': 117, 'H': 117, 'D': 116, 'E': 111, 'G': 106, 'B': 104}\n",
            "Epoch 3: Loss: Train 0.3098; Val 0.3437\n",
            "Classification Acc: Train 0.8823; Val 0.8464\n",
            "Classification Macro F1: Train 0.603; Val 0.6342\n",
            "Instance Acc: Val 0.4752\n",
            "Test preds frequency: {'H': 127, 'A': 123, 'F': 123, 'G': 113, 'C': 112, 'D': 110, 'B': 106, 'E': 106}\n",
            "Epoch 4: Loss: Train 0.2574; Val 0.3966\n",
            "Classification Acc: Train 0.9022; Val 0.8359\n",
            "Classification Macro F1: Train 0.7192; Val 0.6518\n",
            "Instance Acc: Val 0.4654\n",
            "Test preds frequency: {'A': 132, 'H': 123, 'F': 120, 'C': 119, 'G': 113, 'D': 112, 'B': 101, 'E': 100}\n",
            "Epoch 5: Loss: Train 0.2138; Val 0.385\n",
            "Classification Acc: Train 0.9187; Val 0.8371\n",
            "Classification Macro F1: Train 0.7828; Val 0.6705\n",
            "Instance Acc: Val 0.4752\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▁▂▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▇▅▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ██▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss ▆▁▂█▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.9187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.2138\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8371\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.4752\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/TEAM/wandb/offline-run-20230112_203518-2hoqa6ax\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20230112_203518-2hoqa6ax/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_qasc.py --name \"roberta-large\" --epochs 5 --lr 3e-6 --shuffle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TEAM - DeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmfzMn8qJHy9",
        "outputId": "86777def-2d2b-4cf7-a9a8-596771096fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, bs=8, epochs=5, eval_bs=8, input_format='1', lr=3e-06, name='microsoft/deberta-v3-base', shuffle=True, warm_up_steps=0, wd=0.0)\n",
            "Downloading tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 47.1kB/s]\n",
            "Downloading config.json: 100% 579/579 [00:00<00:00, 534kB/s]\n",
            "Downloading spm.model: 100% 2.35M/2.35M [00:01<00:00, 1.88MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading pytorch_model.bin: 100% 354M/354M [00:04<00:00, 81.9MB/s]\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias']\n",
            "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalyvasman\u001b[0m (\u001b[33mnlpteam_gr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230115_160433-1rahq0f8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeach-glitter-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/QASC-deberta-v3-base\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/QASC-deberta-v3-base/runs/1rahq0f8\u001b[0m\n",
            "Test preds frequency: {'D': 128, 'C': 126, 'A': 119, 'F': 117, 'H': 111, 'G': 111, 'E': 105, 'B': 103}\n",
            "Epoch 1: Loss: Train 0.347; Val 0.2889\n",
            "Classification Acc: Train 0.8741; Val 0.8867\n",
            "Classification Macro F1: Train 0.4792; Val 0.5792\n",
            "Instance Acc: Val 0.6069\n",
            "Test preds frequency: {'F': 132, 'C': 123, 'A': 118, 'G': 117, 'B': 110, 'E': 108, 'H': 107, 'D': 105}\n",
            "Epoch 2: Loss: Train 0.288; Val 0.273\n",
            "Classification Acc: Train 0.8903; Val 0.8921\n",
            "Classification Macro F1: Train 0.6592; Val 0.6955\n",
            "Instance Acc: Val 0.5907\n",
            "Test preds frequency: {'F': 128, 'C': 122, 'H': 121, 'A': 120, 'D': 110, 'E': 109, 'B': 107, 'G': 103}\n",
            "Epoch 3: Loss: Train 0.2391; Val 0.3019\n",
            "Classification Acc: Train 0.9083; Val 0.8699\n",
            "Classification Macro F1: Train 0.7485; Val 0.7009\n",
            "Instance Acc: Val 0.5508\n",
            "Test preds frequency: {'F': 132, 'C': 123, 'A': 123, 'G': 122, 'B': 107, 'H': 106, 'E': 106, 'D': 101}\n",
            "Epoch 4: Loss: Train 0.2008; Val 0.2964\n",
            "Classification Acc: Train 0.9235; Val 0.8838\n",
            "Classification Macro F1: Train 0.8015; Val 0.7126\n",
            "Instance Acc: Val 0.5713\n",
            "Test preds frequency: {'F': 133, 'C': 122, 'H': 119, 'A': 119, 'G': 114, 'E': 107, 'B': 105, 'D': 101}\n",
            "Epoch 5: Loss: Train 0.166; Val 0.3688\n",
            "Classification Acc: Train 0.9374; Val 0.8506\n",
            "Classification Macro F1: Train 0.8437; Val 0.7038\n",
            "Instance Acc: Val 0.5594\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▃▅▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▆▄▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▇█▄▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy █▆▁▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss ▂▁▃▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.9374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.166\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.5594\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.3688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mpeach-glitter-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/QASC-deberta-v3-base/runs/1rahq0f8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230115_160433-1rahq0f8/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_qasc.py --name \"microsoft/deberta-v3-base\" --epochs 5 --lr 3e-6 --shuffle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### SCORE - RoBEERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSmwWRsi__VH",
        "outputId": "0f39521b-1354-4a14-827c-bf5583253b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saved/qasc/mcq/roberta-large/runs/Jan15_15-17-39_f5ec42a1e8f2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=saved/qasc/mcq/roberta-large,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=10,\n",
            "per_device_train_batch_size=5,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=MCQA qasc ROBERTA,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.005,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-ee3bb49e6900120e\n",
            "INFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 12839.71it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1187.74it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 750.90it/s]\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:41,859 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8gzjrlhi\n",
            "Downloading config.json: 100% 482/482 [00:00<00:00, 399kB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 15:17:42,778 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|hub.py:621] 2023-01-15 15:17:42,778 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:17:42,778 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:17:42,780 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:404] 2023-01-15 15:17:43,693 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:17:44,607 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:17:44,607 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:46,458 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4mj7ttw2\n",
            "Downloading vocab.json: 100% 878k/878k [00:01<00:00, 670kB/s] \n",
            "[INFO|hub.py:613] 2023-01-15 15:17:48,731 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|hub.py:621] 2023-01-15 15:17:48,732 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:49,647 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgp7oxj_g\n",
            "Downloading merges.txt: 100% 446k/446k [00:01<00:00, 406kB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 15:17:51,692 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:621] 2023-01-15 15:17:51,692 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:52,607 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptgioprmk\n",
            "Downloading tokenizer.json: 100% 1.29M/1.29M [00:01<00:00, 1.00MB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 15:17:54,906 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|hub.py:621] 2023-01-15 15:17:54,906 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-15 15:17:57,648 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:681] 2023-01-15 15:17:58,560 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:730] 2023-01-15 15:17:58,561 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|hub.py:600] 2023-01-15 15:17:59,559 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp81x2hinf\n",
            "Downloading pytorch_model.bin: 100% 1.33G/1.33G [00:15<00:00, 90.3MB/s]\n",
            "[INFO|hub.py:613] 2023-01-15 15:18:15,375 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[INFO|hub.py:621] 2023-01-15 15:18:15,375 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[INFO|modeling_utils.py:2041] 2023-01-15 15:18:15,375 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "[WARNING|modeling_utils.py:2425] 2023-01-15 15:18:19,329 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2437] 2023-01-15 15:18:19,329 >> Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0% 0/9 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-ff0c7627b8efec73.arrow\n",
            "100% 9/9 [00:03<00:00,  2.69ba/s]\n",
            "  0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-914372885e09b527.arrow\n",
            "100% 1/1 [00:00<00:00,  2.10ba/s]\n",
            "  0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-ee3bb49e6900120e/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-14209fe1256c3792.arrow\n",
            "100% 1/1 [00:00<00:00,  3.46ba/s]\n",
            "Epoch count 0\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:722] 2023-01-15 15:18:27,964 >> The following columns in the training set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-15 15:18:27,985 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-15 15:18:27,985 >>   Num examples = 8134\n",
            "[INFO|trainer.py:1607] 2023-01-15 15:18:27,985 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2023-01-15 15:18:27,985 >>   Instantaneous batch size per device = 5\n",
            "[INFO|trainer.py:1609] 2023-01-15 15:18:27,985 >>   Total train batch size (w. parallel, distributed & accumulation) = 5\n",
            "[INFO|trainer.py:1610] 2023-01-15 15:18:27,985 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-15 15:18:27,985 >>   Total optimization steps = 8135\n",
            "[INFO|integrations.py:607] 2023-01-15 15:18:27,987 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalyvasman\u001b[0m (\u001b[33mnlpteam_gr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/TEAM/wandb/run-20230115_151829-1lvrhmk8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMCQA qasc ROBERTA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/1lvrhmk8\u001b[0m\n",
            "{'loss': 2.0986, 'learning_rate': 2.8156115550092194e-06, 'epoch': 0.31}\n",
            "{'loss': 2.0588, 'learning_rate': 2.631223110018439e-06, 'epoch': 0.61}\n",
            "{'loss': 1.9575, 'learning_rate': 2.4468346650276585e-06, 'epoch': 0.92}\n",
            " 20% 1627/8135 [17:22<1:03:23,  1.71it/s][INFO|trainer.py:722] 2023-01-15 15:35:53,509 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 15:35:53,512 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 15:35:53,513 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 15:35:53,513 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.37it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.06it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.09it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:24,  3.63it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.29it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.31it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.36it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.26it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.09it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.89it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.07it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.97it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.85it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:29,  2.69it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.80it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.90it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.98it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.02it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:23,  3.05it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.08it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.10it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.12it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.09it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.11it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.00it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.86it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.01it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.04it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.08it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.11it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.18it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:18,  3.21it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:18,  3.15it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.22it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.17it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.13it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.13it/s]\u001b[A\n",
            " 44% 41/93 [00:13<00:16,  3.15it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.15it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.16it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.15it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.15it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.27it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.45it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.36it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.10it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.13it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.02it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.86it/s]\u001b[A\n",
            " 57% 53/93 [00:17<00:14,  2.69it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.82it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.90it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.02it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:13,  2.76it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.84it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.73it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.61it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.59it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.77it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.85it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.91it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.98it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.01it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.10it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.15it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.01it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.85it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.91it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.79it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.06it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.09it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  2.99it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.02it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.86it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.05it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.13it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.13it/s]\u001b[A\n",
            " 89% 83/93 [00:27<00:03,  3.14it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.89it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.94it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  2.98it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.78it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.72it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.74it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.83it/s]\u001b[A\n",
            " 98% 91/93 [00:30<00:00,  2.74it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.83it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.22it/s]\u001b[A\n",
            "{'eval_loss': 1.6697083711624146, 'eval_accuracy': 0.3963, 'eval_runtime': 30.9179, 'eval_samples_per_second': 29.95, 'eval_steps_per_second': 3.008, 'epoch': 1.0}\n",
            "\n",
            " 20% 1627/8135 [17:53<1:03:23,  1.71it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 15:36:24,433 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-1627\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 15:36:24,435 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-1627/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 15:36:29,696 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-1627/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 15:36:29,697 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-1627/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 15:36:29,698 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-1627/special_tokens_map.json\n",
            "{'loss': 1.8461, 'learning_rate': 2.262446220036878e-06, 'epoch': 1.23}\n",
            "{'loss': 1.761, 'learning_rate': 2.078057775046097e-06, 'epoch': 1.54}\n",
            "{'loss': 1.6944, 'learning_rate': 1.8936693300553166e-06, 'epoch': 1.84}\n",
            " 40% 3254/8135 [35:41<48:36,  1.67it/s][INFO|trainer.py:722] 2023-01-15 15:54:11,982 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 15:54:11,985 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 15:54:11,985 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 15:54:11,985 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.29it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.07it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.11it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:24,  3.65it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.30it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.33it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.38it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.27it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.10it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.90it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.08it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.98it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.86it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:28,  2.70it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.80it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.89it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.98it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.02it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:24,  3.04it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.07it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.09it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.12it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.02it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.10it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.12it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.01it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.86it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.04it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.06it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.10it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.12it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.21it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:17,  3.17it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.18it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.14it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.14it/s]\u001b[A\n",
            " 44% 41/93 [00:13<00:16,  3.15it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.15it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.17it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.15it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.15it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.28it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.45it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.37it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.11it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.14it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.02it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.86it/s]\u001b[A\n",
            " 57% 53/93 [00:16<00:14,  2.70it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.83it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.89it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.01it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:12,  2.78it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.85it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.75it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.63it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.60it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.78it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.87it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.92it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.99it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.02it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.12it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.17it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.15it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.02it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.85it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.92it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.81it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.07it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.10it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  3.01it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.04it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.87it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.06it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.14it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.15it/s]\u001b[A\n",
            " 89% 83/93 [00:27<00:03,  3.17it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.91it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.95it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  3.00it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.80it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.74it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.75it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.85it/s]\u001b[A\n",
            " 98% 91/93 [00:29<00:00,  2.75it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.84it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.23it/s]\u001b[A\n",
            "{'eval_loss': 1.4767152070999146, 'eval_accuracy': 0.4838, 'eval_runtime': 30.8273, 'eval_samples_per_second': 30.038, 'eval_steps_per_second': 3.017, 'epoch': 2.0}\n",
            "\n",
            " 40% 3254/8135 [36:12<48:36,  1.67it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 15:54:42,815 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-3254\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 15:54:42,816 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-3254/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 15:54:48,109 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-3254/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 15:54:48,110 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-3254/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 15:54:48,111 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-3254/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 15:54:59,472 >> Deleting older checkpoint [saved/qasc/mcq/roberta-large/checkpoint-1627] due to args.save_total_limit\n",
            "{'loss': 1.5568, 'learning_rate': 1.7092808850645358e-06, 'epoch': 2.15}\n",
            "{'loss': 1.4548, 'learning_rate': 1.5248924400737553e-06, 'epoch': 2.46}\n",
            "{'loss': 1.459, 'learning_rate': 1.3405039950829748e-06, 'epoch': 2.77}\n",
            " 60% 4881/8135 [53:59<32:41,  1.66it/s][INFO|trainer.py:722] 2023-01-15 16:12:30,437 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:12:30,440 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:12:30,440 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:12:30,440 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.35it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.06it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.10it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:23,  3.69it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.32it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.33it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.39it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.29it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.11it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.92it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.09it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.99it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.87it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:28,  2.71it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.82it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.91it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.98it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.03it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:23,  3.05it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.07it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.10it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.13it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.02it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.03it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.11it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.14it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.02it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.86it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.03it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.05it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.09it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.11it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.19it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:17,  3.17it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.19it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.16it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.16it/s]\u001b[A\n",
            " 44% 41/93 [00:12<00:16,  3.18it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.18it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.18it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.16it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.15it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.28it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.45it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.38it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.11it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.13it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.02it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.86it/s]\u001b[A\n",
            " 57% 53/93 [00:16<00:14,  2.70it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.83it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.91it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.02it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:12,  2.77it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.86it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.75it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.62it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.60it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.78it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.86it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.93it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.99it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.03it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.12it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.16it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.02it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.85it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.92it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.80it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.06it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.10it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  3.00it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.01it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.85it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.04it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.11it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.12it/s]\u001b[A\n",
            " 89% 83/93 [00:27<00:03,  3.14it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.89it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.94it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  2.98it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.77it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.72it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.74it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.83it/s]\u001b[A\n",
            " 98% 91/93 [00:29<00:00,  2.74it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.83it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.21it/s]\u001b[A\n",
            "{'eval_loss': 1.4598098993301392, 'eval_accuracy': 0.4914, 'eval_runtime': 30.8214, 'eval_samples_per_second': 30.044, 'eval_steps_per_second': 3.017, 'epoch': 3.0}\n",
            "\n",
            " 60% 4881/8135 [54:30<32:41,  1.66it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:13:01,267 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-4881\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:13:01,269 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-4881/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:13:06,573 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-4881/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:13:06,574 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-4881/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:13:06,574 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-4881/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:13:17,905 >> Deleting older checkpoint [saved/qasc/mcq/roberta-large/checkpoint-3254] due to args.save_total_limit\n",
            "{'loss': 1.38, 'learning_rate': 1.1561155500921944e-06, 'epoch': 3.07}\n",
            "{'loss': 1.3165, 'learning_rate': 9.717271051014137e-07, 'epoch': 3.38}\n",
            "{'loss': 1.3006, 'learning_rate': 7.87338660110633e-07, 'epoch': 3.69}\n",
            "{'loss': 1.288, 'learning_rate': 6.029502151198525e-07, 'epoch': 4.0}\n",
            " 80% 6508/8135 [1:12:19<16:55,  1.60it/s][INFO|trainer.py:722] 2023-01-15 16:30:49,972 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:30:49,975 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:30:49,975 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:30:49,975 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.39it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.02it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.09it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:24,  3.66it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.29it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.32it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.38it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.27it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.10it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.91it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.08it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.98it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.86it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:28,  2.70it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.80it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.90it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.99it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.02it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:24,  3.04it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.07it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.10it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.12it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.02it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.10it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.11it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.01it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.86it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.02it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.05it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.09it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.12it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.20it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:18,  3.22it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:18,  3.16it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.19it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.16it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.16it/s]\u001b[A\n",
            " 44% 41/93 [00:13<00:16,  3.17it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.17it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.18it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.16it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.15it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.28it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.45it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.36it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.11it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.12it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.02it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.87it/s]\u001b[A\n",
            " 57% 53/93 [00:16<00:14,  2.70it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.83it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.90it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.02it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:13,  2.76it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.86it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.76it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.63it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.61it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.78it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.86it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.93it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.99it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.02it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.13it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.17it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.15it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.02it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.85it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.93it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.80it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.07it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.11it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  3.01it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.04it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.88it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.06it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.13it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.14it/s]\u001b[A\n",
            " 89% 83/93 [00:27<00:03,  3.16it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.91it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.95it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  2.99it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.79it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.73it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.75it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.85it/s]\u001b[A\n",
            " 98% 91/93 [00:29<00:00,  2.75it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.84it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.23it/s]\u001b[A\n",
            "{'eval_loss': 1.5148590803146362, 'eval_accuracy': 0.4946, 'eval_runtime': 30.8167, 'eval_samples_per_second': 30.049, 'eval_steps_per_second': 3.018, 'epoch': 4.0}\n",
            "\n",
            " 80% 6508/8135 [1:12:50<16:55,  1.60it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:31:20,798 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-6508\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:31:20,800 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-6508/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:31:26,093 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-6508/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:31:26,094 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-6508/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:31:26,094 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-6508/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:31:37,495 >> Deleting older checkpoint [saved/qasc/mcq/roberta-large/checkpoint-4881] due to args.save_total_limit\n",
            "{'loss': 1.1974, 'learning_rate': 4.185617701290719e-07, 'epoch': 4.3}\n",
            "{'loss': 1.1527, 'learning_rate': 2.3417332513829134e-07, 'epoch': 4.61}\n",
            "{'loss': 1.1758, 'learning_rate': 4.978488014751076e-08, 'epoch': 4.92}\n",
            "100% 8135/8135 [1:30:36<00:00,  1.66it/s][INFO|trainer.py:722] 2023-01-15 16:49:07,333 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:49:07,337 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:49:07,337 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:49:07,337 >>   Batch size = 10\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/93 [00:00<00:09,  9.69it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:17,  5.10it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:21,  4.12it/s]\u001b[A\n",
            "  5% 5/93 [00:01<00:23,  3.69it/s]\u001b[A\n",
            "  6% 6/93 [00:01<00:26,  3.32it/s]\u001b[A\n",
            "  8% 7/93 [00:01<00:25,  3.33it/s]\u001b[A\n",
            "  9% 8/93 [00:02<00:25,  3.39it/s]\u001b[A\n",
            " 10% 9/93 [00:02<00:25,  3.28it/s]\u001b[A\n",
            " 11% 10/93 [00:02<00:26,  3.09it/s]\u001b[A\n",
            " 12% 11/93 [00:03<00:28,  2.91it/s]\u001b[A\n",
            " 13% 12/93 [00:03<00:26,  3.08it/s]\u001b[A\n",
            " 14% 13/93 [00:03<00:26,  2.97it/s]\u001b[A\n",
            " 15% 14/93 [00:04<00:27,  2.86it/s]\u001b[A\n",
            " 16% 15/93 [00:04<00:28,  2.70it/s]\u001b[A\n",
            " 17% 16/93 [00:04<00:27,  2.81it/s]\u001b[A\n",
            " 18% 17/93 [00:05<00:26,  2.91it/s]\u001b[A\n",
            " 19% 18/93 [00:05<00:25,  2.99it/s]\u001b[A\n",
            " 20% 19/93 [00:05<00:24,  3.03it/s]\u001b[A\n",
            " 22% 20/93 [00:06<00:23,  3.06it/s]\u001b[A\n",
            " 23% 21/93 [00:06<00:23,  3.08it/s]\u001b[A\n",
            " 24% 22/93 [00:06<00:22,  3.11it/s]\u001b[A\n",
            " 25% 23/93 [00:07<00:22,  3.13it/s]\u001b[A\n",
            " 26% 24/93 [00:07<00:22,  3.01it/s]\u001b[A\n",
            " 27% 25/93 [00:07<00:22,  3.03it/s]\u001b[A\n",
            " 28% 26/93 [00:08<00:21,  3.12it/s]\u001b[A\n",
            " 29% 27/93 [00:08<00:21,  3.14it/s]\u001b[A\n",
            " 30% 28/93 [00:08<00:21,  3.03it/s]\u001b[A\n",
            " 31% 29/93 [00:09<00:22,  2.87it/s]\u001b[A\n",
            " 32% 30/93 [00:09<00:20,  3.04it/s]\u001b[A\n",
            " 33% 31/93 [00:09<00:20,  3.05it/s]\u001b[A\n",
            " 34% 32/93 [00:10<00:19,  3.09it/s]\u001b[A\n",
            " 35% 33/93 [00:10<00:19,  3.13it/s]\u001b[A\n",
            " 37% 34/93 [00:10<00:18,  3.20it/s]\u001b[A\n",
            " 38% 35/93 [00:11<00:17,  3.23it/s]\u001b[A\n",
            " 39% 36/93 [00:11<00:17,  3.18it/s]\u001b[A\n",
            " 40% 37/93 [00:11<00:17,  3.24it/s]\u001b[A\n",
            " 41% 38/93 [00:12<00:17,  3.19it/s]\u001b[A\n",
            " 42% 39/93 [00:12<00:17,  3.15it/s]\u001b[A\n",
            " 43% 40/93 [00:12<00:16,  3.15it/s]\u001b[A\n",
            " 44% 41/93 [00:12<00:16,  3.16it/s]\u001b[A\n",
            " 45% 42/93 [00:13<00:16,  3.16it/s]\u001b[A\n",
            " 46% 43/93 [00:13<00:15,  3.17it/s]\u001b[A\n",
            " 47% 44/93 [00:13<00:15,  3.17it/s]\u001b[A\n",
            " 48% 45/93 [00:14<00:15,  3.16it/s]\u001b[A\n",
            " 49% 46/93 [00:14<00:14,  3.28it/s]\u001b[A\n",
            " 51% 47/93 [00:14<00:13,  3.46it/s]\u001b[A\n",
            " 52% 48/93 [00:15<00:13,  3.37it/s]\u001b[A\n",
            " 53% 49/93 [00:15<00:14,  3.12it/s]\u001b[A\n",
            " 54% 50/93 [00:15<00:13,  3.14it/s]\u001b[A\n",
            " 55% 51/93 [00:16<00:13,  3.03it/s]\u001b[A\n",
            " 56% 52/93 [00:16<00:14,  2.87it/s]\u001b[A\n",
            " 57% 53/93 [00:16<00:14,  2.70it/s]\u001b[A\n",
            " 58% 54/93 [00:17<00:13,  2.83it/s]\u001b[A\n",
            " 59% 55/93 [00:17<00:13,  2.91it/s]\u001b[A\n",
            " 60% 56/93 [00:17<00:12,  3.01it/s]\u001b[A\n",
            " 61% 57/93 [00:18<00:13,  2.76it/s]\u001b[A\n",
            " 62% 58/93 [00:18<00:12,  2.85it/s]\u001b[A\n",
            " 63% 59/93 [00:19<00:12,  2.75it/s]\u001b[A\n",
            " 65% 60/93 [00:19<00:12,  2.62it/s]\u001b[A\n",
            " 66% 61/93 [00:19<00:12,  2.60it/s]\u001b[A\n",
            " 67% 62/93 [00:20<00:11,  2.77it/s]\u001b[A\n",
            " 68% 63/93 [00:20<00:10,  2.86it/s]\u001b[A\n",
            " 69% 64/93 [00:20<00:09,  2.91it/s]\u001b[A\n",
            " 70% 65/93 [00:21<00:09,  2.98it/s]\u001b[A\n",
            " 71% 66/93 [00:21<00:08,  3.03it/s]\u001b[A\n",
            " 72% 67/93 [00:21<00:08,  3.12it/s]\u001b[A\n",
            " 73% 68/93 [00:22<00:07,  3.17it/s]\u001b[A\n",
            " 74% 69/93 [00:22<00:07,  3.15it/s]\u001b[A\n",
            " 75% 70/93 [00:22<00:07,  3.14it/s]\u001b[A\n",
            " 76% 71/93 [00:23<00:07,  3.01it/s]\u001b[A\n",
            " 77% 72/93 [00:23<00:07,  2.84it/s]\u001b[A\n",
            " 78% 73/93 [00:23<00:06,  2.92it/s]\u001b[A\n",
            " 80% 74/93 [00:24<00:06,  2.80it/s]\u001b[A\n",
            " 81% 75/93 [00:24<00:05,  3.05it/s]\u001b[A\n",
            " 82% 76/93 [00:24<00:05,  3.10it/s]\u001b[A\n",
            " 83% 77/93 [00:25<00:05,  3.00it/s]\u001b[A\n",
            " 84% 78/93 [00:25<00:04,  3.03it/s]\u001b[A\n",
            " 85% 79/93 [00:25<00:04,  2.87it/s]\u001b[A\n",
            " 86% 80/93 [00:26<00:04,  3.05it/s]\u001b[A\n",
            " 87% 81/93 [00:26<00:03,  3.14it/s]\u001b[A\n",
            " 88% 82/93 [00:26<00:03,  3.15it/s]\u001b[A\n",
            " 89% 83/93 [00:26<00:03,  3.16it/s]\u001b[A\n",
            " 90% 84/93 [00:27<00:03,  2.90it/s]\u001b[A\n",
            " 91% 85/93 [00:27<00:02,  2.95it/s]\u001b[A\n",
            " 92% 86/93 [00:28<00:02,  2.99it/s]\u001b[A\n",
            " 94% 87/93 [00:28<00:02,  2.78it/s]\u001b[A\n",
            " 95% 88/93 [00:28<00:01,  2.73it/s]\u001b[A\n",
            " 96% 89/93 [00:29<00:01,  2.75it/s]\u001b[A\n",
            " 97% 90/93 [00:29<00:01,  2.83it/s]\u001b[A\n",
            " 98% 91/93 [00:29<00:00,  2.74it/s]\u001b[A\n",
            " 99% 92/93 [00:30<00:00,  2.83it/s]\u001b[A\n",
            "100% 93/93 [00:30<00:00,  3.23it/s]\u001b[A\n",
            "{'eval_loss': 1.5644265413284302, 'eval_accuracy': 0.4914, 'eval_runtime': 30.8012, 'eval_samples_per_second': 30.064, 'eval_steps_per_second': 3.019, 'epoch': 5.0}\n",
            "\n",
            "100% 8135/8135 [1:31:07<00:00,  1.66it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2640] 2023-01-15 16:49:38,141 >> Saving model checkpoint to saved/qasc/mcq/roberta-large/checkpoint-8135\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:49:38,142 >> Configuration saved in saved/qasc/mcq/roberta-large/checkpoint-8135/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:49:43,439 >> Model weights saved in saved/qasc/mcq/roberta-large/checkpoint-8135/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:49:43,440 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/checkpoint-8135/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:49:43,441 >> Special tokens file saved in saved/qasc/mcq/roberta-large/checkpoint-8135/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2023-01-15 16:49:54,800 >> Deleting older checkpoint [saved/qasc/mcq/roberta-large/checkpoint-6508] due to args.save_total_limit\n",
            "[INFO|trainer.py:1850] 2023-01-15 16:49:54,864 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5486.879, 'train_samples_per_second': 7.412, 'train_steps_per_second': 1.483, 'train_loss': 1.5372323212626526, 'epoch': 5.0}\n",
            "100% 8135/8135 [1:31:24<00:00,  1.48it/s]\n",
            "[INFO|trainer.py:2640] 2023-01-15 16:49:54,870 >> Saving model checkpoint to saved/qasc/mcq/roberta-large\n",
            "[INFO|configuration_utils.py:451] 2023-01-15 16:49:54,871 >> Configuration saved in saved/qasc/mcq/roberta-large/config.json\n",
            "[INFO|modeling_utils.py:1566] 2023-01-15 16:50:00,442 >> Model weights saved in saved/qasc/mcq/roberta-large/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2023-01-15 16:50:00,443 >> tokenizer config file saved in saved/qasc/mcq/roberta-large/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2023-01-15 16:50:00,444 >> Special tokens file saved in saved/qasc/mcq/roberta-large/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     1.5372\n",
            "  train_runtime            = 1:31:26.87\n",
            "  train_samples            =       8134\n",
            "  train_samples_per_second =      7.412\n",
            "  train_steps_per_second   =      1.483\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-15 16:50:00,574 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:50:00,578 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:50:00,578 >>   Num examples = 926\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:50:00,579 >>   Batch size = 10\n",
            "100% 93/93 [00:29<00:00,  3.11it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.4914\n",
            "  eval_loss               =     1.5644\n",
            "  eval_runtime            = 0:00:30.32\n",
            "  eval_samples            =        926\n",
            "  eval_samples_per_second =     30.532\n",
            "  eval_steps_per_second   =      3.066\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:722] 2023-01-15 16:50:30,910 >> The following columns in the test set don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context. If choice3, choice1, choice2, choice7, choice6, choice0, choice5, choice4, context are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-15 16:50:30,913 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2893] 2023-01-15 16:50:30,914 >>   Num examples = 920\n",
            "[INFO|trainer.py:2896] 2023-01-15 16:50:30,914 >>   Batch size = 10\n",
            "100% 92/92 [00:30<00:00,  3.04it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▂▁▃▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▇▇▇▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▂▂▂▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▂▂▂▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ██▇▆▆▅▄▃▃▃▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.4914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.56443\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 30.3287\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 30.532\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 3.066\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 8135\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 1.1758\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.3161141648293328e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.53723\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 5486.879\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 7.412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMCQA qasc ROBERTA\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/nlpteam_gr/huggingface/runs/1lvrhmk8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230115_151829-1lvrhmk8/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run_mcqa_score.py --learning_rate=3e-6 --num_train_epochs 5 --seed 42 \\\n",
        "--train_file=\"data/qasc/mcq_train.json\" --validation_file=\"data/qasc/mcq_dev.json\" --test_file=\"data/qasc/mcq_test.json\" \\\n",
        "--output_dir=\"saved/qasc/mcq/roberta-large\" --model_name_or_path=\"roberta-large\" \\\n",
        "--per_device_train_batch_size=5 --per_device_eval_batch_size=10 --weight_decay=0.005 \\\n",
        "--do_train True --do_eval True --do_predict True --evaluation_strategy=\"epoch\" --save_strategy=\"epoch\" \\\n",
        "--report_to \"wandb\" --run_name \"MCQA qasc ROBERTA\" --save_total_limit=1 --overwrite_output_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Dataset | Model | Instance Accuracy (%)  | CLS Accuracy (%)  |\n",
        "|---|---|---|---|\n",
        "| CSQA  | TEAM RoBERTa  | 0.7428 | 0.8314 |\n",
        "| CSQA  | TEAM DeBEARTa  | 0.7666 | 0.8537 |\n",
        "| CSQA  | SCORE RoBEARTa  | 0.6536 | |\n",
        "| CSQA2  | TEAM RoBERTa  | 0.5222 | 0.5 |\n",
        "| CSQA2  | TEAM DeBEARTa  | 0.669 | 0.671 |\n",
        "| CSQA2  | SCORE RoBERTa  | 0.5333 | |\n",
        "| QASC  | TEAM RoBERTa  | 0.4752 | 0.8371 |\n",
        "| QASC  | TEAM DeBEARTa  | 0.5594 | 0.8506 |\n",
        "| QASC  | SCORE RoBERTa  | 0.4914 | |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
